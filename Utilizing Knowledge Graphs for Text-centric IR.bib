@inproceedings{Voskarides2015,
author = {Voskarides, Nikos and Meij, Edgar and Tsagkias, Manos and de Rijke, Maarten and Weerkamp, Wouter},
booktitle = {ACL (1)},
pages = {564--574},
title = {{Learning to Explain Entity Relationships in Knowledge Graphs.}},
url = {http://dblp.uni-trier.de/db/conf/acl/acl2015-1.html{\#}VoskaridesMTRW15},
year = {2015}
}
@inproceedings{Graus2016,
abstract = {{\textcopyright} 2016 Copyright held by the owner/author(s).Entity ranking, i.e., successfully positioning a relevant entity at the top of the ranking for a given query, is inherently difficult due to the potential mismatch between the entity's description in a knowledge base, and the way people refer to the entity when searching for it. To counter this issue we propose a method for constructing dynamic collective entity representations. We collect entity descriptions from a variety of sources and combine them into a single entity representation by learning to weight the content from different sources that are associated with an entity for optimal retrieval effectiveness. Our method is able to add new descriptions in real time and learn the best representation as time evolves so as to capture the dynamics of how people search entities. Incorporating dynamic description sources into dynamic collective entity representations improves retrieval effectiveness by 7{\%} over a state-of-the-art learning to rank baseline. Periodic retraining of the ranker enables higher ranking effectiveness for dynamic collective entity representations.},
author = {Graus, D. and Tsagkias, M. and Weerkamp, W. and Meij, E. and {De Rijke}, M.},
booktitle = {WSDM 2016 - Proceedings of the 9th ACM International Conference on Web Search and Data Mining},
doi = {10.1145/2835776.2835819},
isbn = {9781450337168},
keywords = {[Content representation, Entity ranking]},
title = {{Dynamic collective entity representations for entity ranking}},
year = {2016}
}
@inproceedings{SIGIR:2001:Lavrenko,
abstract = {We explore the relation between classical probabilistic models of information retrieval and the emerging language modeling approaches. It has long been recognized that the primary obstacle to effective performance of classical models is the need to estimate arelevance model: probabilities of words in the relevant class. We propose a novel technique for estimating these probabilities using the query alone. We demonstrate that our technique can produce highly accurate relevance models, addressing important notions of synonymy and polysemy. Our experiments show relevance models outperforming baseline language modeling systems on TREC retrieval and TDT tracking tasks. The main contribution of this work is an effective formal method for estimating a relevance model with no training data.},
address = {New Orleans, Louisiana, United States},
author = {Lavrenko, Victor and Croft, W. Bruce},
doi = {10.1145/383952.383972},
isbn = {1-58113-331-6},
keywords = {document{\_}model,lm,query{\_}model,relevance{\_}feedback,relevance{\_}model,retrieval{\_}model},
mendeley-tags = {document{\_}model,lm,query{\_}model,relevance{\_}feedback,relevance{\_}model,retrieval{\_}model},
pages = {120--127},
publisher = {ACM},
series = {SIGIR '01},
title = {{Relevance based language models}},
type = {Conference proceedings (article)},
url = {http://portal.acm.org/citation.cfm?id=383972},
year = {2001}
}
@inproceedings{SIGIR:2005:metzler,
abstract = {This paper develops a general, formal framework for modeling term dependencies via Markov random fields. The model allows for arbitrary text features to be incorporated as evidence. In particular, we make use of features based on occurrences of single terms, ordered phrases, and unordered phrases. We explore full independence, sequential dependence, and full dependence variants of the model. A novel approach is developed to train the model that directly maximizes the mean average precision rather than maximizing the likelihood of the training data. Ad hoc retrieval experiments are presented on several newswire and web collections, including the GOV2 collection used at the TREC 2004 Terabyte Track. The results show significant improvements are possible by modeling dependencies, especially on the larger web collections.},
address = {Salvador, Brazil},
author = {Metzler, Donald and Croft, W. Bruce},
doi = {10.1145/1076034.1076115},
isbn = {1-59593-034-5},
keywords = {query{\_}modeling},
mendeley-tags = {query{\_}modeling},
pages = {472--479},
publisher = {ACM},
series = {SIGIR '05},
title = {{A Markov random field model for term dependencies}},
type = {Conference proceedings (article)},
url = {http://portal.acm.org/citation.cfm?id=1076034.1076115},
year = {2005}
}
@inproceedings{Reinanda2015,
address = {New York, New York, USA},
author = {Reinanda, Ridho and Meij, Edgar and de Rijke, Maarten},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '15},
doi = {10.1145/2766462.2767724},
isbn = {9781450336215},
pages = {263--272},
publisher = {ACM Press},
title = {{Mining, Ranking and Recommending Entity Aspects}},
url = {http://dblp.uni-trier.de/db/conf/sigir/sigir2015.html{\#}ReinandaMR15},
year = {2015}
}
@inproceedings{Voskarides2015,
author = {Voskarides, Nikos and Meij, Edgar and Tsagkias, Manos and de Rijke, Maarten and Weerkamp, Wouter},
booktitle = {ACL (1)},
pages = {564--574},
title = {{Learning to Explain Entity Relationships in Knowledge Graphs.}},
url = {http://dblp.uni-trier.de/db/conf/acl/acl2015-1.html{\#}VoskaridesMTRW15},
year = {2015}
}
@inproceedings{mintz2009distant,
author = {Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
booktitle = {ACL},
title = {{Distant supervision for relation extraction without labeled data}},
year = {2009}
}
@inproceedings{chakrabarti2007dynamicperspagerank,
author = {Chakrabarti, Soumen},
booktitle = {Proceedings of the 16th international conference on World Wide Web},
organization = {ACM},
pages = {571--580},
title = {{Dynamic personalized pagerank in entity-relation graphs}},
year = {2007}
}
@inproceedings{bendersky2012multisourceexpansion,
author = {Bendersky, Michael and Metzler, Donald and Croft, W Bruce},
booktitle = {Proc. of WSDM-12},
pages = {443--452},
title = {{Effective query formulation with multiple information sources}},
year = {2012}
}
@article{middleton2004ontologicalrecommendersystems,
author = {Middleton, Stuart E and Shadbolt, Nigel R and {De Roure}, David C},
journal = {ACM Transactions on Information Systems (TOIS)},
number = {1},
pages = {54--88},
publisher = {ACM},
title = {{Ontological user profiling in recommender systems}},
volume = {22},
year = {2004}
}
@inproceedings{zhiltsov2015fielded,
author = {Zhiltsov, Nikita and Kotov, Alexander and Nikolaev, Fedor},
booktitle = {SIGIR},
title = {{Fielded Sequential Dependence Model for Ad-Hoc Entity Retrieval in the Web of Data}},
year = {2015}
}
@inbook{herbrich00,
author = {Herbrich, Ralf and Graepel, Thore and Obermayer, Klaus},
booktitle = {{\{}A{\}}dvances in {\{}L{\}}arge {\{}M{\}}argin {\{}C{\}}lassifiers},
chapter = {7},
pages = {115???132},
publisher = {MIT Press},
title = {{{\{}L{\}}arge {\{}M{\}}argin {\{}R{\}}ank {\{}B{\}}oundaries for {\{}O{\}}rdinal {\{}R{\}}egression}},
year = {2000}
}
@inproceedings{hoffart14b,
author = {Hoffart, Johannes and Altun, Yasemin and Weikum, Gerhard},
booktitle = {WWW},
title = {{{\{}D{\}}iscovering emerging entities with ambiguous names}},
year = {2014}
}
@inproceedings{berant2013semantic,
author = {Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
booktitle = {EMNLP},
pages = {1533--1544},
title = {{Semantic parsing on freebase from question-answer pairs}},
year = {2013}
}
@inproceedings{carlson10,
author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Hruschka, Estevam R and Mitchell, Tom M},
booktitle = {Proc. of AAAI-10},
pages = {1306--1313},
title = {{Toward an Architecture for Never-Ending Language Learning}},
year = {2010}
}
@inproceedings{ji2014overview,
author = {Ji, Heng and Nothman, Joel and Hachey, Ben and Others},
booktitle = {TAC},
pages = {1333--1339},
title = {{Overview of tac-kbp2014 entity discovery and linking tasks}},
year = {2014}
}
@inproceedings{raviv12,
author = {Raviv, Hadas and Carmel, David and Kurland, Oren},
booktitle = {JIWES},
title = {{{\{}A{\}} {\{}R{\}}anking {\{}F{\}}ramework for {\{}E{\}}ntity {\{}O{\}}riented {\{}S{\}}earch {\{}U{\}}sing {\{}M{\}}arkov {\{}R{\}}andom {\{}F{\}}ields}},
year = {2012}
}
@inproceedings{dietz2013opinions,
author = {Dietz, Laura and Wang, Ziqi and Huston, Samuel and Croft, W Bruce},
booktitle = {CIKM},
pages = {1225--1228},
title = {{Retrieving opinions from discussion forums}},
year = {2013}
}
@inproceedings{pehcevski08,
author = {Pehcevski, Jovan and Vercoustre, Anne-Marie and Thom, James A},
pages = {258--269},
shorttitle = {{\#}ecir-08s{\#}},
title = {{{\{}E{\}}xploiting {\{}L{\}}ocality of {\{}W{\}}ikipedia {\{}L{\}}inks in {\{}E{\}}ntity {\{}R{\}}anking}},
year = {2008}
}
@inproceedings{dietz14,
author = {Dietz, Laura and Schuhmacher, Michael and Ponzetto, Simone Paolo},
booktitle = {AKBC-14},
title = {{Queripidia: Query-specific {\{}W{\}}ikipedia Construction}},
year = {2014}
}
@inproceedings{kotov2011mining,
author = {Kotov, Alexander and Zhai, ChengXiang and Sproat, Richard},
booktitle = {WSDM},
title = {{Mining named entities with temporally correlated bursts from multilingual web news streams}},
year = {2011}
}
@inproceedings{he2017measuring,
author = {He, Jiyin and Bron, Marc},
booktitle = {KG4IR Workshop at SIGIR},
title = {{Measuring Demonstrated Potential Domain Knowledge with Knowledge Graphs}},
year = {2017}
}
@article{liu10,
author = {Liu, Tie-Yan and Joachims, Thorsten and Li, Hang and Zhai, Chengxiang},
journal = {{\{}J{\}}ournal of {\{}I{\}}nformation {\{}R{\}}etrieval},
number = {3},
pages = {197--200},
title = {{{\{}I{\}}ntroduction to the special issue on learning to rank for information retrieval}},
volume = {13},
year = {2010}
}
@article{hovy13,
author = {Hovy, Eduard and Navigli, Roberto and Ponzetto, Simone Paolo},
journal = {{\{}A{\}}rtificial {\{}I{\}}ntelligence},
pages = {2--27},
title = {{{\{}C{\}}ollaboratively built semi-structured content and {\{}A{\}}rtificial {\{}I{\}}ntelligence: {\{}T{\}}he story so far}},
volume = {194},
year = {2013}
}
@inproceedings{hasibi2017entity,
author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
booktitle = {ECIR},
pages = {40--53},
title = {{Entity linking in queries: Efficiency vs. effectiveness}},
year = {2017}
}
@inproceedings{hasibi2017dbpedia,
author = {Hasibi, Faegheh and Nikolaev, Fedor and Xiong, Chenyan and Balog, Krisztian and Bratsberg, Svein Erik and Kotov, Alexander and Callan, Jamie},
booktitle = {SIGIR},
pages = {1265--1268},
title = {{DBpedia-Entity v2: A Test Collection for Entity Search}},
year = {2017}
}
@book{brusilovski2007personalization,
author = {Brusilovski, Peter and Kobsa, Alfred and Nejdl, Wolfgang},
publisher = {Springer Science {\&} Business Media},
title = {{The adaptive web: methods and strategies of web personalization}},
volume = {4321},
year = {2007}
}
@article{liu2015les,
author = {Liu, Xitong and Fang, Hui},
journal = {Information Retrieval Journal},
number = {6},
pages = {473--503},
publisher = {Springer},
title = {{Latent entity space: a novel retrieval approach for entity-bearing queries}},
volume = {18},
year = {2015}
}
@inproceedings{voorhees2005trec,
author = {Voorhees, Ellen M},
booktitle = {ACM SIGIR Forum},
pages = {11--20},
title = {{{\{}T{\}}he {\{}TREC{\}} robust retrieval track}},
volume = {39},
year = {2005}
}
@inproceedings{bowden2017combining,
author = {Bowden, Kevin K and Oraby, Shereen and Wu, Jiaqi and Misra, Amita and Walker, Marilyn},
booktitle = {SCAI Workshop at ICTIR},
title = {{Combining Search with Structured Data to Create a More Engaging User Experience in Open Domain Dialogue}},
year = {2017}
}
@inproceedings{christensen14,
author = {Christensen, Janara and Soderland, Stephen and Bansal, Gagan and Mausam},
booktitle = {Proc. of ACL-14},
pages = {902--912},
title = {{Hierarchical Summarization: Scaling Up Multi-Document Summarization}},
year = {2014}
}
@inproceedings{balog06,
author = {Balog, Krisztian and Azzopardi, Leif and de Rijke, Maarten},
booktitle = {Proc. of SIGIR-06},
pages = {43--50},
title = {{{\{}F{\}}ormal {\{}M{\}}odels for {\{}E{\}}xpert {\{}F{\}}inding in {\{}E{\}}nterprise {\{}C{\}}orpora}},
year = {2006}
}
@article{miller1995wordnet,
author = {Miller, George A},
journal = {Communications of the ACM},
number = {11},
pages = {39--41},
publisher = {ACM},
title = {{WordNet: a lexical database for English}},
volume = {38},
year = {1995}
}
@inproceedings{metzler2005markov,
author = {Metzler, Donald and Croft, W Bruce},
booktitle = {Proc. of SIGIR-05},
pages = {472--479},
title = {{{\{}A{\}} {\{}M{\}}arkov random field model for term dependencies}},
year = {2005}
}
@article{kelly07,
author = {Kelly, Diane and Lin, Jimmy},
journal = {SIGIR Forum},
number = {1},
pages = {107--116},
title = {{Overview of the {\{}TREC{\}} 2006 {\{}ciQA{\}} Task}},
volume = {41},
year = {2007}
}
@inproceedings{kotov2012tapping,
author = {Kotov, Alexander and Zhai, ChengXiang},
booktitle = {WSDM},
title = {{Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries}},
year = {2012}
}
@inproceedings{rao2013entitylinking,
author = {Rao, Delip and McNamee, Paul and Dredze, Mark},
booktitle = {Multi-source, Multilingual Information Extraction and Summarization},
title = {{Entity linking: Finding extracted entities in a knowledge base}},
year = {2013}
}
@inproceedings{cheng07,
author = {Cheng, Tao and Yan, Xifeng and Chang, Kevin Chen-Chuan},
booktitle = {Proc. of VLDB '07},
pages = {387--398},
title = {{{\{}EntityRank{\}}: searching entities directly and holistically}},
year = {2007}
}
@book{li11,
author = {Li, Hang},
publisher = {Morgan {\&} Claypool Publ.},
series = {Synthesis Lectures on Human Language Technologies},
title = {{{\{}L{\}}earning to {\{}R{\}}ank for {\{}I{\}}nformation {\{}R{\}}etrieval and {\{}N{\}}atural {\{}L{\}}anguage {\{}P{\}}rocessing}},
year = {2011}
}
@inproceedings{xiong2015esdrank,
author = {Xiong, Chenyan and Callan, Jamie},
booktitle = {CIKM},
title = {{Esdrank: Connecting query and documents through external semi-structured data}},
year = {2015}
}
@article{carletta96,
author = {Carletta, Jean},
journal = {Computational Linguistics},
number = {2},
pages = {249--254},
title = {{Assessing agreement on classification tasks: The kappa Statistic}},
volume = {22},
year = {1996}
}
@inproceedings{lavrenko2001relevance,
author = {Lavrenko, Victor and Croft, W Bruce},
booktitle = {Proc. of SIGIR-01},
pages = {120--127},
title = {{{\{}R{\}}elevance based language models}},
year = {2001}
}
@inproceedings{entitytrack2010,
author = {Balog, Krisztian and Serdyukov, Pavel and de Vries, Arjen P},
booktitle = {TREC},
title = {{Overview of the {\{}TREC{\}} 2010 entity track}},
year = {2010}
}
@inproceedings{dalton13,
author = {Dalton, Jeffrey and Dietz, Laura},
booktitle = {AKBC Workshop at CIKM},
pages = {55--60},
title = {{Constructing query-specific knowledge bases}},
year = {2013}
}
@inproceedings{roth14,
author = {Roth, Benjamin and Barth, Tassilo and Chrupa{\l}a, Grzegorz and Gropp, Martin and Klakow, Dietrich},
booktitle = {Proc. of EACL-14},
pages = {89},
title = {{{\{}R{\}}elationfactory: {\{}A{\}} fast, modular and effective system for knowledge base population}},
year = {2014}
}
@inproceedings{zhiltsov13,
author = {Zhiltsov, Nikita and Agichtein, Eugene},
booktitle = {CIKM},
title = {{Improving entity search over linked data by modeling latent semantics}},
year = {2013}
}
@inproceedings{elbassuoni2009language,
author = {Elbassuoni, Shady and Ramanath, Maya and Schenkel, Ralf and Sydow, Marcin and Weikum, Gerhard},
booktitle = {Proc. of CIKM-09},
pages = {977--986},
title = {{{\{}L{\}}anguage-model-based ranking for queries on {\{}RDF{\}}-graphs}},
year = {2009}
}
@inproceedings{robertson10a,
author = {Robertson, Stephen E and Kanoulas, Evangelos and Yilmaz, Emine},
booktitle = {Proc. of SIGIR-10},
organization = {ACM},
pages = {603--610},
title = {{{\{}E{\}}xtending average precision to graded relevance judgments}},
year = {2010}
}
@inproceedings{xiong2017duet,
author = {Xiong, Chenyan and Callan, Jamie and Liu, Tie-Yan},
booktitle = {SIGIR},
pages = {763--772},
title = {{Word-Entity Duet Representations for Document Ranking}},
year = {2017}
}
@inproceedings{spitkovsky2012crosswiki,
author = {Spitkovsky, Valentin I and Chang, Angel X},
booktitle = {LREC},
pages = {3168--3175},
title = {{A Cross-Lingual Dictionary for English Wikipedia Concepts.}},
year = {2012}
}
@inproceedings{cornolti13,
author = {Cornolti, Marco and Ferragina, Paolo and Ciaramita, Massimiliano},
booktitle = {WWW},
pages = {249--260},
title = {{A framework for benchmarking entity-annotation systems}},
year = {2013}
}
@inproceedings{kaptein10,
author = {Kaptein, Rianne and Serdyukov, Pavel and de Vries, Arjen P and Kamps, Jaap},
pages = {69--78},
title = {{{\{}E{\}}ntity ranking using {\{}W{\}}ikipedia as a pivot}},
year = {2010}
}
@inproceedings{bilotti2010rank,
author = {Bilotti, Matthew W and Elsas, Jonathan and Carbonell, Jaime and Nyberg, Eric},
booktitle = {Proc. of CIKM-10},
pages = {459--468},
title = {{Rank learning for factoid question answering with linguistic and semantic constraints}},
year = {2010}
}
@inproceedings{duan2015intentrepresentation,
author = {Duan, Huizhong and Zhai, ChengXiang},
booktitle = {CIKM},
organization = {ACM},
pages = {333--342},
title = {{Mining Coordinated Intent Representation for Entity Search and Recommendation}},
year = {2015}
}
@misc{graff03,
annote = {LDC2003T05. Philadelphia: Linguistic Data Consortium},
author = {Graff, David and Cieri, Christopher},
title = {{{\{}E{\}}nglish {\{}G{\}}igaword}},
year = {2003}
}
@inproceedings{dietz2013umass,
author = {Dietz, Laura and Dalton, Jeffrey},
booktitle = {TREC},
title = {{UMass at TREC 2013 Knowledge Base Acceleration Track: Bi-directional Entity Linking and Time-aware Evaluation.}},
year = {2013}
}
@article{getoor2005link,
author = {Getoor, Lise and Diehl, Christopher P},
journal = {ACM SIGKDD Explorations Newsletter},
number = {2},
pages = {3--12},
publisher = {ACM},
title = {{Link mining: a survey}},
volume = {7},
year = {2005}
}
@inproceedings{kaptein2010entityranking_extlink,
author = {Kaptein, Rianne and Serdyukov, Pavel and de Vries, Arjen P and Kamps, Jaap},
booktitle = {CIKM},
pages = {69--78},
title = {{Entity ranking using Wikipedia as a pivot}},
year = {2010}
}
@inproceedings{ji09,
author = {Ji, Heng and Grishman, Ralph and Dang, Hoa Trang and Griffitt, Kira and Ellis, Joe},
booktitle = {Proc. of {\{}TAC{\}} 2010},
title = {{{\{}O{\}}verview of the {\{}TAC{\}} 2010 {\{}K{\}}nowledge {\{}B{\}}ase {\{}P{\}}opulation {\{}T{\}}rack}},
year = {2010}
}
@inproceedings{vakulenko2017conversational,
author = {Vakulenko, Svitlana and Markov, Ilya and de Rijke, Maarten},
booktitle = {SCAI},
title = {{Conversational Exploratory Search via Interactive Storytelling}},
year = {2017}
}
@inproceedings{queripidia-akbc,
author = {Dietz, Laura and Schuhmacher, Michael and Ponzetto, Simone Paolo},
booktitle = {AKBC-14},
title = {{Queripidia: Query-specific Wikipedia Construction}},
year = {2014}
}
@inproceedings{voskarides15,
author = {Voskarides, Nikos and Meij, Edgar and Tsagkias, Manos and de Rijke, Maarten and Weerkamp, Wouter},
booktitle = {ACL},
title = {{{\{}L{\}}earning to {\{}E{\}}xplain {\{}E{\}}ntity {\{}R{\}}elationships in {\{}K{\}}nowledge {\{}G{\}}raphs}},
year = {2015}
}
@inproceedings{unger12,
author = {Unger, Christina and B{\"{u}}hmann, Lorenz and Lehmann, Jens and {Ngonga Ngomo}, Axel-Cyrille and Gerber, Daniel and Cimiano, Philipp},
booktitle = {WWW},
pages = {639--648},
title = {{Template-based Question Answering over {\{}RDF{\}} Data}},
year = {2012}
}
@inproceedings{yahya2012nlpsparql,
author = {Yahya, Mohamed and Berberich, Klaus and Elbassuoni, Shady and Ramanath, Maya and Tresp, Volker and Weikum, Gerhard},
booktitle = {EMNLP},
pages = {379--390},
title = {{Natural language questions for the web of data}},
year = {2012}
}
@incollection{nenkova2012summarizationsurvey,
author = {Nenkova, Ani and McKeown, Kathleen},
booktitle = {Mining Text Data},
pages = {43--76},
publisher = {Springer},
title = {{A survey of text summarization techniques}},
year = {2012}
}
@article{ferraginaS12,
author = {Ferragina, Paolo and Scaiella, Ugo},
journal = {IEEE Software},
number = {1},
pages = {70--75},
title = {{Fast and Accurate Annotation of Short Texts with {\{}W{\}}ikipedia Pages}},
volume = {29},
year = {2012}
}
@inproceedings{radlinski2017theoretical,
author = {Radlinski, Filip and Craswell, Nick},
booktitle = {CHIIR},
pages = {117--126},
title = {{A theoretical framework for conversational search}},
year = {2017}
}
@inproceedings{roth13,
author = {Roth, Benjamin and Klakow, Dietrich},
booktitle = {Proc. of CIKM-13},
organization = {ACM},
pages = {1181--1184},
title = {{{\{}F{\}}eature-based models for improving the quality of noisy training data for relation extraction}},
year = {2013}
}
@inproceedings{Xiong2015QueryEW,
author = {Xiong, Chenyan and Callan, James P},
booktitle = {ICTIR},
title = {{Query Expansion with Freebase}},
year = {2015}
}
@inproceedings{demartini09,
author = {Demartini, Gianluca and Iofciu, Tereza and de Vries, Arjen P},
booktitle = {INEX},
title = {{{\{}O{\}}verview of the {\{}INEX{\}} 2009 {\{}E{\}}ntity {\{}R{\}}anking {\{}T{\}}rack}},
year = {2009}
}
@article{kaptein2013entityranking_cat,
author = {Kaptein, Rianne and Kamps, Jaap},
journal = {Artificial Intelligence},
pages = {111--129},
title = {{Exploiting the category structure of Wikipedia for entity ranking}},
volume = {194},
year = {2013}
}
@inproceedings{lin12,
author = {Lin, Thomas and Pantel, Patrick and Gamon, Michael and Kannan, Anitha and Fuxman, Ariel},
booktitle = {WWW},
pages = {589--598},
title = {{Active objects: Actions for entity-centric search}},
year = {2012}
}
@inproceedings{riedel13,
author = {Riedel, Sebastian and Yao, Limin and McCallum, Andrew and Marlin, Benjamin M},
booktitle = {Proc. of NAACL-HLT'13},
pages = {74--84},
title = {{{\{}R{\}}elation extraction with matrix factorization and universal schemas}},
year = {2013}
}
@article{bast2016semantic,
author = {Bast, Hannah and Buchhold, Bj{\"{o}}rn and Haussmann, Elmar and Others},
journal = {Foundations and Trends{\textregistered}in Information Retrieval},
number = {2-3},
pages = {119--271},
publisher = {Now Publishers, Inc.},
title = {{Semantic search on text and knowledge bases}},
volume = {10},
year = {2016}
}
@inproceedings{bergkirkpatrick11,
author = {Berg-Kirkpatrick, Taylor and Gillick, Dan and Klein, Dan},
booktitle = {ACL-HLT-11},
pages = {481--490},
title = {{Jointly Learning to Extract and Compress}},
year = {2011}
}
@inproceedings{louis14,
author = {Louis, Annie},
booktitle = {ACL},
pages = {333--338},
title = {{A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization}},
year = {2014}
}
@article{kotov2016knowledge,
author = {Kotov, Alexander},
journal = {tutorial chapter, RuSSIR},
title = {{Knowledge Graph Entity Representation and Retrieval}},
year = {2016}
}
@inproceedings{woodsend10,
author = {Woodsend, Kristian and Lapata, Mirella},
booktitle = {ACL},
pages = {565--574},
title = {{Automatic Generation of Story Highlights}},
year = {2010}
}
@article{li2011letor,
author = {Li, Hang},
journal = {Synthesis Lectures on Human Language Technologies},
number = {1},
pages = {1--113},
publisher = {Morgan {\&} Claypool Publishers},
title = {{Learning to Rank for Information Retrieval and Natural Language Processing}},
volume = {4},
year = {2011}
}
@inproceedings{West14,
author = {West, Robert and Gabrilovich, Evgeniy and Murphy, Kevin and Sun, Shaohua and Gupta, Rahul and Lin, Dekang},
booktitle = {Proc. of WWW-14},
pages = {515--526},
title = {{{\{}K{\}}nowledge {\{}B{\}}ase {\{}C{\}}ompletion via {\{}S{\}}earch-based {\{}Q{\}}uestion {\{}A{\}}nswering}},
year = {2014}
}
@inproceedings{Jarvelin00,
author = {J{\"{a}}rvelin, Kalervo and Kek{\"{a}}l{\"{a}}inen, Jaana},
booktitle = {Proc. of SIGIR-00},
pages = {41--48},
title = {{{\{}IR{\}} {\{}E{\}}valuation {\{}M{\}}ethods for {\{}R{\}}etrieving {\{}H{\}}ighly {\{}R{\}}elevant {\{}D{\}}ocuments}},
year = {2000}
}
@proceedings{dietz2017kg4ir,
editor = {Dietz, Laura and Xiong, Chenyan and Meij, Edgar},
publisher = {CEUR-WS.org},
series = {{\{}CEUR{\}} Workshop Proceedings},
title = {{Proceedings of the First Workshop on Knowledge Graphs and Semantics for Text Retrieval and Analysis {\{}(KG4IR{\}} 2017) co-located with the 40th International {\{}ACM{\}} {\{}SIGIR{\}} Conference on Research and Development in Information Retrieval {\{}(SIGIR{\}} 2017), Shinjuk}},
url = {http://ceur-ws.org/Vol-1883},
volume = {1883},
year = {2017}
}
@article{lao2010relational,
author = {Lao, Ni and Cohen, William W},
journal = {Machine Learning},
number = {1},
pages = {53--67},
publisher = {Springer},
title = {{Relational retrieval using a combination of path-constrained random walks}},
volume = {81},
year = {2010}
}
@inproceedings{sauper2009wikigeneration,
author = {Sauper, Christina and Barzilay, Regina},
booktitle = {ACL},
pages = {208--216},
title = {{Automatically generating {\{}W{\}}ikipedia articles: A structure-aware approach}},
year = {2009}
}
@inproceedings{kadry2017open,
author = {Kadry, Amina and Dietz, Laura},
booktitle = {SIGIR},
pages = {1149--1152},
title = {{Open Relation Extraction for Support Passage Retrieval: Merit and Open Issues}},
year = {2017}
}
@inproceedings{Nie2006topicalpagerank,
address = {New York, NY, USA},
author = {Nie, Lan and Davison, Brian D and Qi, Xiaoguang},
booktitle = {SIGIR},
pages = {91--98},
title = {{Topical Link Analysis for Web Search}},
year = {2006}
}
@article{turney10,
author = {Turney, Peter D and Pantel, Patrick},
journal = {{\{}J{\}}ournal of {\{}A{\}}rtificial {\{}I{\}}ntelligence {\{}R{\}}esearch},
pages = {141--188},
title = {{{\{}F{\}}rom {\{}F{\}}requency to {\{}M{\}}eaning: {\{}V{\}}ector {\{}S{\}}pace {\{}M{\}}odels of {\{}S{\}}emantics}},
volume = {37},
year = {2010}
}
@techreport{tac13,
author = {NIST},
institution = {NIST},
title = {{{\{}C{\}}old {\{}S{\}}tart {\{}K{\}}nowledge {\{}B{\}}ase {\{}P{\}}opulation at {\{}TAC{\}} 2013 {\{}T{\}}ask {\{}D{\}}escription ({\{}V{\}}.1.1)}},
year = {2013}
}
@inproceedings{dutta14,
author = {Dutta, Arnab and Meilicke, Christian and Ponzetto, Simone Paolo},
booktitle = {Proc. of ESWC-14},
pages = {286--301},
title = {{{\{}A{\}} {\{}P{\}}robabilistic {\{}A{\}}pproach for {\{}I{\}}ntegrating {\{}H{\}}eterogeneous {\{}K{\}}nowledge {\{}S{\}}ources}},
year = {2014}
}
@inproceedings{raviv2016entitylinking,
author = {Raviv, Hadas and Kurland, Oren and Carmel, David},
booktitle = {SIGIR},
title = {{Document Retrieval Using Entity-Based Language Models}},
year = {2016}
}
@inproceedings{bendersky2011quality,
author = {Bendersky, Michael and Croft, W Bruce and Diao, Yanlei},
booktitle = {Proc. of WSDM-11},
pages = {95--104},
title = {{Quality-biased ranking of web documents}},
year = {2011}
}
@article{hovy13b,
author = {Hovy, Eduard and Navigli, Roberto and Ponzetto, Simone Paolo},
journal = {Artificial Intelligence},
pages = {2--27},
title = {{Collaboratively built semi-structured content and {\{}A{\}}rtificial {\{}I{\}}ntelligence: {\{}T{\}}he story so far}},
volume = {194},
year = {2013}
}
@book{voorhees05,
author = {Voorhees, Ellen M and Harman, Donna K},
publisher = {The MIT Press},
title = {{{\{}TREC{\}}: {\{}E{\}}xperiment and {\{}E{\}}valuation in {\{}I{\}}nformation {\{}R{\}}etrieval}},
year = {2005}
}
@inproceedings{li2017natural,
author = {Li, Hongyu and Xiong, Chenyan and Callan, Jamie},
booktitle = {KG4IR Workshop at SIGIR},
title = {{Natural Language Supported Relation Matching for Question Answering with Knowledge Graphs}},
year = {2017}
}
@inproceedings{fader2011openie,
author = {Fader, Anthony and Soderland, Stephen and Etzioni, Oren},
booktitle = {EMNLP},
title = {{Identifying relations for open information extraction}},
year = {2011}
}
@article{carpineto2012subtopicdiversification,
author = {Carpineto, Claudio and {D Amico}, Massimiliano and Romano, Giovanni},
journal = {Information Processing and Management},
number = {2},
pages = {358--373},
title = {{Evaluating subtopic retrieval methods: Clustering versus diversification of search results}},
volume = {48},
year = {2012}
}
@inproceedings{dalton14,
author = {Dalton, Jeffrey and Dietz, Laura and Allan, James},
booktitle = {SIGIR},
title = {{Entity Query Feature Expansion Using Knowledge Base Links}},
year = {2014}
}
@inproceedings{mackinnon08,
author = {MacKinnon, Ian and Vechtomova, Olga},
booktitle = {ECIR},
pages = {438--445},
title = {{Improving Complex Interactive Question Answering with {\{}W{\}}ikipedia Anchor Text}},
year = {2008}
}
@inproceedings{nanni2018entityaspect,
author = {Nanni, Federico and Ponzetto, Simone Paolo and Dietz, Laura},
booktitle = {JCDL},
title = {{Entity-aspect linking: providing fine-grained semantics of entities in context}},
year = {2018}
}
@inproceedings{balog10,
author = {Balog, K and de Vries, A P and Serdyukov, P and Thomas, P and Westerveld, T},
booktitle = {Proc. of TREC-09},
title = {{{\{}O{\}}verview of the {\{}TREC{\}} 2009 {\{}E{\}}ntity {\{}T{\}}rack}},
year = {2010}
}
@incollection{bendersky2008passageir,
author = {Bendersky, Michael and Kurland, Oren},
booktitle = {Advances in Information Retrieval},
pages = {162--174},
publisher = {Springer},
title = {{Utilizing passage-based language models for document retrieval}},
year = {2008}
}
@article{krishnan2017leveraging,
author = {Krishnan, Adit and Deepak, P and Ranu, Sayan and Mehta, Sameep},
journal = {World Wide Web},
pages = {1--27},
publisher = {Springer},
title = {{Leveraging semantic resources in diversified query expansion}},
year = {2017}
}
@inproceedings{tonon12,
author = {Tonon, Alberto and Demartini, Gianluca and Cudr{\'{e}}-Mauroux, Philippe},
booktitle = {SIGIR},
title = {{Combining Inverted Indices and Structured Search for Ad-hoc Object Retrieval}},
year = {2012}
}
@article{bizer09,
author = {Bizer, Christian and Lehmann, Jens and Kobilarov, Georgi and Auer, S{\"{o}}ren and Becker, Christian and Cyganiak, Richard and Hellmann, Sebastian},
journal = {Journal of Web Semantics},
number = {3},
title = {{DBpedia -- {\{}A{\}} Crystallization Point for the Web of Data}},
volume = {7},
year = {2009}
}
@inproceedings{bota2014compositesearch,
author = {Bota, Horatiu and Zhou, Ke and Jose, Joemon M and Lalmas, Mounia},
booktitle = {Proceedings of the 23rd international conference on World wide web},
organization = {ACM},
pages = {119--130},
title = {{Composite retrieval of heterogeneous web search}},
year = {2014}
}
@inproceedings{pound2010objectretrieval,
author = {Pound, Jeffrey and Mika, Peter and Zaragoza, Hugo},
booktitle = {WWW},
title = {{Ad-hoc object retrieval in the web of data}},
year = {2010}
}
@incollection{aktolga2011passage,
author = {Aktolga, Elif and Allan, James and Smith, David A},
booktitle = {Advances in Information Retrieval},
pages = {617--628},
publisher = {Springer},
title = {{Passage reranking for question answering using syntactic structures and answer types}},
year = {2011}
}
@inproceedings{zhang2015topic,
author = {Zhang, Lei and Liu, Cong and Rettinger, Achim},
booktitle = {LD4IE Workshop at ISWC},
pages = {63--70},
title = {{A Topic-Sensitive Model for Salient Entity Linking.}},
year = {2015}
}
@inproceedings{robertson10,
author = {Robertson, Stephen E and Kanoulas, Evangelos and Yilmaz, Emine},
booktitle = {Proc. of SIGIR-10},
organization = {ACM},
pages = {603--610},
title = {{{\{}E{\}}xtending average precision to graded relevance judgments}},
year = {2010}
}
@inproceedings{hoffart14,
author = {Hoffart, Johannes and Milchevski, Dragan and Weikum, Gerhard},
booktitle = {SIGIR},
title = {{STICS: Searching with Strings, Things, and Cats}},
year = {2014}
}
@inproceedings{schuhmacher15,
author = {Schuhmacher, Michael and Dietz, Laura and {Paolo Ponzetto}, Simone},
booktitle = {CIKM},
title = {{{\{}R{\}}anking {\{}E{\}}ntities for {\{}W{\}}eb {\{}Q{\}}ueries through {\{}T{\}}ext and {\{}K{\}}nowledge}},
year = {2015}
}
@inproceedings{dalvi09,
author = {Dalvi, Nilesh and Kumar, Ravi and Pang, Bo and Ramakrishnan, Raghu and Tomkins, Andrew and Bohannon, Philip and Keerthi, Sathiya and Merugu, Srujana},
booktitle = {PODS},
pages = {1--12},
title = {{A web of concepts}},
year = {2009}
}
@misc{freebase:wex,
author = {Technologies, Metaweb},
howpublished = {$\backslash$url{\{}http://download.freebase.com/wex/{\}}},
title = {{Freebase Wikipedia Extraction (WEX)}},
year = {2012}
}
@article{lev65,
author = {Levenshtein, Vladimir I},
journal = {{\{}P{\}}roblems of {\{}I{\}}nformation {\{}T{\}}ransmission},
pages = {8--17},
title = {{{\{}B{\}}inary codes capable of correcting spurious insertions and deletions of ones}},
volume = {1},
year = {1965}
}
@article{metzler07,
author = {Metzler, Donald and {Bruce Croft}, W},
journal = {{\{}I{\}}nformation {\{}R{\}}etrieval},
number = {3},
pages = {257--274},
title = {{{\{}L{\}}inear {\{}F{\}}eature-based {\{}M{\}}odels for {\{}I{\}}nformation {\{}R{\}}etrieval}},
volume = {10},
year = {2007}
}
@article{egozi11,
author = {Egozi, Ofer and Markovitch, Shaul and Gabrilovich, Evgeniy},
journal = {{\{}ACM{\}} {\{}T{\}}ransactions on {\{}I{\}}nformation {\{}S{\}}ystems},
number = {2},
pages = {8:1----8:34},
title = {{{\{}C{\}}oncept-{\{}B{\}}ased {\{}I{\}}nformation {\{}R{\}}etrieval using {\{}E{\}}xplicit {\{}S{\}}emantic {\{}A{\}}nalysis}},
volume = {29},
year = {2011}
}
@article{milne13,
author = {Milne, David N and Witten, Ian H},
journal = {{\{}A{\}}rtificial {\{}I{\}}ntelligence},
pages = {222--239},
title = {{{\{}A{\}}n open-source toolkit for mining {\{}W{\}}ikipedia}},
volume = {194},
year = {2013}
}
@inproceedings{navigli2010babelnet,
author = {Navigli, Roberto and Ponzetto, Simone Paolo},
booktitle = {ACL},
pages = {216--225},
title = {{BabelNet: Building a very large multilingual semantic network}},
year = {2010}
}
@inproceedings{schuhmacher2016relations,
author = {Schuhmacher, Michael and Roth, Benjamin and Ponzetto, Simone Paolo and Dietz, Laura},
booktitle = {ECIR},
title = {{Finding Relevant Relations in Relevant Documents}},
year = {2016}
}
@inproceedings{dhingra2017towards,
author = {Dhingra, Bhuwan and Li, Lihong and Li, Xiujun and Gao, Jianfeng and Chen, Yun-Nung and Ahmed, Faisal and Deng, Li},
booktitle = {ACL},
pages = {484--495},
title = {{Towards End-to-End Reinforcement Learning of Dialogue Agents for Information Access}},
year = {2017}
}
@inproceedings{dali2011learning,
author = {Dali, Lorand and Fortuna, Bla{\v{z}}},
booktitle = {Proc. Int. Sem. Search Workshop},
title = {{Learning to rank for semantic search}},
year = {2011}
}
@article{ferrucci10,
author = {Ferrucci, David A and Brown, Eric W and Chu-Carroll, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya and Lally, Adam and Murdock, J William and Nyberg, Eric and Prager, John M and Schlaefer, Nico and Welty, Christopher A},
journal = {AI Magazine},
number = {3},
pages = {59--79},
title = {{Building {\{}W{\}}atson: An Overview of the {\{}DeepQA{\}} Project}},
volume = {31},
year = {2010}
}
@article{liu2004conceptnet,
author = {Liu, Hugo and Singh, Push},
journal = {BT technology journal},
number = {4},
pages = {211--226},
title = {{ConceptNet -- a practical commonsense reasoning tool-kit}},
volume = {22},
year = {2004}
}
@techreport{dalton2012bidirectional,
author = {Dalton, Jeffrey and Dietz, Laura},
institution = {DTIC Document},
title = {{Bi-directional linkability from Wikipedia to documents and back again: UMass at TREC 2012 knowledge base acceleration track}},
year = {2012}
}
@inproceedings{hasibi2017nordlys,
author = {Hasibi, Faegheh and Balog, Krisztian and Garigliotti, Dar$\backslash$'$\backslash$io and Zhang, Shuo},
booktitle = {SIGIR},
pages = {1289--1292},
title = {{Nordlys: A Toolkit for Entity-Oriented and Semantic Search}},
year = {2017}
}
@inproceedings{bast2014semantic,
author = {Bast, Hannah and B{\"{a}}urle, Florian and Buchhold, Bj{\"{o}}rn and Hau{\ss}mann, Elmar},
booktitle = {SIGIR},
pages = {1265--1266},
title = {{Semantic full-text search with broccoli}},
year = {2014}
}
@article{surdeanu11,
author = {Surdeanu, Mihai and Ciaramita, Massimiliano and Zaragoza, Hugo},
journal = {{\{}C{\}}omputational {\{}L{\}}inguistics},
number = {2},
pages = {351--383},
title = {{{\{}L{\}}earning to {\{}R{\}}ank {\{}A{\}}nswers to {\{}N{\}}on-{\{}F{\}}actoid {\{}Q{\}}uestions from {\{}W{\}}eb {\{}C{\}}ollections}},
volume = {37},
year = {2011}
}
@article{lopez11,
author = {Lopez, Vanessa and Uren, Victoria and Sabou, Marta and Motta, Enrico},
journal = {{\{}J{\}}ournal of {\{}S{\}}emantic {\{}W{\}}eb},
number = {2},
pages = {125--155},
title = {{{\{}I{\}}s {\{}Q{\}}uestion {\{}A{\}}nswering {\{}F{\}}it for the {\{}S{\}}emantic {\{}W{\}}eb? {\{}A{\}} {\{}S{\}}urvey}},
volume = {2},
year = {2011}
}
@article{balasubramanianC10,
author = {Balasubramanian, Niranjan and Cucerzan, Silviu},
journal = {International Journal of Semantic Computing},
number = {4},
pages = {509--534},
title = {{Beyond Ranked Lists in Web Search: Aggregating Web Content into Topic Pages.}},
volume = {4},
year = {2010}
}
@inproceedings{balaneshinkordan2016empirical,
author = {Balaneshinkordan, Saeid and Kotov, Alexander},
booktitle = {ECIR},
title = {{An Empirical Comparison of Term Association and Knowledge Graphs for Query Expansion}},
year = {2016}
}
@inproceedings{fetahu15,
author = {Fetahu, Besnik and Markert, Katja and Anand, Avishek},
booktitle = {{\{}P{\}}roc. of {\{}CIKM{\}}'15},
title = {{{\{}A{\}}utomated {\{}N{\}}ews {\{}S{\}}uggestions for {\{}P{\}}opulating {\{}W{\}}ikipedia {\{}E{\}}ntity {\{}P{\}}ages}},
year = {2015}
}
@inproceedings{dietz2017trec,
author = {Dietz, Laura and Verma, Manisha and Radlinski, Filip and Craswell, Nick},
booktitle = {TREC},
title = {{TREC Complex Answer Retrieval Overview}},
year = {2017}
}
@inproceedings{hixon2015learning,
author = {Hixon, Ben and Clark, Peter and Hajishirzi, Hannaneh},
booktitle = {NAACL-HLT},
pages = {851--861},
title = {{Learning knowledge graphs for question answering through conversational dialog}},
year = {2015}
}
@inproceedings{nikolaev2016parameterized,
author = {Nikolaev, Fedor and Kotov, Alexander and Zhiltsov, Nikita},
booktitle = {SIGIR},
title = {{Parameterized Fielded Term Dependence Models for Ad-hoc Entity Retrieval from Knowledge Graph}},
year = {2016}
}
@inproceedings{balaneshin2016sequential,
author = {Balaneshin-kordan, Saeid and Kotov, Alexander},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {155--164},
title = {{Sequential query expansion using concept graph}},
year = {2016}
}
@article{Kaptein13,
author = {Kaptein, Rianne and Kamps, Jaap},
journal = {{\{}A{\}}rtificial {\{}I{\}}ntelligence},
pages = {111--129},
title = {{{\{}E{\}}xploiting the category structure of {\{}W{\}}ikipedia for entity ranking}},
volume = {194},
year = {2013}
}
@inproceedings{meij12,
author = {Meij, Edgar and Weerkamp, Wouter and de Rijke, Maarten},
booktitle = {WSDM},
title = {{Adding Semantics to Microblog Posts}},
year = {2012}
}
@inproceedings{ferragina2010tagme,
author = {Ferragina, Paolo and Scaiella, Ugo},
booktitle = {CIKM},
pages = {1625--1628},
title = {{Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)}},
year = {2010}
}
@article{demartini10,
author = {Demartini, Gianluca and Firan, Claudiu S and Iofciu, Tereza and Krestel, Ralf and Nejdl, Wolfgang},
journal = {{\{}I{\}}nformation {\{}R{\}}etrieval},
number = {5},
pages = {534--567},
title = {{{\{}W{\}}hy finding entities in {\{}W{\}}ikipedia is difficult, sometimes}},
volume = {13},
year = {2010}
}
@inproceedings{garigliotti2017type,
author = {Garigliotti, Dar$\backslash$'$\backslash$io and Balog, Krisztian},
booktitle = {SIGIR},
pages = {27--34},
title = {{On Type-Aware Entity Retrieval}},
year = {2017}
}
@incollection{demartini2010,
author = {Demartini, Gianluca and Iofciu, Tereza and de Vries, Arjen P},
booktitle = {Focused Retrieval and Evaluation},
pages = {254--264},
publisher = {Springer},
title = {{Overview of the {\{}INEX{\}} 2009 entity ranking track}},
year = {2010}
}
@inproceedings{li2010entitysummaries,
author = {Li, Peng and Jiang, Jing and Wang, Yinglin},
booktitle = {ACL},
pages = {640--649},
title = {{Generating templates of entity summaries with an entity-aspect model and pattern mining}},
year = {2010}
}
@inproceedings{dunietz14,
author = {Dunietz, Jesse and Gillick, Daniel},
booktitle = {EACL},
title = {{A New Entity Salience Task with Millions of Training Examples}},
year = {2014}
}
@inproceedings{nie2007verticalsearch,
author = {Nie, Zaiqing and Wen, Ji-Rong and Ma, Wei-Ying},
booktitle = {CIDR},
pages = {235--246},
title = {{Object-level Vertical Search.}},
year = {2007}
}
@inproceedings{Gupta14,
author = {Gupta, Rahul and Halevy, Alon and Wang, Xuezhi and Whang, Steven and Wu, Fei},
booktitle = {Proc. of PVLDB-14},
pages = {505--516},
title = {{{\{}B{\}}iperpedia: {\{}A{\}}n {\{}O{\}}ntology for {\{}S{\}}earch {\{}A{\}}pplications}},
year = {2014}
}
@inproceedings{schuhmacher14,
author = {Schuhmacher, Michael and Ponzetto, Simone Paolo},
title = {{{\{}K{\}}nowledge-based {\{}G{\}}raph {\{}D{\}}ocument {\{}M{\}}odeling}},
year = {2014}
}
@inproceedings{fader11,
author = {Fader, Anthony and Soderland, Stephen and Etzioni, Oren},
booktitle = {EMNLP},
title = {{Identifying Relations for Open Information Extraction}},
year = {2011}
}
@inproceedings{blanco10,
author = {Blanco, Roi and Zaragoza, Hugo},
booktitle = {SIGIR},
title = {{{\{}F{\}}inding support sentences for entities}},
year = {2010}
}
@inproceedings{mitchell15,
author = {Mitchell, T and Cohen, W and Hruschka, E and Talukdar, P and Betteridge, J and Carlson, A and Dalvi, B and Gardner, M and Kisiel, B and Krishnamurthy, J and Lao, N and Mazaitis, K and Mohamed, T and Nakashole, N and Platanios, E and Ritter, A and Samadi, M and Settles, B and Wang, R and Wijaya, D and Gupta, A and Chen, X and Saparov, A and Greaves, M and Welling, J},
booktitle = {Proc. of AAAI'15},
title = {{{\{}N{\}}ever-{\{}E{\}}nding {\{}L{\}}earning}},
year = {2015}
}
@inproceedings{Wu12,
author = {Wu, Wentao and Li, Hongsong and Wang, Haixun and Zhu, Kenny},
booktitle = {Proc. of SIGMOD-12},
pages = {481--492},
title = {{{\{}P{\}}robase: {\{}A{\}} {\{}P{\}}robabilistic {\{}T{\}}axonomy for {\{}T{\}}ext {\{}U{\}}nderstanding}},
year = {2012}
}
@book{liu11,
address = {Berlin},
author = {Liu, Tie-Yan},
publisher = {Springer-Verlag},
title = {{{\{}L{\}}earning to rank for information retrieval}},
year = {2011}
}
@misc{gabrilovich13,
author = {Gabrilovich, Evgeniy and Ringgaard, Michael and Subramanya, Amarnag},
title = {{{\{}FACC{\}}1: {\{}F{\}}reebase annotation of {\{}ClueWeb{\}} corpora, {\{}V{\}}ersion 1}},
year = {2013}
}
@incollection{panem2014entitytracking,
author = {Panem, Sandeep and Bansal, Romil and Gupta, Manish and Varma, Vasudeva},
booktitle = {Advances in Information Retrieval},
pages = {528--533},
publisher = {Springer},
title = {{Entity Tracking in Real-Time using Sub-Topic Detection on Twitter}},
year = {2014}
}
@article{Sil2018,
abstract = {A major challenge in Entity Linking (EL) is making effective use of contextual information to disambiguate mentions to Wikipedia that might refer to different entities in different contexts. The problem exacerbates with cross-lingual EL which involves linking mentions written in non-English documents to entries in the English Wikipedia: to compare textual clues across languages we need to compute similarity between textual fragments across languages. In this paper, we propose a neural EL model that trains fine-grained similarities and dissimilarities between the query and candidate document from multiple perspectives, combined with convolution and tensor networks. Further, we show that this English-trained system can be applied, in zero-shot learning, to other languages by making surprisingly effective use of multi-lingual embeddings. The proposed system has strong empirical evidence yielding state-of-the-art results in English as well as cross-lingual: Spanish and Chinese TAC 2015 datasets.},
archivePrefix = {arXiv},
arxivId = {1712.01813},
author = {Sil, Avirup and Kundu, Gourab and Florian, Radu and Hamza, Wael},
eprint = {1712.01813},
file = {::},
journal = {Association for the Advancement of Artificial Intelligence (AAAI), 2018},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
month = {dec},
title = {{Neural Cross-Lingual Entity Linking}},
url = {http://arxiv.org/abs/1712.01813},
year = {2017}
}
@inproceedings{Pershina2015,
abstract = {The task of Named Entity Disambiguation is to map entity mentions in the document to their correct entries in some knowledge base. We present a novel graph-based disambiguation approach based on Personalized PageRank (PPR) that combines local and global evidence for disambiguation and effectively filters out noise introduced by incorrect candidates. Experiments show that our method outperforms state-of-the-art approaches by achieving 91.7{\%} in micro- and 89.9{\%} in macroaccuracy on a dataset of 27.8K named entity mentions},
annote = {https://github.com/masha-p/PPRforNED},
author = {Pershina, Maria and He, Yifan and Grishman, Ralph},
booktitle = {Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL},
file = {::},
isbn = {9781941643495},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {238--243},
title = {{Personalized Page Rank for Named Entity Disambiguation}},
url = {http://www.aclweb.org/anthology/N15-1026},
year = {2015}
}
@inproceedings{Yang2018,
abstract = {We present a gradient-tree-boosting-based structured learning model for jointly disambiguating named entities in a document. Gradient tree boosting is a widely used machine learning algorithm that underlies many top-performing natural language processing systems. Surprisingly, most works limit the use of gradient tree boosting as a tool for regular classification or regression problems, despite the structured nature of language. To the best of our knowledge, our work is the first one that employs the structured gradient tree boosting (SGTB) algorithm for collective entity disambiguation. By defining global features over previous disambiguation decisions and jointly modeling them with local features, our system is able to produce globally optimized entity assignments for mentions in a document. Exact inference is prohibitively expensive for our globally normalized model. To solve this problem, we propose Bidirectional Beam Search with Gold path (BiBSG), an approximate inference algorithm that is a variant of the standard beam search algorithm. BiBSG makes use of global information from both past and future to perform better local search. Experiments on standard benchmark datasets show that SGTB significantly improves upon published results. Specifically, SGTB outperforms the previous state-of-the-art neural system by near 1$\backslash${\%} absolute accuracy on the popular AIDA-CoNLL dataset.},
archivePrefix = {arXiv},
arxivId = {1802.10229},
author = {Yang, Yi and Irsoy, Ozan and Rahman, Kazi Shefaet},
booktitle = {Proceedings of NAACL-HLT 2018},
eprint = {1802.10229},
file = {::},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {777--786},
title = {{Collective Entity Disambiguation with Structured Gradient Tree Boosting}},
url = {http://aclweb.org/anthology/N18-1071 http://arxiv.org/abs/1802.10229},
year = {2018}
}
@inproceedings{Cornolti2014,
address = {New York, New York, USA},
author = {Cornolti, Marco and Ferragina, Paolo and Ciaramita, Massimiliano and Sch{\"{u}}tze, Hinrich and R{\"{u}}d, Stefan},
booktitle = {Proceedings of the first international workshop on Entity recognition {\&} disambiguation - ERD '14},
doi = {10.1145/2633211.2634348},
isbn = {9781450330237},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {25--30},
publisher = {ACM Press},
title = {{The SMAPH system for query entity recognition and disambiguation}},
url = {http://dl.acm.org/citation.cfm?doid=2633211.2634348},
year = {2014}
}
@inproceedings{Gupta,
abstract = {For accurate entity linking, we need to cap-ture various information aspects of an en-tity, such as its description in a KB, con-texts in which it is mentioned, and struc-tured knowledge. Additionally, a linking system should work on texts from different domains without requiring domain-specific training data or hand-engineered features. In this work we present a neural, mod-ular entity linking system that learns a unified dense representation for each en-tity using multiple sources of information, such as its description, contexts around its mentions, and its fine-grained types. We show that the resulting entity link-ing system is effective at combining these sources, and performs competitively, some-times out-performing current state-of-the-art systems across datasets, without requir-ing any domain-specific training data or hand-engineered features. We also show that our model can effectively " embed " en-tities that are new to the KB, and is able to link its mentions accurately.},
author = {Gupta, Nitish and Singh, Sameer and Roth, Dan},
booktitle = {EMNLP},
doi = {10.18653/v1/D17-1284},
file = {::},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {2671--2680},
title = {{Entity Linking via Joint Encoding of Types, Descriptions, and Context}},
url = {http://www.aclweb.org/anthology/D17-1284 https://pdfs.semanticscholar.org/a66b/6a3ac0aa9af6c178c1d1a4a97fd14a882353.pdf{\%}0Ahttp://aclweb.org/anthology/D17-1283},
year = {2017}
}
@inproceedings{Pappu2017,
abstract = {The pervasive presence of interconnected objects enables new communication paradigms where devices can easily reach each other while interacting within their environment. The so-called Internet of Things (IoT) represents the integration of several computing and communications systems aiming at facilitating the interaction between these devices. Arduino is one of the most popular platforms used to prototype new IoT devices due to its open, flexible and easy-to-use archi- tecture. Ardunio Yun is a dual board microcontroller that supports a Linux distribution and it is currently one of the most versatile and powerful Arduino systems. This feature positions Arduino Yun as a popular platform for developers, but it also introduces unique infection vectors from the secu- rity viewpoint. In this work, we present a security analysis of Arduino Yun. We show that Arduino Yun is vulnerable to a number of attacks and we implement a proof of concept capable of exploiting some of them.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {arXiv:1508.06655v1},
author = {Pappu, Aasish and Blanco, Roi and Mehdad, Yashar and Stent, Amanda and Thadani, Kapil},
booktitle = {Proceedings of the Tenth ACM International Conference on Web Search and Data Mining - WSDM '17},
doi = {10.1145/3018661.3018724},
eprint = {arXiv:1508.06655v1},
isbn = {9781450346757},
issn = {9781450321389},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {365--374},
publisher = {ACM Press},
title = {{Lightweight Multilingual Entity Extraction and Linking}},
url = {http://dl.acm.org/citation.cfm?doid=3018661.3018724},
year = {2017}
}
@inproceedings{Ran2018,
address = {New York, New York, USA},
author = {Ran, Chenwei and Wang, Jianyong},
booktitle = {The Web Conference (WWW)},
doi = {10.1145/3178876.3186012},
file = {::},
isbn = {9781450356398},
keywords = {Twitter,attention- tweet entity linking,entity linking,entity{\_}linking,factor g,factor graph model,for example,found,have been,knowledge graph,signal,to be a powerful,twitter,user interest is shown,various novel tweet-specific signals},
mendeley-tags = {entity{\_}linking},
pages = {1135--1144},
publisher = {ACM Press},
title = {{An Attention Factor Graph Model for Tweet Entity Linking}},
url = {http://dl.acm.org/citation.cfm?doid=3178876.3186012},
volume = {2},
year = {2018}
}
@article{Zhu2018,
abstract = {With the increasing popularity of large scale Knowledge Graph (KG)s, many applications such as semantic analysis, search and question answering need to link entity mentions in texts to entities in KGs. Because of the polysemy problem in natural language, entity disambiguation is thus a key problem in current research. Existing disambiguation methods have considered entity prominence, context similarity and entity-entity relatedness to discriminate ambiguous entities, which are mainly working on document or paragraph level texts containing rich contextual information, and based on lexical matching for computing context similarity. When meeting short texts containing limited contextual information, such as web queries, questions and tweets, those conventional disambiguation methods are not good at handling single entity mention and measuring context similarity. In order to enhance the performance of disambiguation methods based on context similarity with such short texts, we propose SCSNED method for disambiguation based on semantic similarity between contextual words and informative words of entities in KGs. Specially, we exploit the effectiveness of both knowledge-based and corpus-based semantic similarity methods for entity disambiguation with SCSNED. Moreover, we propose a Category2Vec embedding model based on joint learning of word and category embedding, in order to compute word-category similarity for entity disambiguation. We show the effectiveness of these proposed methods with illustrative examples, and evaluate their effectiveness in a comparative experiment for entity disambiguation in real world web queries, questions and tweets. The experimental results have identified the effectiveness of different semantic similarity methods, and demonstrated the improvement of semantic similarity methods in SCSNED and Category2Vec over the conventional context similarity baseline. We further compare the proposed approaches with the state of the art entity disambiguation systems and show the performances of the proposed approaches are among the best performing systems. In addition, one important feature of the proposed approaches using semantic similarity, is the potential application on any existing KGs since they mainly use common features of entity descriptions and categories. Another contribution of the paper is an updated survey on background of entity disambiguation in KGs and semantic similarity methods.},
author = {Zhu, Ganggao and Iglesias, Carlos A.},
doi = {10.1016/J.ESWA.2018.02.011},
file = {::},
journal = {Expert Systems with Applications},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
month = {jul},
pages = {8--24},
publisher = {Pergamon},
title = {{Exploiting semantic similarity for named entity disambiguation in knowledge graphs}},
url = {https://www.sciencedirect.com/science/article/pii/S0957417418300897},
volume = {101},
year = {2018}
}
@inproceedings{Lv2015,
address = {New York, New York, USA},
author = {Lv, Yuanhua and Fuxman, Ariel},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval - SIGIR '15},
doi = {10.1145/2766462.2767696},
isbn = {9781450336215},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {655--664},
publisher = {ACM Press},
title = {{In Situ Insights}},
url = {http://dl.acm.org/citation.cfm?doid=2766462.2767696},
year = {2015}
}
@article{Yamada2018,
author = {Yamada, Ikuya and Ito, Tomotaka and Takeda, Hideaki and Takefuji, Yoshiyasu},
doi = {10.1109/MIS.2018.111144233},
journal = {IEEE Intelligent Systems},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {1--1},
title = {{Linkify: Enhancing Text Reading Experience by Detecting and Linking Helpful Entities to Users}},
url = {http://ieeexplore.ieee.org/document/8255781/},
year = {2018}
}
@inproceedings{Liao2017,
address = {New York, New York, USA},
author = {Liao, Zhen and Song, Xinying and Shen, Yelong and Lee, Saekoo and Gao, Jianfeng and Liao, Ciya},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management - CIKM '17},
doi = {10.1145/3132847.3132856},
isbn = {9781450349185},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
pages = {1757--1765},
publisher = {ACM Press},
title = {{Deep Context Modeling for Web Query Entity Disambiguation}},
url = {http://dl.acm.org/citation.cfm?doid=3132847.3132856},
year = {2017}
}
@article{Liu2017,
author = {Liu, Ming and Chen, Lei and Liu, Bingquan and Zheng, Guidong and Zhang, Xiaoming},
doi = {10.1145/3086703},
journal = {ACM Transactions on Information Systems},
keywords = {entity{\_}linking},
mendeley-tags = {entity{\_}linking},
month = {aug},
number = {2},
pages = {1--34},
publisher = {ACM},
title = {{DBpedia-Based Entity Linking via Greedy Search and Adjusted Monte Carlo Random Walk}},
url = {http://dl.acm.org/citation.cfm?doid=3133943.3086703},
volume = {36},
year = {2017}
}
@inproceedings{Hasibi2015a,
address = {New York, New York, USA},
author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
booktitle = {Proceedings of the 2015 International Conference on Theory of Information Retrieval - ICTIR '15},
doi = {10.1145/2808194.2809473},
isbn = {9781450338332},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {171--180},
publisher = {ACM Press},
title = {{Entity Linking in Queries}},
url = {http://dl.acm.org/citation.cfm?doid=2808194.2809473},
year = {2015}
}
@article{Guo2013b,
author = {Guo and Qin, B and Liu, T and Li, S},
isbn = {9781937284978},
journal = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
keywords = {entity linking},
mendeley-tags = {entity linking},
number = {October},
pages = {863--868},
title = {{Microblog entity linking by leveraging extra posts}},
year = {2013}
}
@article{Gattani2013,
author = {Gattani, Abhishek and Doan, AnHai and Lamba, Digvijay S. and Garera, Nikesh and Tiwari, Mitul and Chai, Xiaoyong and Das, Sanjib and Subramaniam, Sri and Rajaraman, Anand and Harinarayan, Venky},
doi = {10.14778/2536222.2536237},
isbn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {aug},
number = {11},
pages = {1126--1137},
title = {{Entity extraction, linking, classification, and tagging for social media}},
url = {http://dl.acm.org/citation.cfm?doid=2536222.2536237},
volume = {6},
year = {2013}
}
@inproceedings{Spitz2016,
address = {New York, New York, USA},
author = {Spitz, Andreas and Gertz, Michael},
booktitle = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval - SIGIR '16},
doi = {10.1145/2911451.2911529},
isbn = {9781450340694},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {503--512},
publisher = {ACM Press},
title = {{Terms over LOAD}},
url = {http://dl.acm.org/citation.cfm?doid=2911451.2911529},
year = {2016}
}
@inproceedings{Prasojo2015,
address = {New York, New York, USA},
author = {Prasojo, Radityo Eko and Kacimi, Mouna and Nutt, Werner},
booktitle = {Proceedings of the 24th ACM International on Conference on Information and Knowledge Management - CIKM '15},
doi = {10.1145/2806416.2806576},
isbn = {9781450337946},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {233--242},
publisher = {ACM Press},
title = {{Entity and Aspect Extraction for Organizing News Comments}},
url = {http://dl.acm.org/citation.cfm?doid=2806416.2806576},
year = {2015}
}
@inproceedings{Xiong2016,
address = {New York, New York, USA},
author = {Xiong, Chenyan and Callan, Jamie and Liu, Tie-Yan},
booktitle = {Proceedings of the 2016 ACM on International Conference on the Theory of Information Retrieval - ICTIR '16},
doi = {10.1145/2970398.2970423},
isbn = {9781450344975},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {181--184},
publisher = {ACM Press},
title = {{Bag-of-Entities Representation for Ranking}},
url = {http://dl.acm.org/citation.cfm?doid=2970398.2970423},
year = {2016}
}
@inproceedings{Hasibi2016,
address = {New York, New York, USA},
author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
booktitle = {Proceedings of the 2016 ACM on International Conference on the Theory of Information Retrieval - ICTIR '16},
doi = {10.1145/2970398.2970406},
isbn = {9781450344975},
keywords = {entity linking,entity retrieval},
mendeley-tags = {entity linking,entity retrieval},
pages = {209--218},
publisher = {ACM Press},
title = {{Exploiting Entity Linking in Queries for Entity Retrieval}},
url = {http://dl.acm.org/citation.cfm?doid=2970398.2970406},
year = {2016}
}
@article{Chisholm2015,
abstract = {Entity disambiguation with Wikipedia relies on structured information from redirect pages, article text, inter-article links, and categories. We explore whether web links can replace a curated encyclopaedia, obtaining entity prior, name, context, and coherence models from a corpus of web pages with links to Wiki- pedia. Experiments compare web link models to Wikipedia models on well-known CoNLL and TAC data sets. Results show that using 34 million web links approachesWikipedia performance. Combin- ing web link and Wikipedia models produces the best-known disambiguation accuracy of 88.7 on standard newswire test data. 1},
author = {Chisholm, Andrew and Hachey, Ben},
file = {::},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
number = {0},
pages = {145--156},
title = {{Entity Disambiguation with Web Links}},
volume = {3},
year = {2015}
}
@inproceedings{Wang2014,
abstract = {We examine the embedding approach to reason new relational facts from a large- scale knowledge graph and a text corpus. We propose a novel method of jointly em- bedding entities and words into the same continuous vector space. The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus. Entity names and Wikipedia an- chors are utilized to align the embeddings of entities and words in the same space. Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts, compared to separately embedding knowledge graphs and text. Particularly, jointly embedding enables the prediction of facts containing entities out of the knowledge graph, which cannot be han- dled by previous embedding methods. At the same time, concerning the quality of the word embeddings, experiments on the analogical reasoning task showthat jointly embedding is comparable to or slightly better than word2vec (Skip-Gram).},
author = {Wang, Zhen and Zhang, Jianwen and Feng, Jianlin and Chen, Zheng},
booktitle = {EMNLP '14},
pages = {1591--1601},
title = {{Knowledge Graph and Text Jointly Embedding}},
year = {2014}
}
@inproceedings{Zwicklbauer2016c,
abstract = {Entity disambiguation is the task of mapping ambiguous terms in natural-language text to its entities in a knowledge base. It finds its application in the extraction of structured data in RDF (Resource Description Framework) from textual documents, but equally so in facilitating artificial intelligence applications, such as Semantic Search, Reasoning and Question {\&} Answering. In this work, we propose DoSeR (Disambiguation of Semantic Resources), a (named) entity disambiguation framework that is knowledge-base-agnostic in terms of RDF (e.g. DBpedia) and entity-annotated document knowledge bases (e.g. Wikipedia). Initially, our framework automatically generates semantic entity embeddings given one or multiple knowledge bases. In the following, DoSeR accepts documents with a given set of surface forms as input and collectively links them to an entity in a knowledge base with a graph-based approach. We evaluate DoSeR on seven different data sets against publicly available, state-of-the-art (named) entity disambiguation frameworks. Our approach outperforms the state-of-the-art approaches that make use of RDF knowledge bases and/or entity-annotated document knowledge bases by up to 10 {\%} F1 measure.},
address = {Cham},
author = {Zwicklbauer, Stefan and Seifert, Christin and Granitzer, Michael},
booktitle = {The Semantic Web. Latest Advances and New Domains - 13th International Conference, ESWC 2016},
editor = {Sack, Harald and Blomqvist, Eva and D'Aquin, Mathieu and Ghidini, Chiara and Ponzetto, Simone Paolo and Lange, Christoph},
keywords = {entity linking},
language = {en},
mendeley-tags = {entity linking},
month = {may},
pages = {182--198},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{DoSeR - A Knowledge-Base-Agnostic Framework for Entity Disambiguation Using Semantic Embeddings}},
url = {http://link.springer.com/chapter/10.1007/978-3-319-34129-3{\_}12/fulltext.html},
volume = {9678},
year = {2016}
}
@article{Usbeck2015,
abstract = {The need to bridge between the unstructured data on the Document Web and the structured data on the Web of Data has led to the development of a considerable number of annotation tools. However, these tools are currently still hard to compare since the published evaluation results are calculated on diverse datasets and evaluated based on different measures. We present GERBIL, an evaluation framework for semantic entity annotation. The rationale behind our framework is to provide developers, end users and researchers with easy-to-use interfaces that allow for the agile, fine-grained and uniform evaluation of annotation tools on multiple datasets. By these means, we aim to ensure that both tool developers and end users can derive meaningful insights pertaining to the extension, integration and use of annotation applications. In particular, GERBIL provides comparable results to tool developers so as to allow them to easily discover the strengths and weaknesses of their implementations with respect to the state of the art. With the permanent experiment URIs provided by our framework, we ensure the reproducibility and archiving of evaluation results. Moreover, the framework generates data in machineprocessable format, allowing for the efficient querying and post-processing of evaluation results. Finally, the tool diagnostics provided by GERBIL allows deriving insights pertaining to the areas in which tools should be further refined, thus allowing developers to create an informed agenda for extensions and end users to detect the right tools for their purposes. GERBIL aims to become a focal point for the state of the art, driving the research agenda of the community by presenting comparable objective evaluation results.},
author = {Usbeck, Ricardo and R{\"{o}}der, Michael and Ngomo, Axel-Cyrille Ngonga and Baron, Ciro and Both, Andreas and Br{\"{u}}mmer, Martin and Ceccarelli, Diego and Cornolti, Marco and Cherix, Didier and Eickmann, Bernd and Ferragina, Paolo and Lemke, Christiane and Moro, Andrea and Navigli, Roberto and Piccinno, Francesco and Rizzo, Giuseppe and Sack, Harald and Speck, Ren{\'{e}} and Troncy, Rapha{\"{e}}l and Waitelonis, J{\"{o}}rg and Wesemann, Lars},
doi = {10.1145/2736277.2741626},
isbn = {9781450334693},
journal = {Proceedings of the 24th International Conference on World Wide Web, WWW 2015, Florence, Italy, May 18-22, 2015},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {1133--1143},
title = {{GERBIL  General Entity Annotator Benchmarking Framework}},
url = {http://doi.acm.org/10.1145/2736277.2741626},
year = {2015}
}
@inproceedings{Zwicklbauer2016a,
address = {New York, New York, USA},
author = {Zwicklbauer, Stefan and Seifert, Christin and Granitzer, Michael},
booktitle = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval - SIGIR '16},
keywords = {embeddings,entity disambiguation,entity linking,neuronal networks},
mendeley-tags = {entity linking},
month = {jul},
pages = {425--434},
publisher = {ACM Press},
title = {{Robust and Collective Entity Disambiguation through Semantic Embeddings}},
url = {http://dl.acm.org/citation.cfm?id=2911451.2911535},
year = {2016}
}
@inproceedings{Trani2014,
abstract = {Entity Linking (EL) enables to automatically link unstruc-tured data with entities in a Knowledge Base. Linking unstructured data (like news, blog posts, tweets) has several important applications: for ex-ample it allows to enrich the text with external useful contents or to improve the categorization and the retrieval of documents. In the latest years many effective approaches for performing EL have been proposed but only a few authors published the code to perform the task. In this work we describe Dexter 2.0, a major revision of our open source frame-work to experiment with different EL approaches. We designed Dexter in order to make it easy to deploy and to use. The new version provides several important features: the possibility to adopt different EL strate-gies at run-time and to annotate semi-structured documents, as well as a well-documented REST-API. In this demo we present the current state of the system, the improvements made, its architecture and the APIs provided.},
author = {Trani, Salvatore and Ceccarelli, Diego and Lucchese, Claudio and Orlando, Salvatore and Perego, Raffaele},
booktitle = {CEUR Workshop Proceedings},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {417--420},
publisher = {CEUR-WS},
title = {{Dexter 2.0 - An open source tool for semantically enriching data}},
volume = {1272},
year = {2014}
}
@article{Hasibi2015,
abstract = {Annotating queries with entities is one of the core problem areas in query understanding. While seeming similar, the task of entity linking in queries is different from entity linking in documents and requires a methodological departure due to the inherent ambiguity of queries. We differentiate between two specific tasks, semantic mapping and interpretation finding, discuss current evaluation methodology, and propose refinements. We examine publicly available datasets for these tasks and introduce a new manually curated dataset for interpretation finding. To further deepen the understanding of task differences, we present a set of approaches for effectively addressing these tasks and report on experimental results.},
address = {New York, New York, USA},
author = {Hasibi, Faegheh and Balog, Krisztian and Bratsberg, Svein Erik},
doi = {10.1145/2808194.2809473},
isbn = {978-1-4503-3833-2},
journal = {Proceedings of the 2015 International Conference on The Theory of Information Retrieval},
keywords = {entity linking,interpretation finding,query understanding,semantic mapping},
mendeley-tags = {entity linking},
pages = {171--180},
publisher = {ACM Press},
title = {{Entity Linking in Queries: Tasks and Evaluation}},
url = {http://dl.acm.org/citation.cfm?doid=2808194.2809473 http://doi.acm.org/10.1145/2808194.2809473},
year = {2015}
}
@article{Piccinno2014,
abstract = {In this paper we propose a novel entity annotator for texts which hinges on Tagme's algorithmic technology, currently the best one available [6, 4]. The novelty is twofold: from the one hand, we have engineered the software in order to be modular and more efficient; from the other hand, we have improved the annotation pipeline by re-designing all of its three main modules: spotting, disambiguation and pruning. In particular, the re-design has involved the detailed inspection of the performance of these modules by developing new algorithms which have been in turn tested over all publicly available datasets (i.e. AIDA, IITB, MSN, AQUAINT, and the one of the ERD Challenge). This extensive experimentation allowed us to derive the best combination which achieved on the ERD development dataset an F1 score of 74.8{\%}, which turned to be 67.2{\%} F1 for the test dataset. This final result was due to an impressive precision equal to 87.6{\%}, but very low recall 54.5{\%}. With respect to classic Tagme on the development dataset the improvement ranged from 1{\%} to 9{\%} on the D2W benchmark, depending on the disambiguation algorithm being used. As a side result, the final software can be interpreted as a flexible library of several parsing/disambiguation and pruning modules that can be used to build up new and more sophisticated entity annotators. We plan to release our library to the public as an open-source project.},
address = {New York, New York, USA},
author = {Piccinno, Francesco and Ferragina, Paolo},
doi = {10.1145/2633211.2634350},
isbn = {9781450330237},
issn = {9781450330237},
journal = {Proceedings of ERD},
keywords = {entity annotation,entity linking,graph-based algo-,tagme,wikipedia},
mendeley-tags = {entity linking},
pages = {1--7},
publisher = {ACM Press},
title = {{From TagME to WAT: A New Entity Annotator Categories and Subject Descriptors}},
url = {http://dl.acm.org/citation.cfm?doid=2633211.2634350},
year = {2014}
}
@inproceedings{Yamada2014a,
abstract = {When we encounter an interesting entity (e.g., a person's name or a geographic location) while reading text, we typically search and retrieve relevant information about it. Entity linking (EL) is the task of linking entities in a text to the corresponding entries in a knowledge base, such as Wikipedia. Recently, EL has received considerable attention. EL can be used to enhance a user's text reading experience by streamlining the process of retrieving information on entities. Several EL methods have been proposed, though they tend to extract all of the entities in a document including unnecessary ones for users. Excessive linking of entities can be distracting and degrade the user experience. In this paper, we propose a new method for evaluating the helpfulness of linking entities to users. We address this task using supervised machine-learning with a broad set of features. Experimental results show that our method significantly outperforms baseline methods by approximately 5.7{\%}-12{\%} F1. In addition, we propose an application, Linkify, which enables developers to integrate EL easily into their web sites.},
address = {New York, New York, USA},
author = {Yamada, Ikuya and Ito, Tomotaka and Usami, Shinnosuke and Takagi, Shinsuke and Takeda, Hideaki and Takefuji, Yoshiyasu},
booktitle = {Proceedings of the 25th ACM conference on Hypertext and social media - HT '14},
doi = {10.1145/2631775.2631802},
file = {::},
isbn = {9781450329545},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {169--178},
publisher = {ACM Press},
title = {{Evaluating the helpfulness of linked entities to readers}},
url = {http://dl.acm.org/citation.cfm?doid=2631775.2631802},
year = {2014}
}
@article{Burdick2016,
abstract = {We introduce and develop a declarative framework for entity linking and, in particular, for entity resolution. As in some earlier approaches, our framework is based on a systematic use of constraints. However, the constraints we adopt are link-to-source constraints, unlike in earlier approaches where source-to-link constraints were used to dictate how to generate links. Our approach makes it possible to focus entirely on the intended properties of the outcome of entity linking, thus separating the constraints from any procedure of how to achieve that outcome. The core language consists of link-to-source constraints that specify the desired properties of a link relation in terms of source relations and built-in predicates such as similarity measures. A key feature of the link-to-source constraints is that they employ disjunction, which enables the declarative listing of all the reasons two entities should be linked. We also consider extensions of the core language that capture collective entity resolution by allowing interdependencies among the link relations.

We identify a class of good solutions for entity-linking specifications, which we call maximum-value solutions and which capture the strength of a link by counting the reasons that justify it. We study natural algorithmic problems associated with these solutions, including the problem of enumerating the good solutions and the problem of finding the certain links, which are the links that appear in every good solution. We show that these problems are tractable for the core language but may become intractable once we allow interdependencies among the link relations. We also make some surprising connections between our declarative framework, which is deterministic, and probabilistic approaches such as ones based on Markov Logic Networks.},
author = {Burdick, Douglas and Fagin, Ronald and Kolaitis, Phokion G. and Popa, Lucian and Tan, Wang-Chiew},
doi = {10.1145/2894748},
file = {::},
journal = {ACM Transactions on Database Systems},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {jul},
number = {3},
pages = {1--38},
publisher = {ACM},
title = {{A Declarative Framework for Linking Entities}},
url = {http://dl.acm.org/citation.cfm?doid=2966276.2894748},
volume = {41},
year = {2016}
}
@article{Ganea2015a,
abstract = {The goal of entity linking is to map spans of text to canonical entity representations such as Freebase entries or Wikipedia articles. It provides a foundation for various natural language processing tasks, including text understanding, summarization and machine translation. Name ambiguity, word polysemy, context dependencies, and a heavy-tailed distribution of entities contribute to the complexity of this problem. We propose a simple, yet effective, probabilistic graphical model for collective entity linking, which resolves entity links jointly across an entire document. Our model captures local information from linkable token spans (i.e., mentions) and their surrounding context and combines it with a document-level prior of entity co-occurrences. The model is acquired automatically from entity-linked text repositories with a lightweight computational step for parameter adaptation. Loopy belief propagation is used as an efficient approximate inference algorithm. In contrast to state-of-the-art methods, our model is conceptually simple and easy to reproduce. It comes with a small memory footprint and is sufficiently fast for real-time usage. We demonstrate its benefits on a wide range of well-known entity linking benchmark datasets. Our empirical results show the merits of the proposed approach and its competitiveness in comparison to state-of-the-art methods.},
annote = {Code at https://github.com/dalab/pboh-entity-linking.},
archivePrefix = {arXiv},
arxivId = {1509.02301},
author = {Ganea, Octavian-Eugen and Horlescu, Marina and Lucchi, Aurelien and Eickhoff, Carsten and Hofmann, Thomas},
doi = {10.1145/2872427.2882988},
eprint = {1509.02301},
file = {::},
isbn = {9781450341431},
journal = {Proceedings of the 25th International Conference on World Wide Web},
keywords = {abilistic graphical models,approximate inference,entity disambiguation,entity linking,loopy,prob-,wikification},
mendeley-tags = {entity linking},
pages = {927--938},
publisher = {International World Wide Web Conferences Steering Committee},
title = {{Probabilistic Bag-Of-Hyperlinks Model for Entity Linking}},
url = {http://arxiv.org/abs/1509.02301},
year = {2015}
}
@article{Raviv2016,
abstract = {We address the ad hoc document retrieval task by devising novel types of entity-based language models. The models utilize information about single terms in the query and documents as well as term sequences marked as entities by some entity-linking tool. The key principle of the language models is accounting, simultaneously, for the uncertainty inherent in the entity-markup process and the balance between using entity-based and term-based information. Empirical evaluation demonstrates the merits of using the language models for retrieval. For example, the performance transcends that of a state-of-the-art term proximity method. We also show that the language models can be effectively used for cluster-based document retrieval and query expansion.},
address = {New York, New York, USA},
author = {Raviv, Hadas and Kurland, Oren and Carmel, David},
doi = {10.1145/2911451.2911508},
isbn = {9781450340694},
journal = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval - SIGIR '16},
keywords = {document retrieval,entity linking,entity-based language models},
mendeley-tags = {entity linking},
pages = {65--74},
publisher = {ACM Press},
title = {{Document Retrieval Using Entity-Based Language Models}},
url = {http://dl.acm.org/citation.cfm?doid=2911451.2911508},
year = {2012}
}
@article{Cornolti2016,
abstract = {In this paper we study the problem of linking open-domain web-search queries towards entities drawn from the full entity inventory of Wikipedia articles. We introduce SMAPH-2, a second-order approach that, by piggybacking on a web search engine, alleviates the noise and irregularities that characterize the language of queries and puts queries in a larger context in which it is easier to make sense of them. The key algorithmic idea underlying SMAPH-2 is to first discover a candidate set of entities and then link-back those entities to their mentions occurring in the input query. This allows us to confine the possible concepts pertinent to the query to only the ones really mentioned in it. The link-back is implemented via a collective disambiguation step based upon a supervised ranking model that makes one joint prediction for the annotation of the complete query optimizing directly the F1 measure. We evaluate both known features, such as word embeddings and semantic relatedness among entities, and several novel features such as an approximate distance between mentions and entities (which can handle spelling errors). We demonstrate that SMAPH-2 achieves state-of-the-art performance on the ERD@SIGIR2014 benchmark. We also publish GERDAQ (General Entity Recognition, Disambiguation and Annotation in Queries), a novel, public dataset built specifically for web-query entity linking via a crowdsourcing effort. SMAPH-2 outperforms the benchmarks by comparable margins also on GERDAQ.},
annote = {System at https://github.com/marcocor/smaph-erd.

Dataset at http://acube.di.unipi.it/datasets/.},
author = {Cornolti, Marco and Ferragina, Paolo and Ciaramita, Massimiliano and R{\"{u}}d, Stefan and Sch{\"{u}}tze, Hinrich},
doi = {10.1145/2872427.2883061},
file = {::},
isbn = {978-1-4503-4143-1},
journal = {Proceedings of the 25th International Conference on World Wide Web},
keywords = {entity linking,erd,piggyback,query annotation},
mendeley-tags = {entity linking},
pages = {567--578},
publisher = {International World Wide Web Conferences Steering Committee},
title = {{A Piggyback System for Joint Entity Mention Detection and Linking in Web Queries}},
url = {http://dx.doi.org/10.1145/2872427.2883061},
year = {2016}
}
@inproceedings{Yang2015,
abstract = {Non-linear models recently receive a lot of attention as people are starting to dis-cover the power of statistical and em-bedding features. However, tree-based models are seldom studied in the con-text of structured learning despite their re-cent success on various classification and ranking tasks. In this paper, we propose S-MART, a tree-based structured learning framework based on multiple additive re-gression trees. S-MART is especially suit-able for handling tasks with dense fea-tures, and can be used to learn many dif-ferent structures under various loss func-tions. We apply S-MART to the task of tweet entity linking  a core component of tweet information extraction, which aims to identify and link name mentions to en-tities in a knowledge base. A novel infer-ence algorithm is proposed to handle the special structure of the task. The exper-imental results show that S-MART signif-icantly outperforms state-of-the-art tweet entity linking systems.},
author = {Yang, Yi and Chang, Ming-Wei},
booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {504--513},
title = {{S-MART: Novel Tree-based Structured Learning Algorithms Applied to Tweet Entity Linking}},
url = {http://www.aclweb.org/anthology/P15-1049},
year = {2015}
}
@inproceedings{Houlsby2014,
abstract = {Entity linking involves labeling phrases in text with their referent entities, such as Wikipedia or Freebase entries. This task is challenging due to the large number of possible entities, in the millions, and heavy-tailed mention ambiguity. We formulate the problem in terms of probabilistic inference within a topic model, where each topic is associated with a Wikipedia article. To deal with the large number of topics we propose a novel efficient Gibbs sampling scheme which can also incorporate side information, such as the Wikipedia graph. This conceptually simple probabilistic approach achieves state-of-the-art performance in entity-linking on the Aida-CoNLL dataset.},
author = {Houlsby, Neil and Ciaramita, Massimiliano},
booktitle = {Advances in Information Retrieval (ECIR 2014)},
doi = {10.1007/978-3-319-06028-6_28},
isbn = {978-3-319-06027-9},
issn = {16113349},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {335--346},
title = {{A Scalable Gibbs Sampler for Probabilistic Entity Linking}},
url = {http://research.google.com/pubs/pub42453.html},
year = {2014}
}
@article{Carmel2014,
abstract = {In this paper we overview the 2014 Entity Recognition and Disambiguation Challenge (ERD'14), which took place from March to June 2014 and was summarized in a dedicated workshop at SIGIR 2014. The main goal of the ERD challenge was to promote research in recognition and disambiguation of entities in unstructured text. Unlike many past entity linking challenges, no mention segmentations were given to the participating systems for a given document. Participants were asked to implement a web service for their system to minimize human involvement during evaluation and to enable measuring the processing times. The challenge has attracted a lot of interest (over 100 teams registered, and 27 of those submitted final results). In this paper we cover the task definition, issues encountered during annotation, and provide a detailed analysis of all the participating systems. Specifically, we show how we adapted the pooling technique to address the difficulties of gathering annotations for the entity linking task. We also summarize the ERD workshop that followed the challenge, including the oral and poster presentations as well as the invited talks.},
author = {Carmel, David and Hsu, Bo-June (Paul) and Wang, Kuansan},
doi = {10.1145/2600428.2600734},
isbn = {978-1-4503-2257-7},
issn = {01635840},
journal = {SIGIR Forum},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {dec},
number = {2},
pages = {63--77},
publisher = {ACM},
title = {{ERD'14 : Entity Recognition and Disambiguation Challenge}},
url = {http://dl.acm.org/citation.cfm?doid=2701583.2701591},
volume = {48},
year = {2014}
}
@article{Guo2014,
abstract = {Entity Linking is the task of assigning entities from a Knowledge Base to textual mentions of such entities in a document. State-of-the-art approaches rely on lexical and statistical features which are abundant for popular entities but sparse for unpopular ones, resulting in a clear bias towards popular entities and poor accuracy for less popular ones. In this work, we present a novel approach that is guided by a natural notion of semantic similarity which is less amenable to such bias. We adopt a unified semantic representation for entities and documents - the probability distribution obtained from a random walk on a subgraph of the knowledge base - which can overcome the feature sparsity issue that affects previous work. Our algorithm continuously updates the semantic signature of the document as mentions are disambiguated, thus focusing the search based on context. Our experimental evaluation uses well-known benchmarks and different samples of a Wikipedia-based benchmark with varying entity popularity; the results illustrate well the bias of previous methods and the superiority of our approach, especially for the less popular entities.},
address = {New York, New York, USA},
author = {Guo, Zhaochen and Barbosa, Denilson},
doi = {10.1145/2661829.2661887},
isbn = {9781450325981},
journal = {Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management - CIKM '14},
keywords = {entity linking,random walk,relatedness measure},
mendeley-tags = {entity linking},
pages = {499--508},
publisher = {ACM Press},
title = {{Robust Entity Linking via Random Walks}},
url = {http://dl.acm.org/citation.cfm?doid=2661829.2661887 http://dl.acm.org/citation.cfm?id=2661829.2661887},
year = {2014}
}
@inproceedings{Durrett2014a,
abstract = {We present a joint model of three core tasks in the entity analysis stack: coreference resolution (within-document clustering), named entity recognition (coarse semantic typing), and entity linking (matching to Wikipedia entities). Our model is formally a structured conditional random field. Unary factors encode local features from strong baselines for each task. We then add binary and ternary factors to capture cross-task interactions, such as the constraint that coreferent mentions have the same semantic type. On the ACE 2005 and OntoNotes datasets, we achieve state-of-the-art results for all three tasks. Moreover, joint modeling improves performance on each task over strong independent baselines.},
annote = {System available at: https://github.com/gregdurrett/berkeley-entity.},
author = {Durrett, Greg and Klein, Dan},
booktitle = {Transactions of the Association for Computational Linguistics},
issn = {2307-387X},
keywords = {entity linking},
language = {en},
mendeley-tags = {entity linking},
month = {nov},
pages = {477--490},
title = {{A Joint Model for Entity Analysis: Coreference, Typing, and Linking}},
url = {https://transacl.org/ojs/index.php/tacl/article/view/412},
volume = {2},
year = {2014}
}
@article{Shen2015,
abstract = {The large number of potential applications from bridging Web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.},
author = {Shen, Wei and Wang, Jianyong and Han, Jiawei},
doi = {10.1109/TKDE.2014.2327028},
isbn = {1041-4347 VO - 27},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Entity linking,entity disambiguation,entity linking,knowledge base},
mendeley-tags = {entity linking},
month = {feb},
number = {2},
pages = {443--460},
pmid = {1609378},
title = {{Entity linking with a knowledge base: Issues, techniques, and solutions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6823700},
volume = {27},
year = {2015}
}
@article{Moro2014,
abstract = {Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language. But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense. In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations. Our experiments show state-of- the-art performances on both tasks on 6 different datasets, including a multilingual setting. Babelfy is online at http://babelfy.org},
author = {Moro, Andrea and Raganato, Alessandro and Navigli, Roberto},
file = {::},
journal = {Transactions of the Association for Computational Linguistics (TACL)},
keywords = {entity linking},
mendeley-tags = {entity linking},
number = {0},
pages = {231--244},
title = {{Entity Linking meets Word Sense Disambiguation: a Unified Approach}},
volume = {2},
year = {2014}
}
@inproceedings{Ceccarelli2013,
address = {New York, New York, USA},
author = {Ceccarelli, Diego and Lucchese, Claudio and Orlando, Salvatore and Perego, Raffaele and Trani, Salvatore},
booktitle = {Proceedings of the 22nd ACM international conference on Conference on information {\&} knowledge management - CIKM '13},
doi = {10.1145/2505515.2505711},
isbn = {9781450322638},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {oct},
pages = {139--148},
publisher = {ACM Press},
title = {{Learning relatedness measures for entity linking}},
url = {http://dl.acm.org/citation.cfm?id=2505515.2505711},
year = {2013}
}
@inproceedings{Blanco2015,
abstract = {Entity linking deals with identifying entities from a knowledge base in a given piece of text and has become a fundamental building block for web search engines, enabling numerous downstream improvements from better document ranking to enhanced search results pages. A key problem in the context of web search queries is that this process needs to run under severe time constraints as it has to be performed before any actual retrieval takes place, typically within milliseconds. In this paper we propose a probabilistic model that leverages user-generated information on the web to link queries to entities in a knowledge base. There are three key ingredients that make the algorithm fast and space-efficient. First, the linking process ignores any dependencies between the different entity candidates, which allows for a O(k2) implementation in the number of query terms. Second, we leverage hashing and compression techniques to reduce the memory footprint. Finally, to equip the algorithm with contextual knowledge without sacrificing speed, we factor the distance between distributional semantics of the query words and entities into the model. We show that our solution significantly outperforms several state-of-the-art baselines by more than 14{\%} while being able to process queries in sub-millisecond times---at least two orders of magnitude faster than existing systems.},
address = {New York, New York, USA},
author = {Blanco, Roi and Ottaviano, Giuseppe and Meij, Edgar},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining - WSDM '15},
doi = {10.1145/2684822.2685317},
isbn = {9781450333177},
keywords = {entity linking,queries,web search,wikipedia},
mendeley-tags = {entity linking},
month = {feb},
pages = {179--188},
publisher = {ACM Press},
title = {{Fast and Space-Efficient Entity Linking for Queries}},
url = {http://dl.acm.org/citation.cfm?id=2684822.2685317},
year = {2015}
}
@inproceedings{Blanco2015a,
address = {New York, New York, USA},
author = {Blanco, Roi and Ottaviano, Giuseppe and Meij, Edgar},
booktitle = {Proceedings of the Eighth ACM International Conference on Web Search and Data Mining - WSDM '15},
keywords = {entity linking,queries,web search,wikipedia},
mendeley-tags = {entity linking},
month = {feb},
pages = {179--188},
publisher = {ACM Press},
title = {{Fast and Space-Efficient Entity Linking for Queries}},
url = {http://dl.acm.org/citation.cfm?id=2684822.2685317},
year = {2015}
}
@inproceedings{Dalton2014a,
address = {New York, New York, USA},
author = {Dalton, Jeffrey and Dietz, Laura and Allan, James},
booktitle = {Proceedings of the 37th international ACM SIGIR conference on Research {\&} development in information retrieval - SIGIR '14},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Dalton, Dietz, Allan - 2014 - Entity query feature expansion using knowledge base links.pdf:pdf},
keywords = {entities,entity linking,entity retrieval,information extraction,information retrieval,ontologies},
mendeley-tags = {entity linking,entity retrieval},
month = {jul},
pages = {365--374},
publisher = {ACM Press},
title = {{Entity query feature expansion using knowledge base links}},
url = {http://dl.acm.org/citation.cfm?id=2600428.2609628},
year = {2014}
}
@article{10.1109/TKDE.2014.2327028,
abstract = {The large number of potential applications from bridging Web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.},
address = {Los Alamitos, CA, USA},
author = {Wang, Jianyong and Han, Jiawei},
doi = {http://doi.ieeecomputersociety.org/10.1109/TKDE.2014.2327028},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {entity linking,survey},
mendeley-tags = {entity linking,survey},
number = {PrePrints},
pages = {1},
publisher = {IEEE Computer Society},
title = {{Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions}},
volume = {99},
year = {2014}
}
@inproceedings{hachey-nothman-radford:2014:P14-2,
address = {Baltimore},
author = {Hachey, Ben and Nothman, Joel and Radford, Will},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {464--469},
publisher = {Association for Computational Linguistics},
title = {{Cheap and easy entity evaluation}},
url = {http://www.aclweb.org/anthology/P/P14/P14-2076},
year = {2014}
}
@inproceedings{charton-EtAl:2014:P14-2,
address = {Baltimore},
author = {Charton, Eric and Meurs, Marie-Jean and Jean-Louis, Ludovic and Gagnon, Michel},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {476--481},
publisher = {Association for Computational Linguistics},
title = {{Mutual Disambiguation for Entity Linking}},
url = {http://www.aclweb.org/anthology/P/P14/P14-2078},
year = {2014}
}
@inproceedings{huang-EtAl:2014:P14-11,
address = {Baltimore},
author = {Huang, Hongzhao and Cao, Yunbo and Huang, Xiaojiang and Ji, Heng and Lin, Chin-Yew},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
keywords = {Twitter,entity linking},
mendeley-tags = {Twitter,entity linking},
pages = {380--390},
publisher = {Association for Computational Linguistics},
title = {{Collective Tweet Wikification based on Semi-supervised Graph Regularization}},
url = {http://www.aclweb.org/anthology/P/P14/P14-1036},
year = {2014}
}
@inproceedings{Shen2014,
address = {New York, New York, USA},
author = {Shen, Wei and Han, Jiawei and Wang, Jianyong},
booktitle = {Proceedings of the 2014 ACM SIGMOD international conference on Management of data - SIGMOD '14},
doi = {10.1145/2588555.2593676},
isbn = {9781450323765},
keywords = {domain-specific entity linking,entity linking,heterogeneous information networks,web},
mendeley-tags = {entity linking,web},
month = {jun},
pages = {1199--1210},
publisher = {ACM Press},
title = {{A probabilistic model for linking named entities in web text with heterogeneous information networks}},
url = {http://dl.acm.org/citation.cfm?id=2588555.2593676},
year = {2014}
}
@inproceedings{Pennacchiotti2009,
abstract = {Combining information extraction systems yields significantly higher quality resources than each system in isolation. In this paper, we generalize such a mixing of sources and features in a framework called Ensemble Semantics. We show very large gains in entity extraction by combining state-of-the-art distributional and pattern-based systems with a large set of features from a webcrawl, query logs, and Wikipedia. Experimental results on a web-scale extraction of actors, athletes and musicians show significantly higher mean average precision scores (29{\%} gain) compared with the current state of the art.},
author = {Pennacchiotti, Marco and Pantel, Patrick},
booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Pennacchiotti, Pantel - 2009 - Entity extraction via ensemble semantics.pdf:pdf},
isbn = {978-1-932432-59-6},
keywords = {ensembles,entity linking},
mendeley-tags = {ensembles,entity linking},
month = {aug},
pages = {238--247},
publisher = {Association for Computational Linguistics},
title = {{Entity extraction via ensemble semantics}},
url = {http://dl.acm.org/citation.cfm?id=1699510.1699542},
year = {2009}
}
@inproceedings{Meij2014,
address = {New York, New York, USA},
author = {Meij, Edgar and Balog, Krisztian and Odijk, Daan},
booktitle = {Proceedings of the 7th ACM international conference on Web search and data mining - WSDM '14},
doi = {10.1145/2556195.2556201},
isbn = {9781450323512},
keywords = {entity linking,entity retrieval,semantic search},
month = {feb},
pages = {683--684},
publisher = {ACM Press},
title = {{Entity linking and retrieval for semantic search}},
url = {http://dl.acm.org/citation.cfm?id=2556195.2556201},
year = {2014}
}
@inproceedings{conf/emnlp/HeLSLZW13,
author = {He, Zhengyan and Liu, Shujie and Song, Yang and Li, Mu and Zhou, Ming and Wang, Houfeng},
booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
keywords = {AIDA,Ratinov,entity linking},
mendeley-tags = {AIDA,Ratinov,entity linking},
pages = {426--435},
publisher = {ACL},
title = {{Efficient Collective Entity Linking with Stacking}},
url = {http://aclweb.org/anthology//D/D13/D13-1041.pdf},
year = {2013}
}
@article{Berlanga2014,
abstract = {This paper presents a novel method for semantic annotation and search of a target corpus using several knowledge resources (KRs). This method relies on a formal statistical framework in which KR concepts and corpus documents are homogeneously represented using statistical language models. Under this framework, we can perform all the necessary operations for an efficient and effective semantic annotation of the corpus. Firstly, we propose a coarse tailoring of the KRs w.r.t the target corpus with the main goal of reducing the ambiguity of the annotations and their computational overhead. Then, we propose the generation of concept profiles, which allow measuring the semantic overlap of the KRs as well as performing a finer tailoring of them. Finally, we propose how to semantically represent documents and queries in terms of the KRs concepts and the statistical framework to perform semantic search. Experiments have been carried out with a corpus about web resources which includes several Life Sciences catalogs and Wikipedia pages related to web resources in general (e.g., databases, tools, services, etc.). Results demonstrate that the proposed method is more effective and efficient than state-of-the-art methods relying on either context-free annotation or keyword-based search.},
author = {Berlanga, Rafael and Nebot, Victoria and P{\'{e}}rez, Mar{\'{i}}a},
doi = {10.1016/j.websem.2014.07.007},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Berlanga, Nebot, P{\'{e}}rez - 2014 - Tailored semantic annotation for semantic search.pdf:pdf},
issn = {15708268},
journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
keywords = {Language models,Semantic annotation,Semantic search,UMLS,entity linking,semantic annotations},
mendeley-tags = {UMLS,entity linking,semantic annotations},
month = {jul},
title = {{Tailored semantic annotation for semantic search}},
url = {http://www.sciencedirect.com/science/article/pii/S1570826814000559},
year = {2014}
}
@inproceedings{Gottipati2011,
abstract = {In this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion. We use both local contexts and global world knowledge to expand query language models. We place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query. Our experiments on the TAC-KBP 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance. Compared with the official results from KBP 2010 participants, our system shows competitive performance.},
author = {Gottipati, Swapna and Jiang, Jing},
booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Gottipati, Jiang - 2011 - Linking entities to a knowledge base with query expansion.pdf:pdf},
isbn = {978-1-937284-11-4},
keywords = {TAC,entity linking,query expansion},
mendeley-tags = {TAC,entity linking,query expansion},
month = {jul},
pages = {804--813},
publisher = {Association for Computational Linguistics},
title = {{Linking entities to a knowledge base with query expansion}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145523},
year = {2011}
}
@inproceedings{Yamada2014,
address = {New York, New York, USA},
author = {Yamada, Ikuya and Ito, Tomotaka and Usami, Shinnosuke and Takagi, Shinsuke and Takeda, Hideaki and Takefuji, Yoshiyasu},
booktitle = {Proceedings of the 25th ACM conference on Hypertext and social media - HT '14},
doi = {10.1145/2631775.2631802},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Yamada et al. - 2014 - Evaluating the helpfulness of linked entities to readers.pdf:pdf},
isbn = {9781450329545},
keywords = {dataset,entity linking,helpfulness,knowledge base,wikipedia},
mendeley-tags = {dataset,entity linking,helpfulness},
month = {sep},
pages = {169--178},
publisher = {ACM Press},
title = {{Evaluating the helpfulness of linked entities to readers}},
url = {http://dl.acm.org/citation.cfm?id=2631775.2631802},
year = {2014}
}
@inproceedings{Chen2011,
abstract = {In this paper, we present a new ranking scheme, collaborative ranking (CR). In contrast to traditional non-collaborative ranking scheme which solely relies on the strengths of isolated queries and one stand-alone ranking algorithm, the new scheme integrates the strengths from multiple collaborators of a query and the strengths from multiple ranking algorithms. We elaborate three specific forms of collaborative ranking, namely, micro collaborative ranking (MiCR), macro collaborative ranking (MaCR) and micro-macro collaborative ranking (MiMaCR). Experiments on entity linking task show that our proposed scheme is indeed effective and promising.},
author = {Chen, Zheng and Ji, Heng},
booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Ji - 2011 - Collaborative ranking a case study on entity linking.pdf:pdf},
isbn = {978-1-937284-11-4},
keywords = {ensemble,entity linking},
mendeley-tags = {ensemble,entity linking},
month = {jul},
pages = {771--781},
publisher = {Association for Computational Linguistics},
title = {{Collaborative ranking: a case study on entity linking}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145520},
year = {2011}
}
@inproceedings{wat,
address = {Gold Coast},
author = {Piccinno, Francesco and Ferragina, Paolo},
booktitle = {ERD '14},
keywords = {ERD,TAGME,entity linking},
mendeley-tags = {ERD,TAGME,entity linking},
title = {{From TagME to WAT: a new Entity Annotator}},
url = {http://web-ngram.research.microsoft.com/ERD2014/Docs/submissions/erd14{\_}submission{\_}4.pdf},
year = {2014}
}
@article{Garcia2014,
author = {Garc{\'{i}}a, Norberto Fern{\'{a}}ndez and Fisteus, Jes{\'{u}}s Arias and Fern{\'{a}}ndez, Luis S{\'{a}}nchez},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
keywords = {TAC,entity linking},
mendeley-tags = {TAC,entity linking},
month = {jan},
number = {1},
pages = {733--773},
publisher = {AI Access Foundation},
title = {{Comparative evaluation of link-based approaches for candidate ranking in link-to-wikipedia systems}},
url = {http://dl.acm.org/citation.cfm?id=2655713.2655734},
volume = {49},
year = {2014}
}
@inproceedings{Meij2013,
address = {New York, New York, USA},
author = {Meij, Edgar and Balog, Krisztian and Odijk, Daan},
booktitle = {Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval - SIGIR '13},
doi = {10.1145/2484028.2484188},
isbn = {9781450320344},
keywords = {entity linkingh,entity retrieval,semantic search},
month = {jul},
pages = {1127},
publisher = {ACM Press},
title = {{Entity linking and retrieval}},
url = {http://dl.acm.org/citation.cfm?id=2484028.2484188},
year = {2013}
}
@inproceedings{DBLP:dblp_conf/ijcnlp/0038ST11,
author = {Zhang, Wei and Su, Jian and Tan, Chew Lim},
booktitle = {Proceedings of the 5th International Joint Conference on Natural Language Processing},
keywords = {LDA,Wikipedia,entity linking},
mendeley-tags = {LDA,Wikipedia,entity linking},
pages = {562--570},
title = {{A Wikipedia-LDA Model for Entity Linking with Batch Size Changing Instance Selection.}},
year = {2011}
}
@inproceedings{Alhelbawy2014,
author = {Alhelbawy, Ayman and Gaizauskas, Robert J.},
booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers),},
keywords = {entity linking,graphs},
mendeley-tags = {entity linking,graphs},
pages = {75--80},
title = {{Graph Ranking for Collective Named Entity Disambiguation}},
url = {http://dblp.uni-trier.de/db/conf/acl/acl2014-2.html{\#}AlhelbawyG14},
year = {2014}
}
@inproceedings{Feyisetan2014,
address = {New York, New York, USA},
author = {Feyisetan, Oluwaseyi and Simperl, Elena and Tinati, Ramine and Luczak-Roesch, Markus and Shadbolt, Nigel},
booktitle = {Proceedings of the 10th International Conference on Semantic Systems - SEM '14},
doi = {10.1145/2660517.2660527},
isbn = {9781450329279},
keywords = {Twitter,entity linking},
mendeley-tags = {Twitter,entity linking},
month = {sep},
pages = {5--12},
publisher = {ACM Press},
title = {{Quick-and-clean extraction of linked data entities from microblogs}},
url = {http://dl.acm.org/citation.cfm?id=2660517.2660527},
year = {2014}
}
@inproceedings{alhelbawycollective,
author = {Alhelbawy, Ayman and Gaizauskas, Robert},
booktitle = {Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers},
keywords = {entity linking,graphs},
mendeley-tags = {entity linking,graphs},
month = {aug},
pages = {1544--1555},
title = {{Collective Named Entity Disambiguation using Graph Ranking and Clique Partitioning Approaches}},
year = {2014}
}
@inproceedings{Xu2008,
address = {New York, New York, USA},
author = {Xu, Yang and Ding, Fan and Wang, Bin},
booktitle = {Proceeding of the 17th ACM conference on Information and knowledge mining - CIKM '08},
keywords = {entity linking,pseudo-relevance feedback,query expansion},
mendeley-tags = {entity linking,pseudo-relevance feedback,query expansion},
month = {oct},
pages = {1441},
publisher = {ACM Press},
title = {{Entity-based query reformulation using wikipedia}},
url = {http://dl.acm.org/citation.cfm?id=1458082.1458322},
year = {2008}
}
@article{Brandao2014,
author = {Brand{\~{a}}o, Wladmir C. and Santos, Rodrygo L. T. and Ziviani, Nivio and de Moura, Edleno S. and da Silva, Altigran S.},
issn = {23301635},
journal = {Journal of the Association for Information Science and Technology},
keywords = {entity linking,pseudo-relevance feedback,query expansion},
mendeley-tags = {entity linking,pseudo-relevance feedback,query expansion},
month = {apr},
pages = {n/a--n/a},
title = {{Learning to expand queries using entities}},
url = {http://doi.wiley.com/10.1002/asi.23084},
year = {2014}
}
@article{Boston2013a,
abstract = {This paper presents an implemented and evaluated methodology for disambiguating terms in search queries and for augmenting queries with expansion terms. By exploiting Wikipedia articles and their reference relations, our method is able to disambiguate terms in particularly short queries with few context words and to effectively expand queries for retrieval of short documents such as tweets. Our strategy can determine when a sequence of words should be treated as a single entity rather than as a sequence of individual entities. This work is part of a larger project to retrieve information graphics in response to user queries.},
author = {Boston, Christopher and Fang, Hui and Carberry, Sandra and Wu, Hao and Liu, Xitong},
issn = {0169023X},
journal = {Data {\&} Knowledge Engineering},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {aug},
title = {{Wikimantic: Toward effective disambiguation and expansion of queries}},
url = {http://www.sciencedirect.com/science/article/pii/S0169023X13000761},
year = {2013}
}
@inproceedings{Yerva2013,
abstract = {Pervasive web and social networks are becoming part of everyone's life. Users through their activities on these networks are leaving traces of their expertise, interests and personalities. With the advances in Web mining and user modeling techniques it is possible to leverage the user social network activity history to extract the semantics of user-generated content. In this work we explore various techniques for constructing user profiles based on the content they publish on social networks. We further show that one of the advantages of maintaining social network user profiles is to provide the context for better understanding of microposts. We propose and experimentally evaluate different approaches for entity disambiguation in social networks based on syntactic and semantic features on top of two different social networks: a general-interest network (i.e., Twitter) and a domain-specific network (i.e., StackOverflow). We demonstrate how disambiguation accuracy increases when considering enriched user profiles integrating content from both social networks.},
author = {Yerva, Surender Reddy and Catasta, Michele and Demartini, Gianluca and Aberer, Karl},
booktitle = {2013 IEEE 14th International Conference on Information Reuse {\&} Integration (IRI)},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {aug},
pages = {120--128},
publisher = {IEEE},
shorttitle = {Information Reuse and Integration (IRI), 2013 IEEE},
title = {{Entity disambiguation in tweets leveraging user social profiles}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6642462},
year = {2013}
}
@article{Pehcevski:2010:ERW,
abstract = {Entity ranking has recently emerged as a research field that aims at retrieving entities as answers to a query. Unlike entity extraction where the goal is to tag names of entities in documents, entity ranking is primarily focused on returning a ranked list of relevant entity names for the query. Many approaches to entity ranking have been proposed, and most of them were evaluated on the INEX Wikipedia test collection. In this paper, we describe a system we developed for ranking Wikipedia entities in answer to a query. The entity ranking approach implemented in our system utilises the known categories, the link structure of Wikipedia, as well as the link co-occurrences with the entity examples (when provided) to retrieve relevant entities as answers to the query. We also extend our entity ranking approach by utilising the knowledge of predicted classes of topic difficulty. To predict the topic difficulty, we generate a classifier that uses features extracted from an INEX topic definition to classify the topic into an experimentally predetermined class. This knowledge is then utilised to dynamically set the optimal values for the retrieval parameters of our entity ranking system. Our experiments demonstrate that the use of categories and the link structure of Wikipedia can significantly improve entity ranking effectiveness, and that topic difficulty prediction is a promising approach that could also be exploited to further improve the entity ranking performance.},
author = {Pehcevski, Jovan and Thom, James A and Vercoustre, Anne-Marie and Naumovski, Vladimir},
doi = {10.1007/s10791-009-9125-9},
file = {::},
issn = {13864564},
journal = {Information Retrieval},
keywords = {INEX-XER,categories,entity retrieval},
mendeley-tags = {INEX-XER,categories,entity retrieval},
number = {5},
pages = {568--600},
title = {{Entity ranking in Wikipedia: utilising categories, links and topic difficulty prediction}},
volume = {13},
year = {2010}
}
@inproceedings{Nie:2007:WOR,
author = {Nie, Zaiqing and Ma, Yunxiao and Shi, Shuming and Wen, Ji-Rong and Ma, Wei-Ying},
booktitle = {Proceedings of the 16th International Conference on World Wide Web},
file = {::},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {81--90},
series = {WWW '07},
title = {{Web Object Retrieval}},
year = {2007}
}
@inproceedings{Macdonald:2011:LMR,
author = {Macdonald, Craig and Ounis, Iadh},
booktitle = {Proceedings of the 33rd European Conference on Advances in Information Retrieval},
file = {::},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {517--529},
series = {ECIR'11},
title = {{Learning Models for Ranking Aggregates}},
year = {2011}
}
@article{Demartini:2010:WFE,
author = {Demartini, Gianluca and Firan, Claudiu S. and Iofciu, Tereza and Krestel, Ralf and Nejdl, Wolfgang},
doi = {10.1007/s10791-010-9135-7},
file = {::},
issn = {1386-4564},
journal = {Information Retrieval},
keywords = {Algorithms,Entity search,Evaluation,Experimentation,INEX-XER,Model,entity retrieval},
mendeley-tags = {INEX-XER,entity retrieval},
month = {may},
number = {5},
pages = {534--567},
publisher = {Kluwer Academic Publishers},
title = {{Why finding entities in Wikipedia is difficult, sometimes}},
url = {http://dl.acm.org/citation.cfm?id=1873014.1873019},
volume = {13},
year = {2010}
}
@inproceedings{Li:2010:EQO,
author = {Li, Xiaonan and Li, Chengkai and Yu, Cong},
booktitle = {Proceedings of the 2Nd International Workshop on Search and Mining User-generated Contents},
file = {::},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {21--28},
series = {SMUC '10},
title = {{Entity-relationship Queries over Wikipedia}},
year = {2010}
}
@article{Kaptein:2013:ECS,
author = {Kaptein, Rianne and Kamps, Jaap},
doi = {10.1016/j.artint.2012.06.003},
file = {::},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Category structure,Entity ranking,INEX-XER,Link structure,Wikipedia,categories,entity retrieval},
mendeley-tags = {INEX-XER,categories,entity retrieval},
month = {jan},
pages = {111--129},
publisher = {Elsevier Science Publishers Ltd.},
title = {{Exploiting the category structure of Wikipedia for entity ranking}},
url = {http://dl.acm.org/citation.cfm?id=2405838.2405908},
volume = {194},
year = {2013}
}
@inproceedings{Ciglan:2012:SMA,
author = {Ciglan, Marek and N{\o}rv{\aa}g, Kjetil and Hluch{\'{y}}, Ladislav},
booktitle = {Proceedings of the 21st International Conference on World Wide Web},
file = {::},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {131--140},
series = {WWW '12},
title = {{The SemSets Model for Ad-hoc Semantic List Search}},
year = {2012}
}
@inproceedings{Cheng:2007:ESE,
author = {Cheng, Tao and Yan, Xifeng and Chang, Kevin Chen-Chuan},
booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
file = {::},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {387--398},
series = {VLDB '07},
title = {{EntityRank: Searching Entities Directly and Holistically}},
year = {2007}
}
@article{Blanco:2013:RRS,
author = {Blanco, Roi and Halpin, Harry and Herzig, Daniel M and Mika, Peter and Pound, Jeffrey and Thompson, Henry S and Tran, Thanh},
journal = {Web Semant.},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
month = {aug},
pages = {14--29},
title = {{Repeatable and Reliable Semantic Search Evaluation}},
volume = {21},
year = {2013}
}
@article{Shen2013a,
abstract = {Integrating the extracted facts with an existing knowledge base has raised an urgent need to address the problem of entity linking. Specifically, entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base. However, this task is challenging due to name ambiguity, textual inconsistency, and lack of world knowledge in the knowledge base. Several methods have been proposed to tackle this problem, but they are largely based on the co-occurrence statistics of terms between the text around the entity mention and the document associated with the entity. In this paper, we propose LINDEN, a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet, by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base. We extensively evaluate the performance of our proposed LINDEN over two public data sets and empirical results show that LINDEN significantly outperforms the state-of-the-art methods in terms of accuracy.},
author = {Shen, Wei and Wang, Jianyong and Luo, Ping and Wang, Min},
doi = {10.1145/2487575.2487686},
isbn = {9781450321747},
journal = {Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '13},
keywords = {entity linking,knowledge base,tweet entity linking,twitter,user interest model-},
mendeley-tags = {entity linking,twitter},
pages = {68},
title = {{Linking named entities in Tweets with knowledge base via user interest modeling}},
url = {http://dl.acm.org/citation.cfm?doid=2487575.2487686},
year = {2013}
}
@article{Boston,
abstract = {This paper presents an implemented and evaluated methodology for disambiguating terms in search queries and for augmenting queries with expansion terms. By exploiting Wikipedia articles and their reference relations, our method is able to disambiguate terms in particularly short queries with few context words and to effectively expand queries for retrieval of short documents such as tweets. Our strategy can determine when a sequence of words should be treated as a single entity rather than as a sequence of individual entities. This work is part of a larger project to retrieve information graphics in response to user queries.},
author = {Boston, Christopher and Fang, Hui and Carberry, Sandra and Wu, Hao and Liu, Xitong},
doi = {10.1016/j.datak.2013.07.004},
issn = {0169023X},
journal = {Data and Knowledge Engineering},
keywords = {Disambiguation,Query expansion,Search queries,entity linking},
mendeley-tags = {entity linking},
title = {{Wikimantic: Toward effective disambiguation and expansion of queries}}
}
@inproceedings{Agarwal2010,
abstract = {We propose to mine structured query templates from search logs, for enabling rich query interpretation that recognizes both query intents and associated attributes. We formalize the notion of template as a sequence of keywords and domain attributes, and our objective is to discover templates with high precision and recall for matching queries in a domain of interest. Our solution bootstraps from small seed input knowledge to discover relevant query templates, by harnessing the wealth of information available in search logs. We model this information in a tri-partite QueST network of queries, sites, and templates. We propose a probabilistic inferencing framework based on the dual metrics of precision and recall- and we show that the dual inferencing correspond respectively to the random walks in backward and forward directions. We deployed and tested our algorithm over a real-world search log of 15 million queries. The algorithm achieved accuracy of as high as 90{\%} (on F-measure), with little seed knowledge and even with incomplete domain schema.},
author = {Agarwal, Ganesh and Kabra, Govind and Chang, Kevin Chen-Chuan},
booktitle = {Proceedings of the 19th international conference on World wide web - WWW '10},
doi = {10.1145/1772690.1772692},
isbn = {9781605587998},
issn = {21508097},
keywords = {query attributes,query intents,query templates,query understanding,search log mining},
mendeley-tags = {query templates,query understanding},
pages = {1},
publisher = {ACM Press},
title = {{Towards rich query interpretation: walking back and forth for mining query templates}},
url = {http://dl.acm.org/citation.cfm?id=1772690.1772692},
year = {2010}
}
@inproceedings{Pantel2011,
abstract = {We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query-product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9{\%} of general web queries with 94{\%} precision.},
author = {Pantel, Patrick and Fuxman, Ariel},
booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
isbn = {978-1-932432-87-9},
keywords = {clicklogs,entity linking,queries},
mendeley-tags = {clicklogs,entity linking,queries},
pages = {83--92},
publisher = {Association for Computational Linguistics},
series = {ACL HLT '11},
title = {{Jigs and Lures: Associating Web Queries with Structured Entities}},
url = {http://www.aclweb.org/anthology/P11-1009},
year = {2011}
}
@inproceedings{Szpektor2011,
address = {New York, New York, USA},
author = {Szpektor, Idan and Gionis, Aristides and Maarek, Yoelle},
booktitle = {Proceedings of the 20th international conference on World wide web - WWW '11},
month = {mar},
pages = {47},
publisher = {ACM Press},
title = {{Improving recommendation for long-tail queries via templates}},
url = {http://dl.acm.org/citation.cfm?id=1963405.1963416},
year = {2011}
}
@inproceedings{Guo2009,
address = {New York, New York, USA},
author = {Guo, Jiafeng and Xu, Gu and Cheng, Xueqi and Li, Hang},
booktitle = {Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '09},
month = {jul},
pages = {267},
publisher = {ACM Press},
title = {{Named entity recognition in query}},
url = {http://dl.acm.org/citation.cfm?id=1571941.1571989},
year = {2009}
}
@article{Dalvi2009,
address = {New York, New York, USA},
author = {Dalvi, Nilesh and Kumar, Ravi and Pang, Bo and Ramakrishnan, Raghu and Tomkins, Andrew and Bohannon, Philip and Keerthi, Sathiya and Merugu, Srujana},
doi = {10.1145/1559795.1559797},
file = {::},
isbn = {9781605585536},
journal = {Proceedings of the twenty-eighth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems - PODS '09},
keywords = {concepts,extraction,ranking,selection},
pages = {1},
publisher = {ACM Press},
title = {{A web of concepts}},
url = {http://portal.acm.org/citation.cfm?doid=1559795.1559797},
year = {2009}
}
@article{Lin2012a,
author = {Lin, Thomas and Pantel, Patrick and Gamon, Michael and Kannan, Anitha and Fuxman, Ariel},
file = {::},
isbn = {9781450312295},
journal = {Proceedings of the 21st {\ldots}},
keywords = {actions,active objects,entity-centric search,query log},
title = {{Active objects: Actions for entity-centric search}},
url = {http://dl.acm.org/citation.cfm?id=2187916},
year = {2012}
}
@inproceedings{Demartini2012,
address = {New York, New York, USA},
author = {Demartini, Gianluca and Difallah, Djellel Eddine and Cudr{\'{e}}-Mauroux, Philippe},
booktitle = {Proceedings of the 21st international conference on World Wide Web - WWW '12},
keywords = {crowdsourcing,entity linking},
mendeley-tags = {crowdsourcing,entity linking},
month = {apr},
pages = {469},
publisher = {ACM Press},
title = {{ZenCrowd}},
url = {http://dl.acm.org/citation.cfm?id=2187836.2187900},
year = {2012}
}
@inproceedings{ChengXiao2013,
abstract = {Wikification, commonly referred to as Disam- biguation to Wikipedia (D2W), is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding Wikipedia pages. Previous ap- proaches to D2W focused on the use of lo- cal and global statistics over the given text, Wikipedia articles and its link structures, to evaluate context compatibility among a list of probable candidates. However, these meth- ods fail (often, embarrassingly), when some level of text understanding is needed to sup- port Wikification. In this paper we introduce a novel approach to Wikification by incorpo- rating, along with statistical methods, richer relational analysis of the text. We provide an extensible, efficient and modular Integer Lin- ear Programming (ILP) formulation of Wik- ification that incorporates the entity-relation inference problem, and show that the ability to identify relations in text helps both candi- date generation and ranking Wikipedia titles considerably. Our results show significant im- provements in both Wikification and the TAC Entity Linking task.},
author = {{Cheng Xiao} and Roth, Dan},
booktitle = {EMNLP '13},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Cheng Xiao, Roth - 2013 - Relational Inference for Wikication.pdf:pdf},
keywords = {ACE,AQUAINT,ILP,MSNBC,TAC,Wikipedia,entity linking},
mendeley-tags = {ACE,AQUAINT,ILP,MSNBC,TAC,Wikipedia,entity linking},
title = {{Relational Inference for Wikication}},
year = {2013}
}
@inproceedings{Sil2013,
abstract = {Recognizing names and linking them to structured data is a fundamental task in text analysis. Existing approaches typically perform these two steps using a pipeline architecture: they use a NamedEntity Recognition (NER) system to nd the boundaries of mentions in text, and an Entity Linking (EL) system to connect the mentions to entries in structured or semi-structured repositories like Wikipedia. However, the two tasks are tightly coupled, and each type of system can benet signicantly from the kind of information provided by the other. We present a joint model for NER and EL, called NEREL, that takes a large set of candidate mentions from typical NER systems and a large set of candidate entity links from EL systems, and ranks the candidate mention-entity pairs together to make joint predictions. In NER and EL experiments across three datasets, NEREL signicantly outperforms or comes close to the performance of two state-of-the-art NER systems, and it outperforms 6 competing EL systems. On the benchmark MSNBC dataset, NEREL provides a 60{\%} reduction in error over the next-best NER system and a 68{\%} reduction in error over the next-best EL system.},
author = {Sil, Avirup and Yates, Alexander},
booktitle = {CIKM '13},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Sil, Yates - 2013 - Re-ranking for Joint Named-Entity Recognition and Linking.pdf:pdf},
keywords = {ACE,Freebase,MSNBC,NER,Wikipedia,YAGO,entity linking},
mendeley-tags = {ACE,Freebase,MSNBC,NER,Wikipedia,YAGO,entity linking},
title = {{Re-ranking for Joint Named-Entity Recognition and Linking}},
year = {2013}
}
@article{Boston2013,
abstract = {This paper presents an implemented and evaluated methodology for disambiguating terms in search queries and for augmenting queries with expansion terms. By exploiting Wikipedia articles and their reference relations, our method is able to disambiguate terms in particularly short queries with few context words and to effectively expand queries for retrieval of short documents such as tweets. Our strategy can determine when a sequence of words should be treated as a single entity rather than as a sequence of individual entities. This work is part of a larger project to retrieve information graphics in response to user queries.},
author = {Boston, Christopher and Fang, Hui and Carberry, Sandra and Wu, Hao and Liu, Xitong},
doi = {10.1016/j.datak.2013.07.004},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Boston et al. - 2013 - Wikimantic Toward effective disambiguation and expansion of queries.pdf:pdf},
issn = {0169023X},
journal = {Data {\&} Knowledge Engineering},
keywords = {disambiguation,entity linking,query expansion,search queries},
mendeley-tags = {entity linking,query expansion},
month = {aug},
number = {null},
title = {{Wikimantic: Toward effective disambiguation and expansion of queries}},
url = {http://dx.doi.org/10.1016/j.datak.2013.07.004},
volume = {null},
year = {2013}
}
@inproceedings{Shen2013,
abstract = {Integrating the extracted facts with an existing knowledge base has raised an urgent need to address the problem of entity linking. Specifically, entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base. However, this task is challenging due to name ambiguity, textual inconsistency, and lack of world knowledge in the knowledge base. Several methods have been proposed to tackle this problem, but they are largely based on the co-occurrence statistics of terms between the text around the entity mention and the document associated with the entity. In this paper, we propose LINDEN, a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet, by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base. We extensively evaluate the performance of our proposed LINDEN over two public data sets and empirical results show that LINDEN significantly outperforms the state-of-the-art methods in terms of accuracy.},
author = {Shen, Wei and Wang, Jianyong and Luo, Ping and Wang, Min},
booktitle = {KDD '13},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Shen et al. - 2013 - Linking Named Entities in Tweets with Knowledge Base via User Interest Modeling.pdf:pdf},
keywords = {Twitter,YAGO,entity linking,fact integration,knowledge base,semantic knowledge,wikipedia},
mendeley-tags = {Twitter,YAGO,entity linking},
title = {{Linking Named Entities in Tweets with Knowledge Base via User Interest Modeling}},
url = {http://dl.acm.org/citation.cfm?id=2187836.2187898},
year = {2013}
}
@inproceedings{Shen2012,
abstract = {Integrating the extracted facts with an existing knowledge base has raised an urgent need to address the problem of entity linking. Specifically, entity linking is the task to link the entity mention in text with the corresponding real world entity in the existing knowledge base. However, this task is challenging due to name ambiguity, textual inconsistency, and lack of world knowledge in the knowledge base. Several methods have been proposed to tackle this problem, but they are largely based on the co-occurrence statistics of terms between the text around the entity mention and the document associated with the entity. In this paper, we propose LINDEN, a novel framework to link named entities in text with a knowledge base unifying Wikipedia and WordNet, by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base. We extensively evaluate the performance of our proposed LINDEN over two public data sets and empirical results show that LINDEN significantly outperforms the state-of-the-art methods in terms of accuracy.},
address = {New York, New York, USA},
author = {Shen, Wei and Wang, Jianyong and Luo, Ping and Wang, Min},
booktitle = {WWW '12},
doi = {10.1145/2187836.2187898},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Shen et al. - 2012 - LINDEN linking named entities with knowledge base via semantic knowledge.pdf:pdf},
isbn = {9781450312295},
keywords = {YAGO,entity linking,fact integration,knowledge base,semantic knowledge,wikipedia},
mendeley-tags = {YAGO,entity linking},
month = {apr},
pages = {449},
publisher = {ACM Press},
title = {{LINDEN: linking named entities with knowledge base via semantic knowledge}},
url = {http://dl.acm.org/citation.cfm?id=2187836.2187898},
year = {2012}
}
@inproceedings{xyz,
abstract = {We study the task of entity linking for Twitter, which tries to associate each mention in a tweet with a corresponding knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, We propose a collective inference model that simultaneously resolves a set of mentions. Particularly, Our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.},
author = {Liu, Xiaohua and Li, Yitong and Wu, Haocheng and Zhou, Ming and {Furu Wei}, Yi Lu},
booktitle = {ACL '13},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2013 - Entity Linking for Tweets.pdf:pdf},
keywords = {Twitter,entity linking},
mendeley-tags = {Twitter,entity linking},
title = {{Entity Linking for Tweets}},
year = {2013}
}
@inproceedings{Yahya2012,
abstract = {The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources. Our method is based on an integer linear program to solve several disambiguation tasks jointly: the segmentation of questions into phrases; the mapping of phrases to semantic entities, classes, and relations; and the construction of SPARQL triple patterns. Our solution harnesses the rich type system provided by knowledge bases in the web of linked data, to constrain our semantic-coherence objective function. We present experiments on both the question translation and the resulting query answering.},
author = {Yahya, Mohamed and Berberich, Klaus and Elbassuoni, Shady and Ramanath, Maya and Tresp, Volker and Weikum, Gerhard},
booktitle = {EMNLP -CoNLL '12},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Yahya et al. - 2012 - Natural language questions for the web of data.pdf:pdf},
keywords = {ILP,IMDB,SPARQL,entity linking,entity retrieval,question answering},
mendeley-tags = {ILP,IMDB,SPARQL,entity linking,entity retrieval,question answering},
month = {jul},
pages = {379--390},
publisher = {Association for Computational Linguistics},
title = {{Natural language questions for the web of data}},
url = {http://dl.acm.org/citation.cfm?id=2390948.2390995},
year = {2012}
}
@inproceedings{Rungsawang,
abstract = {The paper presents a framework that exploits the Thai Wikipedia articles as a knowledge source to train the machine learning classifier for link suggestion purpose. Given an input document, important concepts in the text have been automatically extracted, and the chosen corresponding Wikipedia pages have been determined and suggested to be the destination links for additional information. Preliminary experiments from the prototype running on a test set of Thai Wikipedia articles show that this automatic link suggestion framework provides reasonably up to 90{\%} link suggestion accuracy.},
author = {Rungsawang, Arnon and Siangkhio, Sompop and Surarerk, Athasit and Manaskasemsak, Bundit},
booktitle = {ITCS 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Rungsawang et al. - Unknown - To be ap pe DR ar AF ed T in IT S2 To be ap pe DR ar AF ed T in IT S2.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
title = {{Thai Wikipedia Link Suggestion Framework}}
}
@incollection{Guo2013,
abstract = {Entity linking is the task of linking names in free text to the referent entities in a knowledge base. Most recently proposed linking systems can be broken down into two steps: candidate generation and candidate ranking. The first step searches candidates from the knowledge base and the second step disambiguates them. Previous works have been focused on the recall of the generation because if the target entity is absent in the candidate set, no ranking method can return the correct result. Most of the recall-driven generation strategies will increase the number of the candidates. However, with large candidate sets, memory/time consuming systems are impractical for online applications. In this paper, we propose a novel candidate generation approach to generate high recall candidate set with small size. Experimental results on two KBP data sets show that the candidate generation recall achieves more than 93{\%}. By leveraging our approach, the candidate number is reduced from hundreds to dozens, the system runtime is saved by 70.3{\%} and 76.6{\%} over the baseline and the highest micro-averaged accuracy in the evaluation is improved by 2.2{\%} and 3.4{\%}.},
author = {Guo, Yuhang and Qin, Bing and Li, Yuqin and Liu, Ting and Li, Sheng},
booktitle = {Natural Language Processing and Information Systems},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2013 - Improving Candidate Generation for Entity Linking.pdf:pdf},
isbn = {364238823X},
keywords = {KBP,entity linking},
mendeley-tags = {KBP,entity linking},
pages = {225--236},
publisher = {Springer},
title = {{Improving Candidate Generation for Entity Linking}},
year = {2013}
}
@inproceedings{Li2013,
abstract = {Named entity disambiguation is the task of disambiguating named entity mentions in natural language text and link them to their corresponding entries in a knowledge base such as Wikipedia. Such disambiguation can help enhance readability and add semantics to plain text. It is also a central step in constructing high-quality information network or knowledge graph from unstructured text. Previous research has tackled this problem by making use of various textual and structural features from a knowledge base. Most of the proposed algorithms assume that a knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity. However, the existing knowledge bases are rarely complete (likely will never be), thus leading to poor performance on short queries with not well-known contexts. In such cases, we need to collect additional evidences scattered in internal and external corpus to augment the knowledge bases and enhance their disambiguation power. In this work, we propose a generative model and an incremental algorithm to automatically mine useful evidences across documents. With a specic modeling of background topic and unknown entities, our model is able to harvest useful evidences out of noisy information. Experimental results show that our proposed method outperforms the state-of-the-art approaches significantly: boosting the disambiguation accuracy from 43{\%} (baseline) to 86{\%} on short queries derived from tweets.},
author = {Li, Yang and Wang, Chi and Han, Fangqiu and Han, Jiawei and Roth, Dan and Yan, Xifeng},
booktitle = {KDD 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2013 - Mining Evidences for Named Entity Disambiguation.pdf:pdf},
keywords = {entity linking,topic modeling},
mendeley-tags = {entity linking,topic modeling},
title = {{Mining Evidences for Named Entity Disambiguation}},
year = {2013}
}
@inproceedings{He2013,
abstract = {We propose a novel entity disambiguation model based on Deep Neural Network (DNN). Instead of utilizing simple similarity measure and disjoint combinations of such measures, our method directly optimizes document and entity representation for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.},
author = {He, Zhengyan and Liu, Shujie and Li, Mu and Zhou, Ming and Wang, Houfeng and Zhang, Longkai},
booktitle = {ACL Short Papers '13},
keywords = {deep neural networks,entity linking},
mendeley-tags = {deep neural networks,entity linking},
title = {{Learning Entity Representation for Entity Disambiguation}},
year = {2013}
}
@incollection{Cassidy2012,
abstract = {In this paper we propose two novel approaches to enhance cross-lingual entity linking (CLEL). One is based on cross-lingual information networks, aligned based on monolingual information extraction, and the other uses topic modeling to ensure global consistency. We enhance a strong baseline system derived from a combination of state-of-the-art machine translation and monolingual entity linking to achieve 11.2{\%} improvement in B-Cubed+ F-measure. Our system achieved highly competitive results in the NIST Text Analysis Conference (TAC) Knowledge Base Population (KBP2011) evaluation. We also provide detailed qualitative and quantitative analysis on the contributions of each approach and the remaining challenges.},
author = {Cassidy, Taylor and Ji, Heng and Deng, Hongbo and Zheng, Jing and Han, Jiawei},
booktitle = {Information Access Evaluation. Multilinguality, Multimodality, and Visual Analytics},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Cassidy et al. - 2012 - Analysis and refinement of cross-lingual entity linking.pdf:pdf},
isbn = {3642332463},
keywords = {entity linking,multilingual},
mendeley-tags = {entity linking,multilingual},
pages = {1--12},
publisher = {Springer Berlin Heidelberg},
title = {{Analysis and refinement of cross-lingual entity linking}},
year = {2012}
}
@incollection{Lalmas2011,
author = {Lalmas, Mounia},
booktitle = {Advanced Topics in Information Retrieval SE - 5},
doi = {10.1007/978-3-642-20946-8_5},
editor = {Melucci, Massimo and Baeza-Yates, Ricardo},
isbn = {978-3-642-20945-1},
keywords = {entity retrieval},
language = {English},
mendeley-tags = {entity retrieval},
pages = {109--123},
publisher = {Springer Berlin Heidelberg},
series = {The Information Retrieval Series},
title = {{Aggregated Search}},
url = {http://dx.doi.org/10.1007/978-3-642-20946-8{\_}5},
volume = {33},
year = {2011}
}
@inproceedings{Shu2009,
abstract = {In bibliographies like DBLP and Citeseer, there are three kinds of entity-name problems that need to be solved. First, multiple entities share one name, which is called the name sharing problem. Second, one entity has different names, which is called the name variant problem. Third, multiple entities share multiple names, which is called the name mixing problem. We aim to solve these problems based on one model in this paper. We call this task complete entity resolution. Different from previous work, our work use global information based on data with two types of information, words and author names. We propose a generative latent topic model that involves both author names and words  the LDA-dual model, by extending the LDA (Latent Dirichlet Allocation) model. We also propose a method to obtain model parameters that is global information. Based on obtained model parameters, we propose two algorithms to solve the three problems mentioned above. Experimental results demonstrate the effectiveness and great potential of the proposed model and algorithms.},
author = {Shu, Liangcai and Long, Bo and Meng, Weiyi},
booktitle = {ICDE 2009},
doi = {10.1109/ICDE.2009.29},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Shu, Long, Meng - 2009 - A Latent Topic Model for Complete Entity Resolution.pdf:pdf},
isbn = {978-1-4244-3422-0},
issn = {1084-4627},
keywords = {DBLP,Entity resolution,LDA,entity linking,name disambiguation,topic model},
mendeley-tags = {DBLP,LDA,entity linking},
month = {mar},
pages = {880--891},
publisher = {IEEE},
title = {{A Latent Topic Model for Complete Entity Resolution}},
url = {http://dl.acm.org/citation.cfm?id=1546683.1547324},
year = {2009}
}
@inproceedings{Bhattacharya2006,
abstract = {Entity resolution has received considerable attention in recent years. Given many references to underlying entities, the goal is to predict which references correspond to the same entity. We show how to extend the Latent Dirichlet Allocation model for this task and propose a probabilistic model for collective entity resolution for relational domains where references are connected to each other. Our approach differs from other recently proposed entity resolution approaches in that it is a) generative, b) does not make pair-wise decisions and c) captures relations between entities through a hidden group variable. We propose a novel sampling algorithm for collective entity resolution which is unsupervised and also takes entity relations into account. Additionally, we do not assume the domain of entities to be known and show how to infer the number of entities from the data. We demonstrate the utility and practicality of our relational entity resolution approach for author resolution in two real-world bibliographic datasets. In addition, we present preliminary results on characterizing conditions under which relational information is useful.},
author = {Bhattacharya, Indrajit and Getoor, Lise},
booktitle = {SIAM International Conference on Data Mining},
editor = {Ghosh, Joydeep and Lambert, Diane and Skillicorn, David B and Srivastava, Jaideep},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Bhattacharya, Getoor - 2006 - A Latent Dirichlet Model for Unsupervised Entity Resolution.pdf:pdf},
isbn = {089871611X},
keywords = {LDA,entity linking},
mendeley-tags = {LDA,entity linking},
number = {2},
organization = {University of Maryland},
pages = {47--58},
title = {{A Latent Dirichlet Model for Unsupervised Entity Resolution}},
volume = {29},
year = {2006}
}
@inproceedings{Kataria2011,
abstract = {{Disambiguating entity references by annotating them with unique ids from a catalog is a critical step in the enrichment of unstructured content. In this paper, we show that topic models, such as Latent Dirichlet Allocation (LDA) and its hierarchical variants, form a natural class of models for learning accurate entity disambiguation models from crowd-sourced knowledge bases such as Wikipedia. Our main contribution is a semi-supervised hierarchical model called Wikipedia-based Pachinko Allocation Model{\}} (WPAM) that exploits: (1) All words in the Wikipedia corpus to learn word-entity associations (unlike existing approaches that only use words in a small fixed window around annotated entity references in Wikipedia pages), (2) Wikipedia annotations to appropriately bias the assignment of entity labels to annotated (and co-occurring unannotated) words during model learning, and (3) Wikipedia's category hierarchy to capture co-occurrence patterns among entities. We also propose a scheme for pruning spurious nodes from Wikipedia's crowd-sourced category hierarchy. In our experiments with multiple real-life datasets, we show that WPAM outperforms state-of-the-art baselines by as much as 16{\%} in terms of disambiguation accuracy.},
address = {New York, New York, USA},
author = {Kataria, Saurabh S. and Kumar, Krishnan S. and Rastogi, Rajeev R. and Sen, Prithviraj and Sengamedu, Srinivasan H.},
booktitle = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11},
doi = {10.1145/2020408.2020574},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Kataria et al. - 2011 - Entity disambiguation with hierarchical topic models.pdf:pdf},
isbn = {9781450308137},
keywords = {LDA,Wikipedia,disambiguation,entity linking,entity resolution,topic models},
mendeley-tags = {LDA,Wikipedia,entity linking},
month = {aug},
pages = {1037},
publisher = {ACM Press},
title = {{Entity disambiguation with hierarchical topic models}},
url = {http://dl.acm.org/citation.cfm?id=2020408.2020574},
year = {2011}
}
@article{Tang,
abstract = {Cross-Lingual Link Discovery (CLLD) is a new problem in Information Retrieval. The aim is to automatically identify meaningful and relevant hypertext links between documents in different languages. This is particularly helpful in knowledge discovery if a multi-lingual knowledge base is sparse in one language or another, or the topical coverage in each language is different; such is the case with the Wikipedia. Techniques for identifying new and topically relevant cross-lingual links are a current topic of interest at NTCIR where the CrossLink task has been running since the 2011 NTCIR-9. This paper presents the evaluation framework for benchmarking algorithms for cross-lingual link discovery evaluated in the context of NTCIR-9. This framework includes topics, document collections, assessments, metrics, and a toolkit for pooling, assessment, and evaluation. The assessments are further divided into two separate sets: manual assessments performed by human assessors; and automatic assessments based on links extracted from the Wikipedia itself. Using this framework we show that manual assessment is more robust than automatic assessment in the context of cross-lingual link discovery.},
author = {Tang, Ling-Xiang and Geva, Shlomo and Trotman, Andrew and Xu, Yue and Itakura, Kelly Y.},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Tang et al. - Unknown - An Evaluation Framework for Cross-lingual Link Discovery.pdf:pdf},
journal = {IPM},
keywords = {NTCIR,cross-lingual,entity linking},
mendeley-tags = {NTCIR,cross-lingual,entity linking},
title = {{An Evaluation Framework for Cross-lingual Link Discovery}}
}
@inproceedings{Eskevich2013,
abstract = {Searching for relevant webpages and following hyperlinks to related content is a widely accepted and effective approach to information seeking on the textual web. Existing work on multimedia information retrieval has focused on search for individual relevant items or on content linking without specific attention to search results. We describe our research exploring integrated multimodal search and hyperlinking for multimedia data. Our investigation is based on the MediaEval 2012 Search and Hyperlinking task. This includes a known-item search task using the Blip10000 internet video collection, where automatically created hyperlinks link each relevant item to related items within the collection. The search test queries and link assessment for this task was generated using the Amazon Mechanical Turk crowdsourcing platform. Our investigation examines a range of alternative methods which seek to address the challenges of search and hyperlinking using multimodal approaches. The results of our experiments are used to propose a research agenda for developing effective techniques for search and hyperlinking of multimedia content.},
address = {New York, New York, USA},
author = {Eskevich, Maria and de Nies, Tom and Debevere, Pedro and {Van de Walle}, Rik and Galuscakova, Petra and Pecina, Pavel and Larson, Martha and Jones, Gareth J.F. and Aly, Robin and Ordelman, Roeland J.F. and Chen, Shu and Nadeem, Danish and Guinaudeau, Camille and Gravier, Guillaume and S{\'{e}}billot, Pascale},
booktitle = {ICMR 2013},
doi = {10.1145/2461466.2461511},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Eskevich et al. - 2013 - Multimedia information seeking through search and hyperlinking.pdf:pdf},
isbn = {9781450320337},
keywords = {audio-visual hyperlinking,audio-visual search,entity linking,multimedia},
mendeley-tags = {entity linking,multimedia},
month = {apr},
pages = {287},
publisher = {ACM Press},
title = {{Multimedia information seeking through search and hyperlinking}},
url = {http://dl.acm.org/citation.cfm?id=2461466.2461511},
year = {2013}
}
@inproceedings{Kim2010,
address = {New York, New York, USA},
author = {Kim, Jinyoung and Croft, W. Bruce},
booktitle = {Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '10},
doi = {10.1145/1835449.1835461},
isbn = {9781450301534},
keywords = {desktop search,entity retrieval,human computation game,information retrieval,semi-structured document retrieval,type prediction},
mendeley-tags = {entity retrieval},
month = {jul},
pages = {50},
publisher = {ACM Press},
title = {{Ranking using multiple document types in desktop search}},
url = {http://dl.acm.org/citation.cfm?id=1835449.1835461},
year = {2010}
}
@inproceedings{Guo2013a,
abstract = {Information extraction on microblog posts is an important task nowadays, as microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. Given that the current definition of named entity recognition is too limited, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we find that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy, and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15{\%} F1.},
author = {Guo, Stephen and Chang, Ming-Wei and Kiciman, Emre},
booktitle = {NAACL-HLT 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Guo, Chang, Kiciman - 2013 - To Link or Not to Link A Study on End-to-End Tweet Entity Linking.pdf:pdf},
keywords = {Twitter,entity linking},
mendeley-tags = {Twitter,entity linking},
title = {{To Link or Not to Link? A Study on End-to-End Tweet Entity Linking}},
year = {2013}
}
@inproceedings{OAIR:2013:Balog,
abstract = {Knowledge bases have become indispensable sources of information. It is therefore critical that they rely on the latest information available and get updated every time new facts surface. Knowledge base acceleration (KBA) systems seek to help humans expand knowledge bases like Wikipedia by automatically recommending edits based on incoming content streams. A core step in this process is that of identifying relevant content, i.e., ltering documents that would imply modications to the attributes or relations of a given target entity. We propose two multi-step classication approaches for this task that consist of two and three binary classi- cation steps, respectively. Both methods share the same initial component, which is concerned with the identication of entity mentions in documents, while subsequent steps involve identication of documents being relevant and/or central to a given entity. Using the evaluation platform of the TREC 2012 KBA track and a rich feature set developed for this particular task, we show that both approaches deliver state-of-the-art performance.},
author = {{Balog Krisztian} and Naimdjon, Takhirov and Heri, Ramampiaro and Kjetil, N{\o}rv{\aa}g},
booktitle = {OAIR 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Balog Krisztian et al. - 2013 - Multi-step Classification Approaches to Cumulative Citation Recommendation.pdf:pdf},
keywords = {TREC-KBA,Wikipedia,cumulative citation recommendation,entity linking,information filtering,knowledge base acceleration},
mendeley-tags = {TREC-KBA,Wikipedia,entity linking},
title = {{Multi-step Classification Approaches to Cumulative Citation Recommendation}},
year = {2013}
}
@article{Demartini2013a,
abstract = {We tackle the problems of semiautomatically matching linked data sets and of linking large collections of Web pages to linked data. Our system, ZenCrowd, (1) uses a three-stage blocking technique in order to obtain the best possible instance matches while minimizing both computational complexity and latency, and (2) identifies entities from natural language text using state-of-the-art techniques and automatically connects them to the linked open data cloud. First, we use structured inverted indices to quickly find potential candidate results from entities that have been indexed in our system. Our system then analyzes the candidate matches and refines them whenever deemed necessary using computationally more expensive queries on a graph database. Finally, we resort to human computation by dynamically generating crowdsourcing tasks in case the algorithmic components fail to come up with convincing results. We integrate all results from the inverted indices, from the graph database and from the crowd using a probabilistic framework in order to make sensible decisions about candidate matches and to identify unreliable human workers. In the following, we give an overview of the architecture of our system and describe in detail our novel three-stage blocking technique and our probabilistic decision framework. We also report on a series of experimental results on a standard data set, showing that our system can achieve a 95 {\%} average accuracy on instance matching (as compared to the initial 88 {\%} average accuracy of the purely automatic baseline) while drastically limiting the amount of work performed by the crowd. The experimental evaluation of our system on the entity linking task shows an average relative improvement of 14 {\%} over our best automatic approach.},
author = {Demartini, Gianluca and Difallah, DjellelEddine and Cudr{\'{e}}-Mauroux, Philippe},
doi = {10.1007/s00778-013-0324-z},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Demartini, Difallah, Cudr{\'{e}}-Mauroux - 2013 - Large-scale linked data integration using probabilistic reasoning and crowdsourcing.pdf:pdf},
issn = {1066-8888},
journal = {The VLDB Journal},
keywords = {Crowdsourcing,Data integration,Entity linking,Instance matching,Probabilistic reasoning,entity linking,ontology learning,ontology population},
language = {English},
mendeley-tags = {entity linking,ontology learning,ontology population},
pages = {1--23},
publisher = {Springer Berlin Heidelberg},
title = {{Large-scale linked data integration using probabilistic reasoning and crowdsourcing}},
url = {http://dx.doi.org/10.1007/s00778-013-0324-z},
year = {2013}
}
@inproceedings{HT:2013:Derczynski,
abstract = {Using semantic technologies for mining and intelligent information access to microblogs is a challenging, emerging research area. Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Semantic annotation of tweets is typically performed in a pipeline, comprising successive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). Consequently, errors are cumulative, and earlier-stage problems can severely reduce the performance of final stages. This paper presents a characterisation of genre-specific problems at each semantic annotation stage and the impact on subsequent stages. Critically, we evaluate impact on two high-level semantic annotation tasks: named entity detection and disambiguation. Our results demonstrate the importance of making approaches specific to the genre, and indicate a diminishing returns effect that reduces the effectiveness of complex text normalisation.},
author = {Derczynski, Leon and Maynard, Diana and Aswani, Niraj and Bontcheva, Kalina},
booktitle = {HT 2013},
doi = {10.1145/2481492.2481495},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Derczynski et al. - 2013 - Microblog-genre noise and impact on semantic annotation accuracy.pdf:pdf},
isbn = {9781450319676},
keywords = {Twitter,Wikipedia,entity disambiguation,entity linking,entity recognition,microblog,semantic annotation,text normalisation},
mendeley-tags = {Twitter,Wikipedia,entity linking},
month = {may},
pages = {21--30},
publisher = {ACM Press},
title = {{Microblog-genre noise and impact on semantic annotation accuracy}},
url = {http://dl.acm.org/citation.cfm?id=2481492.2481495},
year = {2013}
}
@inproceedings{Lin2012,
abstract = {We introduce an entity-centric search experience, called Active Objects, in which entity-bearing queries are paired with actions that can be performed on the entities. For example, given a query for a specific flashlight, we aim to present actions such as reading reviews, watching demo videos, and finding the best price online. In an annotation study conducted over a random sample of user query sessions, we found that a large proportion of queries in query logs involve actions on entities, calling for an automatic approach to identifying relevant actions for entity-bearing queries. In this paper, we pose the problem of finding actions that can be performed on entities as the problem of probabilistic inference in a graphical model that captures how an entity bearing query is generated. We design models of increasing complexity that capture latent factors such as entity type and intended actions that determine how a user writes a query in a search box, and the URL that they click on. Given a large collection of real-world queries and clicks from a commercial search engine, the models are learned efficiently through maximum likelihood estimation using an EM algorithm. Given a new query, probabilistic inference enables recommendation of a set of pertinent actions and hosts. We propose an evaluation methodology for measuring the relevance of our recommended actions, and show empirical evidence of the quality and the diversity of the discovered actions.},
address = {New York, New York, USA},
author = {Lin, Thomas and Pantel, Patrick and Gamon, Michael and Kannan, Anitha and Fuxman, Ariel},
booktitle = {WWW 2012},
doi = {10.1145/2187836.2187916},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2012 - Active objects actions for entity-centric search.pdf:pdf},
isbn = {9781450312295},
keywords = {actions,active objects,entity linking,entity search,entity-centric search,query log mining,web search},
mendeley-tags = {entity linking,entity search},
month = {apr},
pages = {589},
publisher = {ACM Press},
title = {{Active objects: actions for entity-centric search}},
url = {http://dl.acm.org/citation.cfm?id=2187836.2187916},
year = {2012}
}
@inproceedings{Pilz2011,
abstract = {Name ambiguity arises from the polysemy of names and causes uncertainty about the true identity of entities referenced in unstructured text. This is a major problem in areas like information retrieval or knowledge management, for example when searching for a specific entity or updating an existing knowledge base. We approach this problem of named entity disambiguation (NED) using thematic information derived from Latent Dirichlet Allocation (LDA) to compare the entity mention's context with candidate entities in Wikipedia represented by their respective articles. We evaluate various distances over topic distributions in a supervised classification setting to find the best suited candidate entity, which is either covered in Wikipedia or unknown. We compare our approach to a state of the art method and show that it achieves significantly better results in predictive performance, regarding both entities covered in Wikipedia as well as uncovered entities. We show that our approach is in general language independent as we obtain equally good results for named entity disambiguation using the English, the German and the French Wikipedia.},
address = {New York, New York, USA},
author = {Pilz, Anja and Paa{\ss}, Gerhard},
booktitle = {CIKM 2011},
doi = {10.1145/2063576.2063700},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Pilz, Paa{\ss} - 2011 - From names to entities using thematic context distance.pdf:pdf},
isbn = {9781450307178},
keywords = {LDA,Wikipedia,classification,entity linking,named entities,named entity disambiguation,named entity resolution,topic modeling},
mendeley-tags = {LDA,Wikipedia,entity linking},
month = {oct},
pages = {857},
publisher = {ACM Press},
title = {{From names to entities using thematic context distance}},
url = {http://dl.acm.org/citation.cfm?id=2063576.2063700},
year = {2011}
}
@inproceedings{Sauper2009,
author = {Sauper, Christina and Barzilay, Regina},
booktitle = {ACL 2009},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Sauper, Barzilay - 2009 - Automatically generating Wikipedia articles a structure-aware approach.pdf:pdf},
isbn = {978-1-932432-45-9},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {aug},
pages = {208--216},
publisher = {Association for Computational Linguistics},
title = {{Automatically generating Wikipedia articles: a structure-aware approach}},
url = {http://dl.acm.org/citation.cfm?id=1687878.1687909},
year = {2009}
}
@inproceedings{Sawant2013,
author = {Sawant, Uma and Chakrabarti, Soumen},
booktitle = {WWW 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Sawant, Chakrabarti - 2013 - Learning joint query interpretation and response ranking.pdf:pdf},
isbn = {978-1-4503-2035-1},
keywords = {entity retrieval,entity search,query interpretation},
mendeley-tags = {entity retrieval},
month = {may},
pages = {1099--1110},
publisher = {International World Wide Web Conferences Steering Committee},
title = {{Learning joint query interpretation and response ranking}},
url = {http://dl.acm.org/citation.cfm?id=2488388.2488484},
year = {2013}
}
@inproceedings{EMNLP:2012:Han,
abstract = {Entity Linking (EL) has received considerable attention in recent years. Given many name mentions in a document, the goal of EL is to predict their referent entities in a knowledge base. Traditionally, there have been two distinct directions of EL research: one focusing on the effects of mention's context compatibility, assuming that "the referent entity of a mention is reflected by its context"; the other dealing with the effects of document's topic coherence, assuming that "a mention's referent entity should be coherent with the document's main topics". In this paper, we propose a generative model -- called entity-topic model, to effectively join the above two complementary directions together. By jointly modeling and exploiting the context compatibility, the topic coherence and the correlation between them, our model can accurately link all mentions in a document using both the local information (including the words and the mentions in a document) and the global knowledge (including the topic knowledge, the entity context knowledge and the entity name knowledge). Experimental results demonstrate the effectiveness of the proposed model.},
author = {Han, Xianpei and Sun, Le},
booktitle = {EMNLP-CoNLL 2012},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Han, Sun - 2012 - An entity-topic model for entity linking.pdf:pdf;:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Han, Sun - 2012 - An entity-topic model for entity linking(2).pdf:pdf},
keywords = {IITB,Kulkarni,MW,Wikify,entity linking,topic modeling},
mendeley-tags = {IITB,Kulkarni,MW,Wikify,entity linking,topic modeling},
month = {jul},
pages = {105--115},
publisher = {Association for Computational Linguistics},
title = {{An entity-topic model for entity linking}},
url = {http://dl.acm.org/citation.cfm?id=2390948.2390962},
year = {2012}
}
@article{SIGIR:2011:Han,
abstract = {Entity Linking (EL) is the task of linking name mentions in Web text with their referent entities in a knowledge base. Traditional EL methods usually link name mentions in a document by assuming them to be independent. However, there is often additional interdependence between different EL decisions, i.e., the entities in the same document should be semantically related to each other. In these cases, Collective Entity Linking, in which the name mentions in the same document are linked jointly by exploiting the interdependence between them, can improve the entity linking accuracy. This paper proposes a graph-based collective EL method, which can model and exploit the global interdependence between different EL decisions. Specifically, we first propose a graph-based representation, called Referent Graph, which can model the global interdependence between different EL decisions. Then we propose a collective inference algorithm, which can jointly infer the referent entities of all name mentions by exploiting the interdependence captured in Referent Graph. The key benefit of our method comes from: 1) The global interdependence model of EL decisions; 2) The purely collective nature of the inference algorithm, in which evidence for related EL decisions can be reinforced into high-probability decisions. Experimental results show that our method can achieve significant performance improvement over the traditional EL methods.},
author = {Han, Xianpei and Sun, Le and Zhao, Jun},
doi = {10.1145/2009916.2010019},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Han, Sun, Zhao - 2011 - Collective entity linking in web text a graph-based method.pdf:pdf},
isbn = {9781450307574},
journal = {SIGIR 2011},
keywords = {Cucerzan,IITB,Kulkarni,MW,Wikify,collective entity disambiguation,collective entity linking,entity linking,graph based entity linking,graph-based,todo},
mendeley-tags = {Cucerzan,IITB,Kulkarni,MW,Wikify,entity linking,graph-based,todo},
pages = {765--774},
publisher = {ACM},
series = {SIGIR '11},
title = {{Collective Entity Linking in Web Text : A Graph-Based Method}},
url = {http://portal.acm.org/citation.cfm?id=2010019},
year = {2011}
}
@article{Rao:2011,
abstract = {In the menagerie of tasks for information extraction, entity linking is a new beast that has drawn a lot of attention from NLP practitioners and researchers recently. Entity Linking, also referred to as record linkage or entity resolution, involves aligning a textual mention of a named-entity to an appropriate entry in a knowledge base, which may or may not contain the entity. This has manifold applications ranging from linking patient health records to maintaining personal credit files, prevention of identity crimes, and supporting law enforcement. We discuss the key challenges present in this task and we present a high-performing system that links entities using max-margin ranking.We also summarize recent work in this area and describe several open research problems.},
author = {Rao, Delip and Mcnamee, Paul and Dredze, Mark},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Rao, Mcnamee, Dredze - 2013 - Entity Linking Finding Extracted Entities in a Knowledge Base.pdf:pdf},
journal = {Multi-source, Multilingual Information Extraction and Summarization},
keywords = {Cucerzan,TAC,edge base population,entity disambiguation,entity linking,entity resolution,knowl,named entities,overview,record linkage},
mendeley-tags = {Cucerzan,TAC,entity linking,overview},
pages = {93--115},
title = {{Entity Linking : Finding Extracted Entities in a Knowledge Base}},
year = {2013}
}
@inproceedings{Mendes2011,
abstract = {Interlinking text documents with Linked Open Data enables the Web of Data to be used as background knowledge within document-oriented applications such as search and faceted browsing. As a step towards interconnecting the Web of Documents with the Web of Data, we developed DBpedia Spotlight, a system for automatically annotating text documents with DBpedia URIs. DBpedia Spotlight allows users to configure the annotations to their specific needs through the DBpedia Ontology and quality measures such as prominence, topical pertinence, contextual ambiguity and disambiguation confidence. We compare our approach with the state of the art in disambiguation, and evaluate our results in light of three baselines and six publicly available annotation systems, demonstrating the competitiveness of our system. DBpedia Spotlight is shared as open source and deployed as a Web Service freely available for public use.},
author = {Mendes, Pablo N and Jakob, Max and Garc{\'{i}}a-silva, Andr{\'{e}}s and Bizer, Christian},
booktitle = {Proceedings of the 7th International Conference on Semantic Systems (I-Semantics).},
doi = {10.1145/2063518.2063519},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Mendes et al. - 2011 - DBpedia Spotlight Shedding Light on the Web of Documents.pdf:pdf},
isbn = {9781450306218},
keywords = {DBpedia,DBpedia Spotlight,dbpedia,entity linking,linked data,named entity,text annotation},
mendeley-tags = {DBpedia,DBpedia Spotlight,entity linking},
number = {2},
pages = {1--8},
pmid = {594429},
publisher = {Facultad de Inform{\'{a}}tica (UPM)},
title = {{DBpedia Spotlight : Shedding Light on the Web of Documents}},
url = {http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/Mendes-Jakob-GarciaSilva-Bizer-DBpediaSpotlight-ISEM2011.pdf},
volume = {95},
year = {2011}
}
@inproceedings{Fogarolli2009,
abstract = {In this paper an approach based on Wikipedia link structure for sense disambiguation is presented and evaluated. Wikipedia is used as a reference to obtain lexicographic relationships and in combination with statistical information extraction it is possible to deduce concepts related to the terms extracted from a corpus. In addition, since the corpus covers a representation of a part of the real world the corpus itself is used as rdquotraining datardquo for choosing the sense which best fit the corpus.},
author = {Fogarolli, A},
booktitle = {2009 IEEE International Conference on Semantic Computing},
doi = {10.1109/ICSC.2009.7},
editor = {Centenario, Sociedad Estatal Quinto},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Fogarolli - 2009 - Word Sense Disambiguation Based on Wikipedia Link Structure.pdf:pdf},
isbn = {9781424449620},
keywords = {WSD,entity linking,graph-based,semantics,wikipedia,wsd},
mendeley-tags = {WSD,entity linking,graph-based},
number = {1},
pages = {77--82},
publisher = {Ieee},
title = {{Word Sense Disambiguation Based on Wikipedia Link Structure}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5298935},
volume = {0},
year = {2009}
}
@inproceedings{gentile2009graph,
abstract = {Natural Language is a mean to express and discuss about concepts, objects, events, i.e. it carries semantic contents. The SemanticWeb aims at tightly coupling contents with their precise meanings. One of the ultimate roles of Natural Language Processing techniques is identifying the meaning of the text, providing effective ways to make a proper linkage between textual references and real world objects. This work adresses the problem of giving a sense to proper names in a text, that is automatically associating words representing Named Entities with their identities. The proposed methodology for Named Entity Disambiguation is based on Semantic Relatedness Scores obtained with a graph based model over Wikipedia. We show that, without building a Bag of Words representation of text, but only considering named entities within the text, the proposed paradigm achieves results competitive with the state of the art on a news story dataset.},
author = {Gentile, Anna Lisa and Zhang, Ziqi and Xia, Lei and Iria, Jos{\'{e}}},
booktitle = {Proceedings of International Conference on Software, Services {\&} Semantic Technologies},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Gentile et al. - 2009 - Graph-based semantic relatedness for named entity disambiguation.pdf:pdf},
keywords = {cucerzan,entity linking,graph-based},
mendeley-tags = {cucerzan,entity linking,graph-based},
publisher = {Demetra EOOD},
title = {{Graph-based semantic relatedness for named entity disambiguation}},
year = {2009}
}
@inproceedings{SWIM:2012:hakimov,
abstract = {Named Entity Recognition (NER) is a subtask of information extraction and aims to identify atomic entities in text that fall into predefined categories such as person, location, organization, etc. Recent efforts in NER try to extract entities and link them to linked data entities. Linked data is a term used for data resources that are created using semantic web standards such as DBpedia. There are a number of online tools that try to identify named entities in text and link them to linked data resources. Although one can use these tools via their APIs and web interfaces, they use different data resources and different techniques to identify named entities and not all of them reveal this information. One of the major tasks in NER is disambiguation that is identifying the right entity among a number of entities with the same names; for example "apple" standing for both "Apple, Inc." the company and the fruit. We developed a similar tool called NERSO, short for Named Entity Recognition Using Semantic Open Data, to automatically extract named entities, disambiguating and linking them to DBpedia entities. Our disambiguation method is based on constructing a graph of linked data entities and scoring them using a graph-based centrality algorithm. We evaluate our system by comparing its performance with two publicly available NER tools. The results show that NERSO performs better. 2012 ACM.},
annote = {Dataset used for evaluation available at http://wis.etu.edu.tr/nerso/evaluation.html.},
author = {Hakimov, Sherzod and Oto, Salih Atilay and Dogdu, Erdogan},
booktitle = {Proceedings of the 4th International Workshop on Semantic Web Information Management SWIM12},
doi = {10.1145/2237867.2237871},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Hakimov, Oto, Dogdu - 2012 - Named entity recognition and disambiguation using linked data and graph-based centrality scoring.pdf:pdf;:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Hakimov, Oto, Dogdu - 2012 - Named entity recognition and disambiguation using linked data and graph-based centrality scoring(2).pdf:pdf},
isbn = {9781450314466},
keywords = {algorithms,closeness centrality,data handling,data mining,dbpedia,entity linking,fruits,graph-based,graphic methods,linked datum,named,spotlight},
mendeley-tags = {dbpedia,entity linking,graph-based,spotlight},
pages = {1--7},
publisher = {ACM Press},
title = {{Named entity recognition and disambiguation using linked data and graph-based centrality scoring}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84863477078{\&}partnerID=40{\&}md5=3c880d6e0cac6fe2ea6e6ecde5a9fc94},
year = {2012}
}
@inproceedings{COLING:2010:Zhou,
abstract = {Ambiguity of entity mentions and concept references is a challenge to mining text beyond surface-level keywords. We describe an effective method of disambiguating surface forms and resolving them to Wikipedia entities and concepts. Our method employs an extensive set of features mined from Wikipedia and other large data sources, and combines the features using a machine learning approach with automatically generated training data. Based on a manually labeled evaluation set containing over 1000 news articles, our resolution model has 85{\%} precision and 87.8{\%} recall. The performance is significantly better than three baselines based on traditional context similarities or sense commonness measurements. Our method can be applied to other languages and scales well to new entities and concepts.},
author = {Zhou, Yiping and Nie, Lan and Flavian, Omid Rouhani-kalleh and Scott, Vasile},
booktitle = {Computational Linguistics},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Zhou et al. - 2010 - Resolving Surface Forms to Wikipedia Topics.pdf:pdf},
keywords = {Cucerzan,MSNBC,Wikipedia,Yahoo! News,commonness,entity linking,gbdt,gbrt,machine{\_}learning},
mendeley-tags = {Cucerzan,MSNBC,Wikipedia,Yahoo! News,commonness,entity linking,gbdt,gbrt,machine{\_}learning},
number = {August},
pages = {1335--1343},
publisher = {Association for Computational Linguistics},
series = {COLING '10},
title = {{Resolving Surface Forms to Wikipedia Topics}},
url = {http://www.aclweb.org/anthology/C10-1150},
year = {2010}
}
@inproceedings{TAC:2011:Ji,
abstract = {In this paper we give an overview of the Knowledge Base Population (KBP) track at TAC 2011. The main goal of KBP is to promote research in discovering facts about entities and expanding a structured knowledge base with this information. Compared to KBP2010, we extended the Entity Linking task to require clustering of mentions of entities not already in the KB (NIL queries'). We also introduced two new tasks - (1) Cross-lingual Entity Linking: Given a set of multi-lingual (English and Chinese) queries, the system is required to provide the ID of the English KB entry to which each query refers and cluster NIL queries without KB references; and (2) Temporal Slot Filling: given an entity query, the system is required to discover the start and end dates for any identied slot ll. KBP2011 has attracted many participants (over 65 teams registered, among which 35 teams submitted results). In this paper we provide an overview of the task denition, annotation issues, successful methods and research challenges associated with each task in KBP2011.},
author = {Ji, Heng and Grishman, Ralph and {Trang Dang}, Hoa},
booktitle = {TAC 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Ji, Grishman, Trang Dang - 2011 - Overview of the TAC 2011 Knowledge Base Population Track.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
title = {{Overview of the TAC 2011 Knowledge Base Population Track}},
year = {2011}
}
@inproceedings{TAC:2010:Ji,
abstract = {In this paper we give an overview of the Knowledge Base Population (KBP) track at TAC 2010. The main goal of KBP is to promote research in discovering facts about entities and expanding a structured knowledge base with this information. A large source collection of newswire and web documents is provided for systems to discover information. Attributes (a.k.a. slots) derived from Wikipedia infoboxes are used to create the reference knowledge base (KB). KBP2010 includes the following four tasks: (1) Regular Entity Linking, where names must be aligned to entities in the KB; (2) Optional Entity linking, without using Wikipedia texts; (3) Regular Slot Filling, which requires a system to automatically discover the attributes of specified entities from the source document collection and use them to expand the KB; (4) Surprise Slot Filling, which requires a system to return answers regarding new slot types within a short time period. KBP2010 has attracted many participants (over 45 teams registered for KBP 2010 (not including the RTEKBP Validation Pilot task), among which 23 teams submitted results). In this paper we provide an overview of the task definition and annotation challenges associated with KBP2010. Then we summarize the evaluation results and discuss the lessons that we have learned based on detailed analysis.},
author = {Ji, Heng and Grishman, Ralph and Dang, Hoa Trang and Griffitt, Kira and Ellis, Joe},
booktitle = {TAC 2010},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Ji et al. - 2010 - Overview of the TAC 2010 Knowledge Base Population Track.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
title = {{Overview of the TAC 2010 Knowledge Base Population Track}},
year = {2010}
}
@inproceedings{WWW:2013:Murnane,
abstract = {We address the Named Entity Disambiguation (NED) problem for short, user-generated texts on the social Web. In such settings, the lack of linguistic features and sparse lexical context result in a high degree of ambiguity and sharp performance drops of nearly 50{\%} in the accuracy of conventional NED systems. We handle these challenges by developing a model of user-interest with respect to a personal knowledge context; and Wikipedia, a particularly well-established and reliable knowledge base, is used to instantiate the procedure. We conduct systematic evaluations using individuals' posts from Twitter, YouTube, and Flickr and demonstrate that our novel technique is able to achieve substantial performance gains beyond state-of-the-art NED methods.},
archivePrefix = {arXiv},
arxivId = {1304.2401},
author = {Murnane, Elizabeth L. and Haslhofer, Bernhard and Lagoze, Carl},
booktitle = {WWW 2013},
eprint = {1304.2401},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Murnane, Haslhofer, Lagoze - 2013 - RESLVE Leveraging User Interest to Improve Entity Disambiguation on Short Text.pdf:pdf},
keywords = {Entity Resolution,Personalized IR,Semantic Knowledge Graph,Social Web,User Interest Modeling,entity linking},
mendeley-tags = {entity linking},
month = {apr},
title = {{RESLVE: Leveraging User Interest to Improve Entity Disambiguation on Short Text}},
url = {http://arxiv.org/abs/1304.2401},
year = {2013}
}
@inproceedings{Balog:2006:FME,
address = {New York, NY, USA},
author = {Balog, Krisztian and Azzopardi, Leif and de Rijke, Maarten},
booktitle = {Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/1148170.1148181},
isbn = {1-59593-369-7},
keywords = {enterprise search,entity retrieval,expert finding},
mendeley-tags = {entity retrieval},
pages = {43--50},
publisher = {ACM},
series = {SIGIR '06},
title = {{Formal models for expert finding in enterprise corpora}},
url = {http://doi.acm.org/10.1145/1148170.1148181},
year = {2006}
}
@inproceedings{Sorg:2011:FRE,
address = {Paris, France},
author = {Sorg, Philipp and Cimiano, Philipp},
booktitle = {Proceedings of the International Conference on Knowledge Discovery and Information Retrieval (KDIR)},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
month = {oct},
publisher = {Reuters},
title = {{Finding the Right Expert: Discriminative Models for Expert Retrieval}},
type = {Inproceedings},
year = {2011}
}
@inproceedings{Kim:2009:PRM,
address = {Berlin, Heidelberg},
author = {Kim, Jinyoung and Xue, Xiaobing and Croft, W Bruce},
booktitle = {Proceedings of the 31th European Conference on IR Research on Advances in Information Retrieval},
doi = {10.1007/978-3-642-00958-7_22},
file = {::},
isbn = {978-3-642-00957-0},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {228--239},
publisher = {Springer-Verlag},
series = {ECIR '09},
title = {{A Probabilistic Retrieval Model for Semistructured Data}},
url = {http://dx.doi.org/10.1007/978-3-642-00958-7{\_}22},
year = {2009}
}
@inproceedings{Macdonald:2007:EDQ,
address = {New York, NY, USA},
author = {Macdonald, Craig and Ounis, Iadh},
booktitle = {Proceedings of the sixteenth ACM conference on Conference on information and knowledge management},
doi = {10.1145/1321440.1321490},
isbn = {978-1-59593-803-9},
keywords = {entity retrieval,expert finding,expert search information retrieval,expertise modelling,query expansion,topic drift},
mendeley-tags = {entity retrieval},
pages = {341--350},
publisher = {ACM},
series = {CIKM '07},
title = {{Expertise drift and query expansion in expert search}},
url = {http://doi.acm.org/10.1145/1321440.1321490},
year = {2007}
}
@inproceedings{Kim:2012:FRM,
address = {Berlin, Heidelberg},
author = {Kim, Jin Young and Croft, W Bruce},
booktitle = {Proceedings of the 34th European conference on Advances in Information Retrieval},
doi = {10.1007/978-3-642-28997-2_9},
isbn = {978-3-642-28996-5},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {97--108},
publisher = {Springer-Verlag},
series = {ECIR'12},
title = {{A field relevance model for structured document retrieval}},
url = {http://dx.doi.org/10.1007/978-3-642-28997-2{\_}9},
year = {2012}
}
@inproceedings{Serdyukov:2008:MMR,
address = {New York, New York, USA},
author = {Serdyukov, Pavel and Rode, Henning and Hiemstra, Djoerd},
booktitle = {Proceeding of the 17th ACM conference on Information and knowledge mining - CIKM '08},
doi = {10.1145/1458082.1458232},
isbn = {9781595939913},
keywords = {enterprise search,entity retrieval,expert finding,expertise,language models,markov processes,random walks},
mendeley-tags = {entity retrieval},
pages = {1133},
publisher = {ACM Press},
series = {CIKM '08},
title = {{Modeling multi-step relevance propagation for expert finding}},
url = {http://doi.acm.org/10.1145/1458082.1458232 http://portal.acm.org/citation.cfm?doid=1458082.1458232},
year = {2008}
}
@inproceedings{Vallet:2008:IMI,
address = {New York, NY, USA},
author = {Vallet, David and Zaragoza, Hugo},
booktitle = {Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/1390334.1390541},
isbn = {978-1-60558-164-4},
keywords = {entity ranking,entity retrieval,faceted search,type ranking},
mendeley-tags = {entity retrieval},
pages = {857--858},
publisher = {ACM},
series = {SIGIR '08},
title = {{Inferring the most important types of a query: a semantic approach}},
url = {http://doi.acm.org/10.1145/1390334.1390541},
year = {2008}
}
@inproceedings{Petkova:2007:PDR,
address = {New York, NY, USA},
author = {Petkova, Desislava and Croft, W Bruce},
booktitle = {Proceedings of the sixteenth ACM conference on Conference on information and knowledge management},
doi = {10.1145/1321440.1321542},
isbn = {978-1-59593-803-9},
keywords = {entity retrieval,expert finding,language models,ne (named entity) retrieval,proximity kernels},
mendeley-tags = {entity retrieval},
pages = {731--740},
publisher = {ACM},
series = {CIKM '07},
title = {{Proximity-based document representation for named entity retrieval}},
url = {http://doi.acm.org/10.1145/1321440.1321542},
year = {2007}
}
@article{Balog:2012:ER,
author = {Balog, K and Fang, Y and de Rijke, M and Serdyukov, P and Si, L},
journal = {Foundations and Trends in Information Retrieval},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
number = {2-3},
pages = {127--256},
title = {{Expertise Retrieval}},
volume = {6},
year = {2012}
}
@inproceedings{Kamps:2008:ILE,
address = {Berlin, Heidelberg},
author = {Kamps, Jaap and Koolen, Marijn},
booktitle = {Proceedings of the IR research, 30th European conference on Advances in information retrieval},
isbn = {3-540-78645-7, 978-3-540-78645-0},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {270--282},
publisher = {Springer-Verlag},
series = {ECIR'08},
title = {{The importance of link evidence in Wikipedia}},
url = {http://dl.acm.org/citation.cfm?id=1793274.1793309},
year = {2008}
}
@inproceedings{Serdyukov:2007:EES,
address = {Berlin, Heidelberg},
author = {Serdyukov, Pavel and Chernov, Sergey and Nejdl, Wolfgang},
booktitle = {Proceedings of the 29th European conference on IR research},
isbn = {978-3-540-71494-1},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {737--740},
publisher = {Springer-Verlag},
series = {ECIR'07},
title = {{Enhancing expert search through query modeling}},
url = {http://dl.acm.org/citation.cfm?id=1763653.1763751},
year = {2007}
}
@article{Balog:2009:LMF,
address = {Tarrytown, NY, USA},
author = {Balog, Krisztian and Azzopardi, Leif and de Rijke, Maarten},
doi = {10.1016/j.ipm.2008.06.003},
issn = {0306-4573},
journal = {Inf. Process. Manage.},
keywords = {Expert finding,Expertise search,Intranet search,Language modeling,entity retrieval},
mendeley-tags = {entity retrieval},
month = {jan},
number = {1},
pages = {1--19},
publisher = {Pergamon Press, Inc.},
title = {{A language modeling framework for expert finding}},
url = {http://dx.doi.org/10.1016/j.ipm.2008.06.003},
volume = {45},
year = {2009}
}
@inproceedings{Balog:2012:HTT,
address = {New York, NY, USA},
author = {Balog, Krisztian and Neumayer, Robert},
booktitle = {Proceedings of the 21st ACM international conference on Information and knowledge management},
doi = {10.1145/2396761.2398648},
isbn = {978-1-4503-1156-4},
keywords = {entity retrieval,query classification,semantic search},
mendeley-tags = {entity retrieval},
pages = {2391--2394},
publisher = {ACM},
series = {CIKM '12},
title = {{Hierarchical target type identification for entity-oriented queries}},
url = {http://doi.acm.org/10.1145/2396761.2398648},
year = {2012}
}
@inproceedings{Fang:2007:PME,
address = {Berlin, Heidelberg},
author = {Fang, Hui and Zhai, ChengXiang},
booktitle = {Proceedings of the 29th European conference on IR research},
isbn = {978-3-540-71494-1},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {418--430},
publisher = {Springer-Verlag},
series = {ECIR'07},
title = {{Probabilistic models for expert finding}},
url = {http://dl.acm.org/citation.cfm?id=1763653.1763703},
year = {2007}
}
@inproceedings{Ogilvie:2003:CDR,
address = {New York, NY, USA},
author = {Ogilvie, Paul and Callan, Jamie},
booktitle = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval},
doi = {10.1145/860435.860463},
isbn = {1-58113-646-3},
keywords = {data fusion,entity retrieval,known-item finding,language models,meta-search algorithms},
mendeley-tags = {entity retrieval},
pages = {143--150},
publisher = {ACM},
series = {SIGIR '03},
title = {{Combining document representations for known-item search}},
url = {http://doi.acm.org/10.1145/860435.860463},
year = {2003}
}
@inproceedings{Neumayer:2012:MEA,
address = {Berlin, Heidelberg},
author = {Neumayer, Robert and Balog, Krisztian and N{\o}rv{\aa}g, Kjetil},
booktitle = {Proceedings of the 34th European conference on Advances in Information Retrieval},
doi = {10.1007/978-3-642-28997-2_12},
isbn = {978-3-642-28996-5},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {133--145},
publisher = {Springer-Verlag},
series = {ECIR'12},
title = {{On the modeling of entities for ad-hoc entity search in the web of data}},
url = {http://dx.doi.org/10.1007/978-3-642-28997-2{\_}12},
year = {2012}
}
@inproceedings{Perez-Aguera:2010:UBS,
address = {New York, NY, USA},
author = {P{\'{e}}rez-Ag{\"{u}}era, Jos{\'{e}} R and Arroyo, Javier and Greenberg, Jane and Iglesias, Joaquin Perez and Fresno, Victor},
booktitle = {Proceedings of the 3rd International Semantic Search Workshop},
doi = {10.1145/1863879.1863881},
isbn = {978-1-4503-0130-5},
keywords = {BM25F,entity retrieval,information retrieval,semantic search},
mendeley-tags = {entity retrieval},
pages = {2:1----2:8},
publisher = {ACM},
series = {SEMSEARCH '10},
title = {{Using BM25F for semantic search}},
url = {http://doi.acm.org/10.1145/1863879.1863881},
year = {2010}
}
@inproceedings{Moreira:2011:LRE,
address = {Berlin, Heidelberg},
author = {Moreira, Catarina and Calado, P{\'{a}}vel and Martins, Bruno},
booktitle = {Proceedings of the 15th Portugese conference on Progress in artificial intelligence},
isbn = {978-3-642-24768-2},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {431--445},
publisher = {Springer-Verlag},
series = {EPIA'11},
title = {{Learning to rank for expert search in digital libraries of academic publications}},
url = {http://dl.acm.org/citation.cfm?id=2051115.2051156},
year = {2011}
}
@inproceedings{Balog:2013:TCE,
author = {Balog, Krisztian and Neumayer, Robert},
booktitle = {Proceedings of the 36th international ACM SIGIR conference on Research and development in Information Retrieval},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
publisher = {ACM},
series = {SIGIR '13},
title = {{A Test Collection for Entity Search in DBpedia}},
year = {2013}
}
@inproceedings{Neumayer:2012:SGE,
address = {Berlin, Heidelberg},
author = {Neumayer, Robert and Balog, Krisztian and N{\o}rv{\aa}g, Kjetil},
booktitle = {Proceedings of the 34th European conference on Advances in Information Retrieval},
doi = {10.1007/978-3-642-28997-2_59},
isbn = {978-3-642-28996-5},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {540--543},
publisher = {Springer-Verlag},
series = {ECIR'12},
title = {{When simple is (more than) good enough: effective semantic search with (almost) no semantics}},
url = {http://dx.doi.org/10.1007/978-3-642-28997-2{\_}59},
year = {2012}
}
@inproceedings{Bron:2013:EBE,
address = {Berlin, Heidelberg},
author = {Bron, Marc and Balog, Krisztian and de Rijke, Maarten},
booktitle = {Proceedings of the 35th European conference on Advances in Information Retrieval},
doi = {10.1007/978-3-642-36973-5_33},
isbn = {978-3-642-36972-8},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {392--403},
publisher = {Springer-Verlag},
series = {ECIR'13},
title = {{Example based entity search in the web of data}},
url = {http://dx.doi.org/10.1007/978-3-642-36973-5{\_}33},
year = {2013}
}
@inproceedings{Blanco:2011:EEE,
address = {Berlin, Heidelberg},
author = {Blanco, Roi and Mika, Peter and Vigna, Sebastiano},
booktitle = {Proceedings of the 10th international conference on The semantic web - Volume Part I},
isbn = {978-3-642-25072-9},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
pages = {83--97},
publisher = {Springer-Verlag},
series = {ISWC'11},
title = {{Effective and efficient entity search in RDF data}},
url = {http://dl.acm.org/citation.cfm?id=2063016.2063023},
year = {2011}
}
@inproceedings{Yang:2009:EFE,
address = {New York, NY, USA},
author = {Yang, Z and Tang, J and Wang, B and Guo, J and Li, J and Chen, S},
booktitle = {Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining},
isbn = {978-1-60558-495-9},
keywords = {entity retrieval},
mendeley-tags = {entity retrieval},
organization = {ACM},
pages = {1--4},
publisher = {ACM},
series = {KDD '09},
title = {{Expert2bole: From expert finding to bole search}},
year = {2009}
}
@inproceedings{Macdonald:2006:VCA,
address = {New York, NY, USA},
author = {Macdonald, Craig and Ounis, Iadh},
booktitle = {Proceedings of the 15th ACM international conference on Information and knowledge management},
doi = {10.1145/1183614.1183671},
isbn = {1-59593-433-2},
keywords = {data fusion,entity retrieval,expert finding,expert search,expertise modelling,information retrieval,ranking,voting},
mendeley-tags = {entity retrieval},
pages = {387--396},
publisher = {ACM},
series = {CIKM '06},
title = {{Voting for candidates: adapting data fusion techniques for an expert search task}},
url = {http://doi.acm.org/10.1145/1183614.1183671},
year = {2006}
}
@inproceedings{Fang:2010:DMI,
address = {New York, NY, USA},
author = {Fang, Yi and Si, Luo and Mathur, Aditya P},
booktitle = {Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval},
doi = {10.1145/1835449.1835563},
isbn = {978-1-4503-0153-4},
keywords = {discriminative models,enterprise search,entity retrieval,expert search},
mendeley-tags = {entity retrieval},
pages = {683--690},
publisher = {ACM},
series = {SIGIR '10},
title = {{Discriminative models of integrating document evidence and document-candidate associations for expert search}},
url = {http://doi.acm.org/10.1145/1835449.1835563},
year = {2010}
}
@inproceedings{SAC:2013:Makris,
abstract = {Text annotation is the procedure of initially identifying, in a segment of text, a set of dominant in meaning words and later on attaching to them extra information (usually drawn from a concept ontology, implemented as a catalog) that expresses their conceptual content in the current context. Attaching additional semantic information and structure helps to represent, in a machine interpretable way, the topic of the text and is a fundamental preprocessing step to many Information Retrieval tasks like indexing, clustering, classification, text summarization and cross-referencing content on web pages, posts, tweets etc. In this paper, we deal with automatic annotation of text documents with entities of Wikipedia, the largest online knowledge base; a process that is commonly known as Wikification. Moving similarly to previous approaches the cross-reference of words in the text to Wikipedia articles is based on local compatibility between the text around the term and textual information embedded in the article. The main contribution of this paper is a set of disambiguation techniques that enhance previously published approaches by employing both the WordNet lexical database and the Wikipedia article's PageRank scores in the disambiguation process. The experimental evaluation performed depicts that the exploitation of these additional semantic information sources leads to more accurate Text Annotation.},
author = {Makris, Christos and Plegas, Yannis and Theodoridis, Evangelos},
booktitle = {SAC 2013},
doi = {10.1145/2480362.2480425},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Makris, Plegas, Theodoridis - 2013 - Improved text annotation with Wikipedia entities.pdf:pdf},
isbn = {9781450316569},
keywords = {TAGME,Wikipedia,Wikipedia entities,WordNet,entity linking,pagerank,relatedness,semantic data linking on web,semantics and ontologies in data integration,text semantic annotation},
mendeley-tags = {TAGME,Wikipedia,WordNet,entity linking,pagerank,relatedness},
month = {mar},
pages = {288},
title = {{Improved text annotation with Wikipedia entities}},
url = {http://dl.acm.org/citation.cfm?id=2480362.2480425},
year = {2013}
}
@inproceedings{COLING:2012:Cassidy,
abstract = {Disambiguation to Wikipedia (D2W) is the task of linking mentions of concepts in text to their corresponding Wikipedia entries. Most previous work has focused on linking terms in formal texts (e.g. newswire) to Wikipedia. Linking terms in short informal texts (e.g. tweets) is difcult for systems and humans alike as they lack a rich disambiguation context. We rst evaluate an existing Twitter dataset as well as the D2W task in general. We then test the effects of two tweet context expansion methods, based on tweet authorship and topic-based clustering, on a state-of-the-art D2W system and evaluate the results.},
author = {Cassidy, Taylor and Ji, Heng and Ratinov, Lev-Arie and Zubiaga, Arkaitz and Huang, Hongzhao},
booktitle = {COLING 2012},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Cassidy et al. - 2012 - Analysis and Enhancement of Wikification for Microblogs with Context Expansion.pdf:pdf},
keywords = {D2W,Disambiguation to Wikipedia,Twitter,Wikipedia,disambiguation context.,entity linking,expansion,twitter},
mendeley-tags = {Wikipedia,entity linking,expansion,twitter},
pages = {441--456},
title = {{Analysis and Enhancement of Wikification for Microblogs with Context Expansion.}},
url = {http://dblp.uni-trier.de/db/conf/coling/coling2012.html{\#}CassidyJRZH12},
year = {2012}
}
@inproceedings{HT:2013:Derczynski,
abstract = {Using semantic technologies for mining and intelligent information access to microblogs is a challenging, emerging research area. Unlike carefully authored news text and other longer content, tweets pose a number of new challenges, due to their short, noisy, context-dependent, and dynamic nature. Semantic annotation of tweets is typically performed in a pipeline, comprising successive stages of language identification, tokenisation, part-of-speech tagging, named entity recognition and entity disambiguation (e.g. with respect to DBpedia). Consequently, errors are cumulative, and earlier-stage problems can severely reduce the performance of final stages. This paper presents a characterisation of genre-specific problems at each semantic annotation stage and the impact on subsequent stages. Critically, we evaluate impact on two high-level semantic annotation tasks: named entity detection and disambiguation. Our results demonstrate the importance of making approaches specific to the genre, and indicate a diminishing returns effect that reduces the effectiveness of complex text normalisation.},
author = {Derczynski, Leon and Maynard, Diana and Aswani, Niraj and Bontcheva, Kalina},
booktitle = {HT 2013},
doi = {10.1145/2481492.2481495},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Derczynski et al. - 2013 - Microblog-genre noise and impact on semantic annotation accuracy.pdf:pdf},
isbn = {9781450319676},
keywords = {NER,POS,Twitter,Wikipedia,entity disambiguation,entity linking,entity recognition,microblog,semantic annotation,text normalisation},
mendeley-tags = {NER,POS,Twitter,Wikipedia,entity linking},
month = {may},
pages = {21--30},
publisher = {ACM Press},
title = {{Microblog-genre noise and impact on semantic annotation accuracy}},
url = {http://dl.acm.org/citation.cfm?id=2481492.2481495},
year = {2013}
}
@inproceedings{WIIAT:2012:Zheng,
abstract = {Entity disambiguation with a knowledge base becomes increasingly popular in the NLP community. In this paper, we employ Freebase as the knowledge base, which contains significantly more entities than Wikipedia and others. While huge in size, Freebase lacks context for most entities, such as the descriptive text and hyperlinks in Wikipedia, which are useful for disambiguation. Instead, we leverage two features of Freebase, namely the naturally disambiguated mention phrases (aka aliases) and the rich taxonomy, to perform disambiguation in an iterative manner. Specifically, we explore both generative and discriminative models for each iteration. Experiments on 2, 430, 707 English sentences and 33, 743 Freebase entities show the effectiveness of the two features, where 90{\%} accuracy can be reached without any labeled data. We also show that discriminative models with proposed split training strategy is robust against over fitting problem, and constantly outperforms the generative ones.},
author = {Zheng, Zhicheng and Si, Xiance and Li, Fangtao and Chang, Edward Y. and Zhu, Xiaoyan},
booktitle = {WI-IAT 2012},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Zheng et al. - 2012 - Entity Disambiguation with Freebase.pdf:pdf},
isbn = {978-0-7695-4880-7},
keywords = {Freebase,Name Entity Disambiguation,Naturally Disambiguated,Type Taxonomy,Wikipedia,entity linking},
mendeley-tags = {Freebase,Wikipedia,entity linking},
month = {dec},
pages = {82--89},
publisher = {IEEE Computer Society},
title = {{Entity Disambiguation with Freebase}},
url = {http://dl.acm.org/citation.cfm?id=2457524.2457667},
year = {2012}
}
@inproceedings{OAIR:2013:Wang,
abstract = {Automatically discovering cross-lingual links (CLs) between wikis can largely enrich the cross-lingual knowledge and facilitate knowledge sharing across different languages. In most existing approaches for cross-lingual knowledge linking, the seed CLs and the inner link structures are two important factors for nding new CLs. When there are insufcient seed CLs and inner links, discovering new CLs becomes a challenging problem. In this paper, we propose an approach that boosts cross-lingual knowledge linking by concept annotation. Given a small number of seed CLs and inner links, our approach rst enriches the inner links in wikis by using concept annotation method, and then predicts new CLs with a regression-based learning model. These two steps mutually reinforce each other, and are executed iteratively to nd as many CLs as possible. Experimental results on the English and Chinese Wikipedia data show that the concept annotation can effectively improve the quantity and quality of predicted CLs. With 50,000 seed CLs and 30{\%} of the original inner links in Wikipedia, our approach discovered 171,393 more CLs in four runs when using concept annotation.},
author = {Wang, Zhichun and Li, Juanzi and Tang, Jie},
booktitle = {IJCAI 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Li, Tang - Unknown - Boosting Cross-lingual Knowledge Linking via Concept Annotation.pdf:pdf},
keywords = {Wikipedia,cross-lingual,entity linking},
mendeley-tags = {Wikipedia,cross-lingual,entity linking},
title = {{Boosting Cross-lingual Knowledge Linking via Concept Annotation}}
}
@inproceedings{WSDM:2013:Bordino,
abstract = {We study the problem of anticipating user search needs, based on their browsing activity. Given the current web page p that a user is visiting we want to recommend a small and diverse set of search queries that are relevant to the content of p, but also non-obvious and serendipitous. We introduce a novel method that is based on the content of the page visited, rather than on past browsing patterns as in previous literature. Our content-based approach can be used even for previously unseen pages. We represent the topics of a page by the set of Wikipedia entities extracted from it. To obtain useful query suggestions for these entities, we exploit a novel graph model that we call EQGraph (Entity-Query Graph), containing entities, queries, and transitions between entities, between queries, as well as from entities to queries. We perform Personalized PageRank computation on such a graph to expand the set of entities extracted from a page into a richer set of entities, and to associate these entities with relevant query suggestions. We develop an efficient implementation to deal with large graph instances and suggest queries from a large and diverse pool. We perform a user study that shows that our method produces relevant and interesting recommendations, and outperforms an alternative method based on reverse IR.},
author = {Bordino, Ilaria and {De Francisci Morales}, Gianmarco and Weber, Ingmar and Bonchi, Francesco},
booktitle = {WSDM 2013},
doi = {10.1145/2433396.2433433},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Bordino et al. - 2013 - From machu{\_}picchu to rafting the urubamba river anticipating information needs via the entity-query graph.pdf:pdf},
isbn = {9781450318693},
keywords = {Wikipedia,application,entity extraction,entity linking,implicit search,pagerank,query suggestions,serendipity},
mendeley-tags = {Wikipedia,application,entity linking,pagerank},
month = {feb},
pages = {275},
title = {{From machu{\_}picchu to "rafting the urubamba river": anticipating information needs via the entity-query graph}},
url = {http://dl.acm.org/citation.cfm?id=2433396.2433433},
year = {2013}
}
@inproceedings{PVLDB:2011:Yosef,
abstract = {We present AIDA, a framework and online tool for entity detection and disambiguation. Given a natural-language text or a Web table, we map mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base like DBpedia, Freebase, or YAGO. AIDA is a robust framework centred around collective disambiguation exploiting the prominence of entities, similarity between the context of the mention and its candidates, and the coherence among candidate entities for all mentions. We have developed a Web-based online interface for AIDA where different formats of inputs can be processed on the y, returning proper entities and showing intermediate steps of the disambiguation process.},
author = {Yosef, Mohamed Amir and Hoffart, Johannes and Bordino, Ilaria and Spaniol, Marc and Weikum, Gerhard},
booktitle = {PVLDB 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Yosef et al. - 2011 - AIDA An Online Tool for Accurate Disambiguation of Named Entities in Text and Tables.pdf:pdf},
keywords = {commonness,entity linking,graph,relatedness},
mendeley-tags = {commonness,entity linking,graph,relatedness},
number = {12},
pages = {1450--1453},
title = {{AIDA: An Online Tool for Accurate Disambiguation of Named Entities in Text and Tables.}},
url = {http://dblp.uni-trier.de/db/journals/pvldb/pvldb4.html{\#}YosefHBSW11},
volume = {4},
year = {2011}
}
@inproceedings{EMNLP:2011:Hoffart,
abstract = {Disambiguating named entities in natural-language text maps mentions of ambiguous names onto canonical entities like people or places, registered in a knowledge base such as DBpedia or YAGO. This paper presents a robust method for collective disambiguation, by harnessing context from knowledge bases and using a new form of coherence graph. It unifies prior approaches into a comprehensive framework that combines three measures: the prior probability of an entity being mentioned, the similarity between the contexts of a mention and a candidate entity, as well as the coherence among candidate entities for all mentions together. The method builds a weighted graph of mentions and candidate entities, and computes a dense subgraph that approximates the best joint mention-entity mapping. Experiments show that the new method significantly outperforms prior methods in terms of accuracy, with robust behavior across a variety of inputs.},
author = {Hoffart, Johannes and Yosef, Mohamed Amir and Bordino, Ilaria and F{\"{u}}rstenau, Hagen and Pinkal, Manfred and Spaniol, Marc and Taneva, Bilyana and Thater, Stefan and Weikum, Gerhard},
booktitle = {EMNLP 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Hoffart et al. - 2011 - Robust disambiguation of named entities in text.pdf:pdf;:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Hoffart et al. - 2011 - Robust disambiguation of named entities in text(2).pdf:pdf},
isbn = {978-1-937284-11-4},
keywords = {NER,SVM,Wikipedia,commonness,entity linking,graph},
mendeley-tags = {NER,SVM,Wikipedia,commonness,entity linking,graph},
month = {jul},
pages = {782--792},
title = {{Robust disambiguation of named entities in text}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145521},
year = {2011}
}
@inproceedings{WWW:2013:Cornolti,
abstract = {In this paper we design and implement a benchmarking framework for fair and exhaustive comparison of entity-annotation systems. The framework is based upon the definition of a set of problems related to the entity-annotation task, a set of measures to evaluate systems performance, and a systematic comparative evaluation involving all publicly available datasets, containing texts of various types such as news, tweets and Web pages. Our framework is easily extensible with novel entity annotators, datasets and evaluation measures for comparing systems, and it has been released to the public as open source. We use this framework to perform the rst extensive comparison among all available entity annotators over all available datasets, and draw many interesting conclusions upon their efficiency and effectiveness. We also draw conclusions between academic versus commercial annotators.},
address = {Rio de Janeiro},
author = {Cornolti, Marco and Ferragina, Paolo and Ciaramita, Massimiliano},
booktitle = {WWW 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Cornolti, Ferragina, Ciaramita - 2013 - A Framework for Benchmarking Entity-Annotation Systems(2).pdf:pdf},
isbn = {9781450320351},
keywords = {Wikipedia,benchmark framework,entity annotation,entity linking,meta evaluation,wikipedia},
mendeley-tags = {Wikipedia,entity linking,meta evaluation},
title = {{A Framework for Benchmarking Entity-Annotation Systems}},
year = {2013}
}
@inproceedings{OAIR:2013:Balog,
abstract = {Knowledge bases have become indispensable sources of information. It is therefore critical that they rely on the latest information available and get updated every time new facts surface. Knowledge base acceleration (KBA) systems seek to help humans expand knowledge bases like Wikipedia by automatically recommending edits based on incoming content streams. A core step in this process is that of identifying relevant content, i.e., ltering documents that would imply modications to the attributes or relations of a given target entity. We propose two multi-step classication approaches for this task that consist of two and three binary classi- cation steps, respectively. Both methods share the same initial component, which is concerned with the identication of entity mentions in documents, while subsequent steps involve identication of documents being relevant and/or central to a given entity. Using the evaluation platform of the TREC 2012 KBA track and a rich feature set developed for this particular task, we show that both approaches deliver state-of-the-art performance.},
author = {{Balog Krisztian} and Naimdjon, Takhirov and Heri, Ramampiaro and Kjetil, N{\o}rv{\aa}g},
booktitle = {OAIR 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Balog Krisztian et al. - 2013 - Multi-step Classification Approaches to Cumulative Citation Recommendation.pdf:pdf},
keywords = {TREC-KBA,Wikipedia,cumulative citation recommendation,entity linking,information filtering,knowledge base acceleration},
mendeley-tags = {TREC-KBA,Wikipedia,entity linking},
title = {{Multi-step Classification Approaches to Cumulative Citation Recommendation}},
year = {2013}
}
@inproceedings{WSDM:2012:Meij,
abstract = {Microblogs have become an important source of information for the purpose of marketing, intelligence, and reputation management. Streams of microblogs are of great value because of their direct and real-time nature. Determining what an individual microblog post is about, however, can be non-trivial because of creative language usage, the highly contextualized and informal nature of microblog posts, and the limited length of this form of communication. We propose a solution to the problem of determining what a microblog post is about through semantic linking: we add semantics to posts by automatically identifying concepts that are semantically related to it and generating links to the corresponding Wikipedia articles. The identified concepts can subsequently be used for, e.g., social media mining, thereby reducing the need for manual inspection and selection. Using a purpose-built test collection of tweets, we show that recently proposed approaches for semantic linking do not perform well, mainly due to the idiosyncratic nature of microblog posts. We propose a novel method based on machine learning with a set of innovative features and show that it is able to achieve significant improvements over all other methods, especially in terms of precision.},
author = {Meij, Edgar and Weerkamp, Wouter and {De Rijke}, Maarten},
booktitle = {WSDM 2012},
doi = {10.1145/2124295.2124364},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Meij, Weerkamp, De Rijke - 2012 - Adding semantics to microblog posts.pdf:pdf},
isbn = {9781450307475},
keywords = {SVM,Twitter,Wikipedia,commonness,entity linking,naive bayes,random forests,tagme},
mendeley-tags = {SVM,Twitter,Wikipedia,commonness,entity linking,naive bayes,random forests,tagme},
pages = {563},
publisher = {ACM Press},
series = {WSDM '12},
title = {{Adding semantics to microblog posts}},
url = {http://dl.acm.org/citation.cfm?doid=2124295.2124364},
year = {2012}
}
@inproceedings{OAIR:2013:Odijk,
abstract = {Television is changing. Increasingly, broadcasts are consumed interactively. This allows broadcasters to provide consumers with additional background information that they may bookmark for later consumption. To support this type of functionality, we consider the task of linking a textual streams derived from live broadcasts to Wikipedia. While link generation has received considerable attention in recent years, our task has unique demands that require an approach that needs to (i) be high-precision oriented, (ii) perform in real-time, (iii) work in a streaming setting, and (iv) typically, with a very limited context. We propose a learning to rerank approach that signicantly improves over a strong baseline in terms of effectiveness and whose processing time is very short. We extend this approach, leveraging the streaming nature of the textual sources that we link by modeling context as a graph. We show how our graph-based context model further improves effectiveness. For evaluation purposes we create a dataset of segments of television subtitles that we make available to the research community.},
author = {Odijk, Daan and Meij, Edgar and de Rijke, Maarten},
booktitle = {OAIR 2013},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Odijk, Meij, de Rijke - 2013 - Feeding the Second Screen Semantic Linking based on Subtitles.pdf:pdf},
keywords = {Wikipedia,centrality,commonness,degree,entity linking,graph,keyphraseness,pagerank,random forests},
mendeley-tags = {Wikipedia,centrality,commonness,degree,entity linking,graph,keyphraseness,pagerank,random forests},
title = {{Feeding the Second Screen: Semantic Linking based on Subtitles}},
year = {2013}
}
@article{JASIST:2013:Li,
abstract = {The semantic knowledge of Wikipedia has proved to be useful for many tasks, for example, named entity disambiguation. Among these applications, the task of identifying the word sense based on Wikipedia is a crucial component because the output of this component is often used in subsequent tasks. In this article, we present a two-stage framework (called TSDW) for word sense disambiguation using knowledge latent in Wikipedia. The disambiguation of a given phrase is applied through a two-stage disambiguation process: (a) The first-stage disambiguation explores the contextual semantic information, where the noisy information is pruned for better effectiveness and efficiency; and (b) the second-stage disambiguation explores the disambiguated phrases of high confidence from the first stage to achieve better redisambiguation decisions for the phrases that are difficult to disambiguate in the first stage. Moreover, existing studies have addressed the disambiguation problem for English text only. Considering the popular usage of Wikipedia in different languages, we study the performance of TSDW and the existing state-of-the-art approaches over both English and Traditional Chinese articles. The experimental results show that TSDW generalizes well to different semantic relatedness measures and text in different languages. More important, TSDW significantly outperforms the state-of-the-art approaches with both better effectiveness and efficiency.},
author = {Li, Chenliang and Sun, Aixin and Datta, Anwitaman},
doi = {10.1002/asi.22829},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Li, Sun, Datta - 2013 - TSDW Two-stage word sense disambiguation using Wikipedia.pdf:pdf},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
keywords = {Wikipedia,commonness,entity linking,global,keyphraseness,local,relatedness},
mendeley-tags = {Wikipedia,commonness,entity linking,global,keyphraseness,local,relatedness},
month = {jun},
number = {6},
pages = {1203--1223},
title = {{TSDW: Two-stage word sense disambiguation using Wikipedia}},
url = {http://doi.wiley.com/10.1002/asi.22829},
volume = {64},
year = {2013}
}
@incollection{INEX:2009:Huang,
abstract = {In the third year of the Link the Wiki track, the focus has been shifted to anchor-to-bep link discovery. The participants were encouraged to utilize different technologies to resolve the issue of focused link discovery. Apart from the 2009 Wikipedia collection, the Te Ara collection was introduced for the first time in INEX. For the link the wiki tasks, 5000 file-to-file topics were randomly selected and 33 anchor-to-bep topics were nominated by the participants. The Te Ara collection does not contain hyperlinks and the task was to cross link the entire collection. A GUI tool for self-verification of the linking results was distributed. This helps participants verify the location of the anchor and bep. The assessment tool and the evaluation tool were revised to improve efficiency. Submission runs were evaluated against Wikipedia ground-truth and manual result set respectively. Focus-based evaluation was undertaken using a new metric. Evaluation results are presented and link discovery approaches are described.},
author = {Huang, Wei Che Darren and Geva, Shlomo and Trotman, Andrew},
booktitle = {Focused Retrieval and Evaluation},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Huang, Geva, Trotman - 2010 - Overview of the INEX 2009 link the wiki track.pdf:pdf},
keywords = {INEX,Wikipedia,entity linking},
mendeley-tags = {INEX,Wikipedia,entity linking},
pages = {312--323},
title = {{Overview of the INEX 2009 link the wiki track}},
year = {2010}
}
@inproceedings{LREC:2010:McNamee,
abstract = {Previous content extraction evaluations have neglected to address problems which complicate the incorporation of extracted information into an existing knowledge base. Previous question answering evaluations have likewise avoided tasks such as explicit disambiguation of target entities and handling a fixed set of questions about entities without previous determination of possible answers. In 2009 NIST conducted a Knowledge Base Population track at its Text Analysis Conference to unite the content extraction and question answering communities and jointly explore some of these issues. This exciting new evaluation attracted 13 teams from 6 countries that submitted results in two tasks, Entity Linking and Slot Filling. This paper explains the motivation and design of the tasks, describes the language resources that were developed for this evaluation, offers comparisons to previous community evaluations, and briefly summarizes the performance obtained by systems. We also identify relevant issues pertaining to target selection, challenging queries, and performance measures.},
author = {McNamee, Paul and Dang, Hoa Trang and Simpson, Heather and Schone, Patrick and Strassel, Stephanie},
booktitle = {LREC 2010},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/McNamee et al. - 2010 - An Evaluation of Technologies for Knowledge Base Population.pdf:pdf},
keywords = {TAC,entity linking},
mendeley-tags = {TAC,entity linking},
title = {{An Evaluation of Technologies for Knowledge Base Population.}},
url = {http://dblp.uni-trier.de/db/conf/lrec/lrec2010.html{\#}McNameeDSSS10},
year = {2010}
}
@inproceedings{SIGKDD:2009:Kulkarni,
abstract = {To take the first step beyond keyword-based search toward entity-based search, suitable token spans ("spots") on documents must be identified as references to real-world entities from an entity catalog. Several systems have been proposed to link spots on Web pages to entities in Wikipedia. They are largely based on local compatibility between the text around the spot and textual metadata associated with the entity. Two recent systems exploit inter-label dependencies, but in limited ways. We propose a general collective disambiguation approach. Our premise is that coherent documents refer to entities from one or a few related topics or domains. We give formulations for the trade-off between local spot-to-entity compatibility and measures of global coherence between entities. Optimizing the overall entity assignment is NP-hard. We investigate practical solutions based on local hill-climbing, rounding integer linear programs, and pre-clustering entities followed by local optimization within clusters. In experiments involving over a hundred manually-annotated Web pages and tens of thousands of spots, our approaches significantly outperform recently-proposed algorithms.},
address = {New York, New York, USA},
author = {Kulkarni, Sayali and Singh, Amit and Ramakrishnan, Ganesh and Chakrabarti, Soumen},
booktitle = {SIGKDD 2009},
doi = {10.1145/1557019.1557073},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Kulkarni et al. - 2009 - Collective annotation of Wikipedia entities in web text(2).pdf:pdf},
isbn = {9781605584959},
keywords = {Wikipedia,collective inference,entity annotation/disambiguation,entity linking,integer linear programming,web},
mendeley-tags = {Wikipedia,entity linking,integer linear programming,web},
month = {jun},
pages = {457},
publisher = {ACM Press},
title = {{Collective annotation of Wikipedia entities in web text}},
url = {http://dl.acm.org/citation.cfm?id=1557019.1557073},
year = {2009}
}
@inproceedings{TAC:2009:varma,
abstract = {In this paper, we report our participation in Update Summarization, Knowledge Base Population and Recognizing Textual Entailment at TAC 2009. This year, we enhanced our basic summaization system with support vector regression to better estimate the combined affect of different features in ranking. A Novelty measure is devised to effectively capture relevance and novelty of a term. For Knowledge Base Population, we analyzed IR approaches and Naive Bayes Classication with Phrase and Token searches. Finally for RTE, we extract templates from sentences and compare them based on rules designed seperately for predicting entailment, contradiction and unknown cases, making use of semantic knowledge from resources like Word Net, Verb Ocean and Acronym database.},
author = {Varma, V. and Bysani, P. and Reddy, K. and Bharat, V. and GSK, S. and Kumar, K. and Kovelamudi, S. and {Kiran Kumar}, N. and Maganti, N.},
booktitle = {TAC 2009},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Varma et al. - 2009 - IIIT Hyderabad at TAC 2009.pdf:pdf},
keywords = {Lucene,Naive Bayes,TAC,entity linking,kNN},
mendeley-tags = {Lucene,Naive Bayes,TAC,entity linking,kNN},
title = {{IIIT Hyderabad at TAC 2009}},
year = {2009}
}
@article{IJHCS:2009:Medelyan,
abstract = {Wikipedia is a goldmine of information; not just for its many readers, but also for the growing community of researchers who recognize it as a resource of exceptional scale and utility. It represents a vast investment of manual effort and judgment: a huge, constantly evolving tapestry of concepts and relations that is being applied to a host of tasks. This article provides a comprehensive description of this work. It focuses on research that extracts and makes use of the concepts, relations, facts and descriptions found in Wikipedia, and organizes the work into four broad categories: applying Wikipedia to natural language processing; using it to facilitate information retrieval and information extraction; and as a resource for ontology building. The article addresses how Wikipedia is being used as is, how it is being improved and adapted, and how it is being combined with other structures to create entirely new resources. We identify the research groups and individuals involved, and how their work has developed in the last few years. We provide a comprehensive list of the open-source software they have produced.},
author = {Medelyan, Olena and Milne, David and Legg, Catherine and Witten, Ian H.},
doi = {10.1016/j.ijhcs.2009.05.004},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Medelyan et al. - 2009 - Mining meaning from Wikipedia(2).pdf:pdf},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {Information extraction,Information retrieval,NLP,Ontologies,Semantic web,Text mining,Wikipedia,Wikipedia mining,entity linking},
mendeley-tags = {entity linking},
month = {sep},
number = {9},
pages = {716--754},
title = {{Mining meaning from Wikipedia}},
url = {http://dblp.uni-trier.de/db/journals/ijmms/ijmms67.html{\#}MedelyanMLW09},
volume = {67},
year = {2009}
}
@inproceedings{HLT:2011:Han,
abstract = {Linking entities with knowledge base (entity linking) is a key issue in bridging the textual data with the structural knowledge base. Due to the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the heterogenous knowledge of entities. In this paper, we propose a generative probabilistic model, called entity-mention model, which can leverage heterogenous entity knowledge (including popularity knowledge, name knowledge and context knowledge) for the entity linking task. In our model, each name mention to be linked is modeled as a sample generated through a three-step generative story, and the entity knowledge is encoded in the distribution of entities in document P(e), the distribution of possible names of a specific entity P(s$\backslash$e), and the distribution of possible contexts of a specific entity P(c$\backslash$e). To find the referent entity of a name mention, our method combines the evidences from all the three distributions P(e), P(s$\backslash$e) and P(c$\backslash$e). Experimental results show that our method can significantly outperform the traditional methods.},
author = {Han, Xianpei and Sun, Le},
booktitle = {HLT 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Han, Sun - 2011 - A generative entity-mention model for linking entities with knowledge base(2).pdf:pdf},
isbn = {978-1-932432-87-9},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {jun},
pages = {945--954},
publisher = {Association for Computational Linguistics},
title = {{A generative entity-mention model for linking entities with knowledge base}},
url = {http://dl.acm.org/citation.cfm?id=2002472.2002592},
year = {2011}
}
@inproceedings{IJCAI:2011:Zhang,
abstract = {Entity linking maps name mentions in the documents to entries in a knowledge base through resolving the name variations and ambiguities. In this paper, we propose three advancements for entity linking. Firstly, expanding acronyms can effectively reduce the ambiguity of the acronym mentions. However, only rule-based approaches relying heavily on the presence of text markers have been used for entity linking. In this paper, we propose a supervised learning algorithm to expand more complicated acronyms encountered, which leads to 15.1{\%} accuracy improvement over state-of-the-art acronym expansion methods. Secondly, as entity linking annotation is expensive and labor intensive, to automate the annotation process without compromise of accuracy, we propose an instance selection strategy to effectively utilize the automatically generated annotation. In our selection strategy, an informative and diverse set of instances are selected for effective disambiguation. Lastly, topic modeling is used to model the semantic topics of the articles. These advancements give statistical significant improvement to entity linking individually. Collectively they lead the highest performance on KBP-2010 task.},
author = {Zhang, Wei and Sim, Yan Chuan and Su, Jian and Tan, Chew Lim},
booktitle = {IJCAI 2011},
doi = {10.5591/978-1-57735-516-8/IJCAI11-319},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2011 - Entity linking with effective acronym expansion, instance selection and topic modeling.pdf:pdf},
isbn = {978-1-57735-515-1},
keywords = {TAC,entity linking},
mendeley-tags = {TAC,entity linking},
month = {jul},
pages = {1909--1914},
publisher = {AAAI Press},
title = {{Entity linking with effective acronym expansion, instance selection and topic modeling}},
url = {http://dl.acm.org/citation.cfm?id=2283696.2283721},
year = {2011}
}
@inproceedings{HLT:2010:Zheng,
abstract = {This paper address the problem of entity linking. Specifically, given an entity mentioned in unstructured texts, the task is to link this entity with an entry stored in the existing knowledge base. This is an important task for information extraction. It can serve as a convenient gateway to encyclopedic information, and can greatly improve the web users' experience. Previous learning based solutions mainly focus on classification framework. However, it's more suitable to consider it as a ranking problem. In this paper, we propose a learning to rank algorithm for entity linking. It effectively utilizes the relationship information among the candidates when ranking. The experiment results on the TAC 2009 dataset demonstrate the effectiveness of our proposed framework. The proposed method achieves 18.5{\%} improvement in terms of accuracy over the classification models for those entities which have corresponding entries in the Knowledge Base. The overall performance of the system is also better than that of the state-of-the-art methods.},
author = {Zheng, Zhicheng and Li, Fangtao and Huang, Minlie and Zhu, Xiaoyan},
booktitle = {HLT 2010},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Zheng et al. - 2010 - Learning to link entities with knowledge base.pdf:pdf;:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Zheng et al. - 2010 - Learning to link entities with knowledge base(2).pdf:pdf},
isbn = {1-932432-65-5},
keywords = {TAC,entity linking},
mendeley-tags = {TAC,entity linking},
month = {jun},
pages = {483--491},
publisher = {Association for Computational Linguistics},
title = {{Learning to link entities with knowledge base}},
url = {http://dl.acm.org/citation.cfm?id=1857999.1858071},
year = {2010}
}
@inproceedings{COLING:2010:Zhang,
abstract = {Entity linking refers entity mentions in a document to their representations in a knowledge base (KB). In this paper, we propose to use additional information sources from Wikipedia to find more name variations for entity linking task. In addition, as manually creating a training corpus for entity linking is laborintensive and costly, we present a novel method to automatically generate a large scale corpus annotation for ambiguous mentions leveraging on their unambiguous synonyms in the document collection. Then, a binary classifier is trained to filter out KB entities that are not similar to current mentions. This classifier not only can effectively reduce the ambiguities to the existing entities in KB, but also be very useful to highlight the new entities to KB for the further population. Furthermore, we also leverage on the Wikipedia documents to provide additional information which is not available in our generated corpus through a domain adaption approach which provides further performance improvements. The experiment results show that our proposed method outperforms the state-of-the-art approaches.},
author = {Zhang, Wei and Su, Jian and Tan, Chew Lim and Wang, Wen Ting},
booktitle = {COLING 2010},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2010 - Entity linking leveraging automatically generated annotation.pdf:pdf;:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2010 - Entity linking leveraging automatically generated annotation(2).pdf:pdf},
keywords = {TAC,Wikipedia,entity linking},
mendeley-tags = {TAC,Wikipedia,entity linking},
month = {aug},
pages = {1290--1298},
publisher = {Association for Computational Linguistics},
title = {{Entity linking leveraging: automatically generated annotation}},
url = {http://dl.acm.org/citation.cfm?id=1873781.1873926},
year = {2010}
}
@inproceedings{ECIR:2011:Li,
abstract = {In this paper we propose a general framework for word sense disambiguation using knowledge latent in Wikipedia. Specifically, we exploit the rich and growing Wikipedia corpus in order to achieve a large and robust knowledge repository consisting of keyphrases and their associated candidate topics. Keyphrases are mainly derived from Wikipedia article titles and anchor texts associated with wikilinks. The disambiguation of a given keyphrase is based on both the commonness of a candidate topic and the context-dependent relatedness where unnecessary (and potentially noisy) context information is pruned. With extensive experimental evaluations using different relatedness measures, we show that the proposed technique achieved comparable disambiguation accuracies with respect to state-of-the-art techniques, while incurring orders of magnitude less computation cost.},
author = {Li, Chenliang and Sun, Aixin and Datta, Anwitaman},
booktitle = {ECIR 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Li, Sun, Datta - 2011 - A generalized method for word sense disambiguation based on wikipedia.pdf:pdf},
isbn = {978-3-642-20160-8},
keywords = {context pruning,entity linking,wikipedia,word sense disambiguation},
mendeley-tags = {entity linking,word sense disambiguation},
month = {apr},
pages = {653--664},
publisher = {Springer-Verlag},
title = {{A generalized method for word sense disambiguation based on wikipedia}},
url = {http://dl.acm.org/citation.cfm?id=1996889.1996972},
year = {2011}
}
@inproceedings{IJCNLP:2011:Guo,
abstract = {In this paper, we formalize the task of nding a knowledge base entry that a given named entity mention refers to, namely entity linking, by identifying the most important node among the graph nodes representing the candidate entries. With the aim of ranking these entities by their importance, we introduce three degree-based measures of graph connectivity. Experimental results on the TACKBP benchmark data sets show that our graph-based method performs comparably with the state-of-the-art methods. We also show that using the name phrase feature outperforms the commonly used bagof-word feature for entity linking.},
author = {Guo, Yuhang and Che, Wanxiang and Liu, Ting and Li, Sheng},
booktitle = {IJCNLP 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2011 - A Graph-based Method for Entity Linking.pdf:pdf},
keywords = {TAC,Wikipedia,entity linking,graph,indegree,outdegree},
mendeley-tags = {TAC,Wikipedia,entity linking,graph,indegree,outdegree},
pages = {1010--1018},
title = {{A Graph-based Method for Entity Linking.}},
url = {http://dblp.uni-trier.de/db/conf/ijcnlp/ijcnlp2011.html{\#}GuoCLL11},
year = {2011}
}
@inproceedings{WISE:2011:Hachey,
abstract = {Named entity linking (NEL) grounds entity mentions to their corresponding Wikipedia article. State-of-the-art supervised NEL systems use features over the rich Wikipedia document and link-graph structure. Graph-based measures have been effective over WordNet for word sense disambiguation (WSD). We draw parallels between NEL and WSD, motivating our unsupervised NEL approach that exploits the Wikipedia article and category link graphs. Our system achieves 85.5{\%} accuracy on the TAC 2010 shared task -- competitive with the best supervised and unsupervised systems.},
author = {Hachey, Ben and Radford, Will and Curran, James R.},
booktitle = {WISE 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Hachey, Radford, Curran - 2011 - Graph-based named entity linking with wikipedia.pdf:pdf},
isbn = {978-3-642-24433-9},
keywords = {TAC,Wikipedia,centrality,entity linking,entity resolution,graph,integration,pagerank,text mining,web intelligence,wikipedia},
mendeley-tags = {TAC,Wikipedia,centrality,entity linking,graph,pagerank},
month = {oct},
pages = {213--226},
publisher = {Springer-Verlag},
title = {{Graph-based named entity linking with wikipedia}},
url = {http://dl.acm.org/citation.cfm?id=2050963.2050980},
year = {2011}
}
@inproceedings{CONLL:2012:Sil,
abstract = {Existing techniques for disambiguating named entities in text mostly focus on Wikipedia as a target catalog of entities. Yet for many types of entities, such as restaurants and cult movies, relational databases exist that contain far more extensive information than Wikipedia. This paper introduces a new task, called Open-Database Named-Entity Disambiguation (Open-DB NED), in which a system must be able to resolve named entities to symbols in an arbitrary database, without requiring labeled data for each new database. We introduce two techniques for Open-DB NED, one based on distant supervision and the other based on domain adaptation. In experiments on two domains, one with poor coverage by Wikipedia and the other with near-perfect coverage, our Open-DB NED strategies outperform a state-of-the-art Wikipedia NED system by over 25{\%} in accuracy.},
author = {Sil, Avirup and Cronin, Ernest and Nie, Penghai and Yang, Yinfei and Popescu, Ana-Maria and Yates, Alexander},
booktitle = {EMNLP-CoNLL 2012},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Sil et al. - 2012 - Linking named entities to any database.pdf:pdf},
keywords = {DB,entity linking},
mendeley-tags = {DB,entity linking},
month = {jul},
pages = {116--127},
publisher = {Association for Computational Linguistics},
title = {{Linking named entities to any database}},
url = {http://dl.acm.org/citation.cfm?id=2390948.2390963},
year = {2012}
}
@inproceedings{CONLL:2012:Lin,
abstract = {Entity linking systems link noun-phrase mentions in text to their corresponding Wikipedia articles. However, NLP applications would gain from the ability to detect and type all entities mentioned in text, including the long tail of entities not prominent enough to have their own Wikipedia articles. In this paper we show that once the Wikipedia entities mentioned in a corpus of textual assertions are linked, this can further enable the detection and fine-grained typing of the unlinkable entities. Our proposed method for detecting unlinkable entities achieves 24{\%} greater accuracy than a Named Entity Recognition baseline, and our method for fine-grained typing is able to propagate over 1,000 types from linked Wikipedia entities to unlinkable entities. Detection and typing of unlinkable entities can increase yield for NLP applications such as typed question answering.},
author = {Lin, Thomas and Mausam and Etzioni, Oren},
booktitle = {EMNLP-CoNLL 2012},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Mausam, Etzioni - 2012 - No noun phrase left behind detecting and typing unlinkable entities.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {jul},
pages = {893--903},
publisher = {Association for Computational Linguistics},
title = {{No noun phrase left behind: detecting and typing unlinkable entities}},
url = {http://dl.acm.org/citation.cfm?id=2390948.2391045},
year = {2012}
}
@article{ACM:2009:Navigli,
abstract = {Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.},
author = {Navigli, Roberto},
doi = {10.1145/1459352.1459355},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Navigli - 2009 - Word sense disambiguation.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {feb},
number = {2},
pages = {1--69},
title = {{Word sense disambiguation}},
url = {http://portal.acm.org/citation.cfm?doid=1459352.1459355},
volume = {41},
year = {2009}
}
@article{Hachey2013,
abstract = {Named Entity Linking (nel) grounds entity mentions to their corresponding node in a Knowledge Base (kb). Recently, a number of systems have been proposed for linking entity mentions in text to Wikipedia pages. Such systems typically search for candidate entities and then disambiguate them, returning either the best candidate or nil. However, comparison has focused on disambiguation accuracy, making it difficult to determine how search impacts performance. Furthermore, important approaches from the literature have not been systematically compared on standard data sets. We reimplement three seminal nel systems and present a detailed evaluation of search strategies. Our experiments find that coreference and acronym handling lead to substantial improvement, and search strategies account for much of the variation between systems. This is an interesting finding, because these aspects of the problem have often been neglected in the literature, which has focused largely on complex candidate ranking algorithms.},
author = {Hachey, Ben and Radford, Will and Nothman, Joel and Honnibal, Matthew and Curran, James R.},
doi = {10.1016/j.artint.2012.04.005},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Hachey et al. - 2013 - Evaluating Entity Linking with Wikipedia.pdf:pdf},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Disambiguation,Information extraction,Named Entity Linking,Semi-structured resources,TAC,Wikipedia,entity linking,meta evaluation},
mendeley-tags = {TAC,Wikipedia,entity linking,meta evaluation},
month = {jan},
pages = {130--150},
publisher = {Elsevier Science Publishers Ltd.},
title = {{Evaluating Entity Linking with Wikipedia}},
url = {http://dl.acm.org/citation.cfm?id=2405838.2405914},
volume = {194},
year = {2013}
}
@inproceedings{WEKEX:2012:Lin,
abstract = {This paper investigates entity linking over millions of high-precision extractions from a corpus of 500 million Web documents, toward the goal of creating a useful knowledge base of general facts. This paper is the first to report on entity linking over this many extractions, and describes new opportunities (such as corpus-level features) and challenges we found when entity linking at Web scale. We present several techniques that we developed and also lessons that we learned. We envision a future where information extraction and entity linking are paired to automatically generate knowledge bases with billions of assertions over millions of linked entities.},
author = {Lin, Thomas and Mausam and Etzioni, Oren},
booktitle = {AKBC-WEKEX 2012},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Mausam, Etzioni - 2012 - Entity linking at web scale.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
month = {jun},
pages = {84--88},
publisher = {Association for Computational Linguistics},
title = {{Entity linking at web scale}},
url = {http://dl.acm.org/citation.cfm?id=2391200.2391216},
year = {2012}
}
@inproceedings{ACL:1998:Bagga,
abstract = {Cross-document coreference occurs when the same person, place, event, or concept is discussed in more than one text source. Computer recognition of this phenomenon is important because it helps break "the document boundary" by allowing a user to examine information about a particular entity from multiple text sources at the same time. In this paper we descrive a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name. In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.},
author = {Bagga, Amit and Baldwin, Breck},
booktitle = {ACL 1998},
doi = {10.3115/980845.980859},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Bagga, Baldwin - 1998 - Entity-based cross-document coreferencing using the Vector Space Model.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {79},
publisher = {Association for Computational Linguistics},
series = {COLING '98},
title = {{Entity-based cross-document coreferencing using the Vector Space Model}},
url = {http://portal.acm.org/citation.cfm?doid=980845.980859},
volume = {1},
year = {1998}
}
@inproceedings{EMNLP-CoNLL:2007:cucerzan,
abstract = {This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.},
address = {Prague},
author = {Cucerzan, Silviu},
booktitle = {EMNLP-CoNLL 2007},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Cucerzan - 2007 - Large-Scale Named Entity Disambiguation Based on Wikipedia Data.pdf:pdf},
keywords = {Wikipedia,entity linking},
mendeley-tags = {Wikipedia,entity linking},
pages = {708--716},
publisher = {Association for Computational Linguistics},
title = {{Large-Scale Named Entity Disambiguation Based on Wikipedia Data}},
url = {http://www.aclweb.org/anthology/D/D07/D07-1074},
year = {2007}
}
@inproceedings{CIKM:2009:han,
abstract = {Name ambiguity problem has raised an urgent demand for efficient, high-quality named entity disambiguation methods. The key problem of named entity disambiguation is to measure the similarity between occurrences of names. The traditional methods measure the similarity using the bag of words (BOW) model. The BOW, however, ignores all the semantic relations such as social relatedness between named entities, associative relatedness between concepts, polysemy and synonymy between key terms. So the BOW cannot reflect the actual similarity. Some research has investigated social networks as background knowledge for disambiguation. Social networks, however, can only capture the social relatedness between named entities, and often suffer the limited coverage problem. To overcome the previous methods' deficiencies, this paper proposes to use Wikipedia as the background knowledge for disambiguation, which surpasses other knowledge bases by the coverage of concepts, rich semantic information and up-to-date content. By leveraging Wikipedia's semantic knowledge like social relatedness between named entities and associative relatedness between concepts, we can measure the similarity between occurrences of names more accurately. In particular, we construct a large-scale semantic network from Wikipedia, in order that the semantic knowledge can be used efficiently and effectively. Based on the constructed semantic network, a novel similarity measure is proposed to leverage Wikipedia semantic knowledge for disambiguation. The proposed method has been tested on the standard WePS data sets. Empirical results show that the disambiguation performance of our method gets 10.7{\%} improvement over the traditional BOW based methods and 16.7{\%} improvement over the traditional social network based methods.},
address = {Hong Kong, China},
author = {Han, Xianpei and Zhao, Jun},
booktitle = {CIKM 2009},
doi = {10.1145/1645953.1645983},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Han, Zhao - 2009 - Named entity disambiguation by leveraging wikipedia semantic knowledge.pdf:pdf},
isbn = {978-1-60558-512-3},
keywords = {Wikipedia,entity linking,semantic similarity,word sense disambiguation},
mendeley-tags = {Wikipedia,entity linking,semantic similarity,word sense disambiguation},
pages = {215--224},
publisher = {ACM},
series = {CIKM '09},
title = {{Named entity disambiguation by leveraging wikipedia semantic knowledge}},
type = {Conference proceedings (article)},
url = {http://portal.acm.org/citation.cfm?id=1645953.1645983},
year = {2009}
}
@inproceedings{EACL:2006:Bunescu,
abstract = {We present a new method for detecting and disambiguating named entities in open domain text. A disambiguation SVM kernel is trained to exploit the high coverage and rich structure of the knowledge encoded in an online encyclopedia. The resulting model significantly outperforms a less informed baseline.},
author = {Bunescu, Razvan and Pasca, Marius},
booktitle = {EACL 2006},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Bunescu, Pasca - 2006 - Using Encyclopedic Knowledge for Named Entity Disambiguation.pdf:pdf},
isbn = {1932432590},
keywords = {SVM,Wikipedia,entity linking},
mendeley-tags = {SVM,Wikipedia,entity linking},
number = {April},
pages = {9--16},
publisher = {ACL},
series = {EACL '06},
title = {{Using Encyclopedic Knowledge for Named Entity Disambiguation}},
url = {http://acl.ldc.upenn.edu/E/E06/E06-1002.pdf},
volume = {6},
year = {2006}
}
@inproceedings{ACL:2011:Ratinov,
abstract = {Disambiguating concepts and entities in a context sensitive way is a fundamental problem in natural language processing. The comprehensiveness of Wikipedia has made the online encyclopedia an increasingly popular target for disambiguation. Disambiguation to Wikipedia is similar to a traditional Word Sense Disambiguation task, but distinct in that the Wikipedia link structure provides additional information about which disambiguations are compatible. In this work we analyze approaches that utilize this information to arrive at coherent sets of disambiguations for a given document (which we call "global" approaches), and compare them to more traditional (local) approaches. We show that previous approaches for global disambiguation can be improved, but even then the local disambiguation provides a baseline which is very hard to beat.},
author = {Ratinov, Lev and Roth, Dan and Downey, Doug and Anderson, Mike},
booktitle = {ACL 2011},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Ratinov et al. - 2011 - Local and Global Algorithms for Disambiguation to Wikipedia.pdf:pdf;:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Ratinov et al. - 2011 - Local and Global Algorithms for Disambiguation to Wikipedia(2).pdf:pdf},
isbn = {9781932432879},
keywords = {SVM,Wikipedia,entity linking},
mendeley-tags = {SVM,Wikipedia,entity linking},
pages = {1375--1384},
publisher = {Association for Computational Linguistics},
title = {{Local and Global Algorithms for Disambiguation to Wikipedia}},
url = {http://cogcomp.cs.illinois.edu/papers/RatinovDoRo.pdf},
volume = {1},
year = {2011}
}
@inproceedings{Drenner2006,
abstract = {Item-oriented Web sites maintain repositories of information about things such as books, games, or products. Many of these Web sites offer discussion forums. However, these forums are often disconnected from the rich data available in the item repositories. We describe a system, movie linking, that bridges a movie recommendation Web site and a movie-oriented discussion forum. Through automatic detection and an interactive component, the system recognizes references to movies in the forum and adds recommendation data to the forums and conversation threads to movie pages. An eight week observational study shows that the system was able to identify movie references with precision of .93 and recall of .78. Though users reported that the feature was useful, their behavior indicates that the feature was more successful at enriching the interface than at integrating the system.},
author = {Drenner, Sara and Harper, Max and Frankowski, Dan and Riedl, John and Terveen, Loren},
booktitle = {SIGCHI 2006},
doi = {10.1145/1124772.1124914},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Drenner et al. - 2006 - Insert movie reference here a system to bridge conversation and item-oriented web sites.pdf:pdf},
isbn = {1595931783},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {951--954},
publisher = {ACM},
title = {{Insert movie reference here: a system to bridge conversation and item-oriented web sites}},
url = {http://portal.acm.org/citation.cfm?id=1124772.1124914{\&}coll=Portal{\&}dl=GUIDE{\&}CFID=83046602{\&}CFTOKEN=93186099},
year = {2006}
}
@inproceedings{ECIR:2012:Vitale,
abstract = {We propose a novel approach to the classification of short texts based on two factors: the use of Wikipedia-based annotators that have been recently introduced to detect the main topics present in an input text, represented via Wikipedia pages, and the design of a novel classification algorithm that measures the similarity between the input text and each output category by deploying only their annotated topics and the Wikipedia link-structure. Our approach waives the common practice of expanding the feature-space with new dimensions derived either from explicit or from latent semantic analysis. As a consequence it is simple and maintains a compact intelligible representation of the output categories. Our experiments show that it is efficient in construction and query time, accurate as state-of-the-art classifiers (see e.g. Phan et al. WWW '08), and robust with respect to concept drifts and input sources.},
address = {Berlin, Heidelberg},
author = {Vitale, Daniele and Ferragina, Paolo and Scaiella, Ugo},
booktitle = {ECIR 2012},
doi = {10.1007/978-3-642-28997-2},
editor = {Baeza-Yates, Ricardo and Vries, Arjen P. and Zaragoza, Hugo and Cambazoglu, B. Barla and Murdock, Vanessa and Lempel, Ronny and Silvestri, Fabrizio},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2012 - Classification of Short Texts by Deploying Topical Annotations.pdf:pdf},
isbn = {978-3-642-28996-5},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {376--387},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Classification of Short Texts by Deploying Topical Annotations}},
url = {http://dblp.uni-trier.de/db/conf/ecir/ecir2012.html{\#}VitaleFS12 http://link.springer.com/chapter/10.1007/978-3-642-28997-2{\_}32},
volume = {7224},
year = {2012}
}
@inproceedings{Ferragina2010,
abstract = {We designed and implemented TAGME, a system that is able to efficiently and judiciously augment a plain-text with pertinent hyperlinks to Wikipedia pages. The specialty of TAGME with respect to known systems [5,8] is that it may annotate texts which are short and poorly composed, such as snippets of search-engine results, tweets, news, etc.. This annotation is extremely informative, so any task that is currently addressed using the bag-of-words paradigm could benefit from using this annotation to draw upon (the millions of) Wikipedia pages and their inter-relations.},
author = {Ferragina, Paolo and Scaiella, Ugo},
booktitle = {CIKM 2010},
doi = {10.1145/1871437.1871689},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Ferragina, Scaiella - 2010 - TAGME on-the-fly annotation of short text fragments (by wikipedia entities).pdf:pdf},
isbn = {9781450300995},
keywords = {Wikipedia,commonness,entity linking,keyphraseness,relatedness,semantic annotation,text mining,wikipedia,word sense disambiguation},
mendeley-tags = {Wikipedia,commonness,entity linking,keyphraseness,relatedness},
month = {oct},
title = {{TAGME: on-the-fly annotation of short text fragments (by wikipedia entities)}},
url = {http://dl.acm.org/citation.cfm?id=1871437.1871689},
year = {2010}
}
@inproceedings{WSDM:2012:Scaiella,
abstract = {Search results clustering (SRC) is a challenging algorithmic problem that requires grouping together the results returned by one or more search engines in topically coherent clusters, and labeling the clusters with meaningful phrases describing the topics of the results included in them. In this paper we propose to solve SRC via an innovative approach that consists of modeling the problem as the labeled clustering of the nodes of a newly introduced graph of topics. The topics are Wikipedia-pages identified by means of recently proposed topic annotators [9, 11, 16, 20] applied to the search results, and the edges denote the relatedness among these topics computed by taking into account the linkage of the Wikipedia-graph. We tackle this problem by designing a novel algorithm that exploits the spectral properties and the labels of that graph of topics. We show the superiority of our approach with respect to academic state-of-the-art work [6] and well-known commercial systems (CLUSTY and LINGO3G) by performing an extensive set of experiments on standard datasets and user studies via Amazon Mechanical Turk. We test several standard measures for evaluating the performance of all systems and show a relative improvement of up to 20{\%}.},
author = {Scaiella, Ugo and Ferragina, Paolo and Marino, Andrea and Ciaramita, Massimiliano},
booktitle = {WSDM 2012},
doi = {10.1145/2124295.2124324},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Scaiella et al. - 2012 - Topical clustering of search results.pdf:pdf},
isbn = {9781450307475},
keywords = {entity linking,search result clustering,spectral clustering,topical annotation,user study},
mendeley-tags = {entity linking},
month = {feb},
pages = {223},
title = {{Topical clustering of search results}},
url = {http://dl.acm.org/citation.cfm?id=2124295.2124324},
year = {2012}
}
@inproceedings{SIGDOC:1986:Lesk,
address = {New York, NY, USA},
author = {Lesk, Michael},
booktitle = {SIGDOC 1986},
doi = {10.1145/318723.318728},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Lesk - 1986 - Automatic Sense Disambiguation Using Machine Readable Dictionaries How to Tell a Pine Cone from an Ice Cream Cone.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
pages = {24--26},
publisher = {ACM Press},
title = {{Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone}},
year = {1986}
}
@inproceedings{NAACL:2007:Mihalcea,
abstract = {This paper describes a method for generating sense-tagged data using Wikipedia as a source of sense annotations. Through word sense disambiguation experiments, we show that the Wikipedia-based sense annotations are reliable and can be used to construct accurate sense classifiers.},
author = {Mihalcea, Rada},
booktitle = {NAACL 2007},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Mihalcea - 2007 - Using Wikipedia for Automatic Word Sense Disambiguation.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
title = {{Using Wikipedia for Automatic Word Sense Disambiguation}},
year = {2007}
}
@inproceedings{WikiAI:2008:Medelyan,
abstract = {Abstract Wikipedia can be utilized as a controlled vocabulary for identifying the main in a document, with article titles serving as terms and redirect titles as their synonyms. Wikipedia contains over 4M such titles},
author = {Medelyan, Olena and Witten, Ian H and Milne, David},
booktitle = {Proceedings of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy},
editor = {Bunescu, Razvan and Gabrilovich, Evgeniy and Mihalcea, Rada},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Medelyan, Witten, Milne - 2008 - {\{}Topic indexing with Wikipedia{\}}.pdf:pdf},
isbn = {9781577353836},
keywords = {entity linking},
mendeley-tags = {entity linking},
organization = {AAAI},
pages = {19--24},
publisher = {AAAI Press},
title = {{Topic Indexing with Wikipedia}},
url = {http://www.aaai.org/Papers/Workshops/2008/WS-08-15/WS08-15-004.pdf},
volume = {1},
year = {2008}
}
@inproceedings{Milne2008b,
abstract = {This paper describes how to automatically cross-reference documents with Wikipedia: the largest knowledge base ever known. It explains how machine learning can be used to identify significant terms within unstructured text, and enrich it with links to the appropriate Wikipedia articles. The resulting link detector and disambiguator performs very well, with recall and precision of almost 75{\%}. This performance is constant whether the system is evaluated on Wikipedia articles or "real world" documents. This work has implications far beyond enriching documents with explanatory links. It can provide structured knowledge about any unstructured fragment of text. Any task that is currently addressed with bags of words - indexing, clustering, retrieval, and summarization to name a few - could use the techniques described here to draw on a vast network of concepts and semantics.},
author = {Milne, David and Witten, IH},
booktitle = {CIKM 2008},
doi = {10.1145/1458082.1458150},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Milne, Witten - 2008 - Learning to link with wikipedia.pdf:pdf},
isbn = {9781595939913},
keywords = {C4.5,SVM,Wikipedia,entity linking,naive bayes,news},
mendeley-tags = {C4.5,SVM,Wikipedia,entity linking,naive bayes,news},
title = {{Learning to link with Wikipedia}},
url = {http://dl.acm.org/citation.cfm?id=1458150},
year = {2008}
}
@inproceedings{WikiAI:2008:Milne,
abstract = {This paper describes a new technique for obtaining measures of semantic relatedness. Like other recent approaches, it uses Wikipedia to provide structured world knowledge about the terms of interest. Our approach is unique in that it does so using the hyperlink structure of Wikipedia rather than its category hierarchy or textual content. Evaluation with manually defined measures of semantic relatedness reveals this to be an effective compromise between the ease of computation of the former approach and the accuracy of the latter.},
author = {Milne, David and Witten, Ian H},
booktitle = {Proceedings of AAAI Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy},
doi = {10.1.1.165.4954},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - An effective, low-cost measure of semantic relatedness obtained from Wikipedia links.pdf:pdf},
keywords = {entity linking},
mendeley-tags = {entity linking},
title = {{An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links}},
url = {http://www.aaai.org/Papers/Workshops/2008/WS-08-15/WS08-15-005.pdf},
year = {2008}
}
@inproceedings{CIKM:2007:Mihalcea,
abstract = {This paper introduces the use of Wikipedia as a resource for automatic keyword extraction and word sense disambiguation, and shows how this online encyclopedia can be used to achieve state-of-the-art results on both these tasks. The paper also shows how the two methods can be combined into a system able to automatically enrich a text with links to encyclopedic knowledge. Given an input document, the system identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages. Evaluations of the system show that the automatic annotations are reliable and hardly distinguishable from manual annotations.},
author = {Mihalcea, Rada and Csomai, Andras},
booktitle = {CIKM 2007},
doi = {10.1145/1321440.1321475},
file = {:Users/emeij/Library/Application Support/Mendeley Desktop/Downloaded/Mihalcea, Csomai - 2007 - Wikify! linking documents to encyclopedic knowledge(2).pdf:pdf},
isbn = {978-1-59593-803-9},
keywords = {entity linking},
mendeley-tags = {entity linking},
series = {CIKM '07},
title = {{Wikify!: linking documents to encyclopedic knowledge}},
url = {http://dx.doi.org/10.1145/1321440.1321475},
year = {2007}
}
